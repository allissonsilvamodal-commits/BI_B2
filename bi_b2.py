# -*- coding: utf-8 -*-
"""BI_B2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qws0HJZuT-zAN6i-f5961IRdPCzFdcr0
"""

!pip install --upgrade gspread

from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)

B2sheet = gc.open('BI_B2')

page = B2sheet.sheet1

all_data = page.get_all_values()
display(all_data)

"""# Task
Load the data from the list of lists `all_data` into a pandas DataFrame, clean and preprocess the data, perform exploratory data analysis, transform the data for business intelligence, create a BI dashboard with relevant visualizations, and summarize the key insights.

## Carregar os dados em um dataframe do pandas

### Subtask:
Converter a lista de listas (`all_data`) em um DataFrame do pandas para facilitar a manipulaÃ§Ã£o e anÃ¡lise.

**Reasoning**:
Import the pandas library and create a DataFrame from the `all_data` list, using the first row as headers. Then, display the head of the DataFrame.
"""

import pandas as pd

df = pd.DataFrame(all_data[1:], columns=all_data[0])
display(df.head())

"""## Limpeza e prÃ©-processamento de dados

### Subtask:
Tratar valores ausentes, converter tipos de dados (especialmente para as colunas de valor e data), e padronizar formatos, se necessÃ¡rio.

**Reasoning**:
Convert value columns to numeric, date columns to datetime, and inspect for missing values.
"""

value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
for col in value_cols:
    df[col] = df[col].astype(str).str.replace('R\$', '').str.replace(',', '').str.strip()
    df[col] = pd.to_numeric(df[col], errors='coerce')

date_cols = ['Data de abertura', 'Data fechamento']
for col in date_cols:
    df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')

display(df.info())
display(df.isnull().sum())

"""## AnÃ¡lise exploratÃ³ria de dados (eda)

### Subtask:
Explorar os dados para entender sua estrutura, identificar padrÃµes, tendÃªncias e outliers. Isso pode incluir calcular estatÃ­sticas descritivas e visualizar a distribuiÃ§Ã£o das variÃ¡veis.

**Reasoning**:
Calculate and display descriptive statistics for numerical columns, and value counts for specified categorical columns, then create plots to visualize the distribution of 'Valor', 'Estado', 'EstÃ¡gio', and 'ResponsÃ¡vel' columns, handling potential non-finite values in 'Valor' before plotting.
"""

import matplotlib.pyplot as plt
import numpy as np

# 1. Calculate descriptive statistics for numerical columns
display(df.describe())

# 2. Calculate value counts for categorical columns
display(df['Estado'].value_counts())
display(df['EstÃ¡gio'].value_counts())
display(df['ResponsÃ¡vel'].value_counts())

# 3. Create a histogram of the 'Valor' column
plt.figure(figsize=(10, 6))
plt.hist(df['Valor'].dropna(), bins=50)
plt.title('Distribution of Valor')
plt.xlabel('Valor')
plt.ylabel('Frequency')
plt.show()

# 4. Create a bar plot for 'Estado'
plt.figure(figsize=(10, 6))
df['Estado'].value_counts().plot(kind='bar')
plt.title('Distribution of Estado')
plt.xlabel('Estado')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# 5. Create a bar plot for 'EstÃ¡gio'
plt.figure(figsize=(12, 6))
df['EstÃ¡gio'].value_counts().plot(kind='bar')
plt.title('Distribution of EstÃ¡gio')
plt.xlabel('EstÃ¡gio')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# 6. Create a bar plot for 'ResponsÃ¡vel'
plt.figure(figsize=(15, 7))
df['ResponsÃ¡vel'].value_counts().plot(kind='bar')
plt.title('Distribution of ResponsÃ¡vel')
plt.xlabel('ResponsÃ¡vel')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Adicionar filtros por data e granularidade (exemplo)
# st.sidebar.subheader("Filtrar por PerÃ­odo")

# OpÃ§Ã£o 1: Filtro por intervalo de datas
# date_cols_for_filter = ['Data de abertura', 'Data fechamento']
# selected_date_col = st.sidebar.selectbox("Selecione a coluna de data para filtrar:", date_cols_for_filter)

# if not df[selected_date_col].empty:
#     min_date = df[selected_date_col].min().date()
#     max_date = df[selected_date_col].max().date()

#     start_date = st.sidebar.date_input("Data de InÃ­cio", min_value=min_date, max_value=max_date, value=min_date)
#     end_date = st.sidebar.date_input("Data de Fim", min_value=min_date, max_value=max_date, value=max_date)

#     # Convert selected dates to datetime objects
#     start_datetime = pd.to_datetime(start_date)
#     end_datetime = pd.to_datetime(end_date)

#     # Apply date filter
#     filtered_df_date = df[(df[selected_date_col] >= start_datetime) & (df[selected_date_col] <= end_datetime)].copy()

#     st.subheader(f"Dados Filtrados por {selected_date_col} entre {start_date} e {end_date}")
#     st.dataframe(filtered_df_date.head()) # Display head of date-filtered data

# OpÃ§Ã£o 2: Selecionar granularidade (exemplo - pode ser expandido)
# granularity = st.sidebar.selectbox("Selecionar Granularidade:", ['Dia', 'Semana', 'MÃªs', 'Ano'])

# TODO: Implementar a agregaÃ§Ã£o e visualizaÃ§Ã£o baseada na granularidade selecionada

# Adicionar um filtro por TÃ­tulo
# st.sidebar.subheader("Filtrar por TÃ­tulo")
# selected_title = st.sidebar.selectbox("Selecione o TÃ­tulo:", df['TÃ­tulo'].unique())

# Filtrar o DataFrame com base no tÃ­tulo selecionado
# filtered_df = df[df['TÃ­tulo'] == selected_title]

# Exibir detalhes da oportunidade filtrada
# st.subheader("Detalhes da Oportunidade Selecionada")
# st.dataframe(filtered_df)

# TODO: Implementar filtros por data/hora e granularidade



# Criar a interface do painel com Streamlit
import streamlit as st

st.title("Painel de BI Operacional")

st.markdown("""
Este painel apresenta insights sobre os dados operacionais,
permitindo acompanhar o desempenho e a distribuiÃ§Ã£o dos negÃ³cios.
""")

# Exibir o DataFrame (opcional, para verificar os dados no painel)
# st.subheader("Dados Brutos (Primeiras 5 linhas)")
# st.dataframe(df.head())

# Limpeza e prÃ©-processamento de dados
value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
for col in value_cols:
    # Handle potential non-string values before applying string methods
    df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
    df[col] = pd.to_numeric(df[col], errors='coerce')

date_cols = ['Data de abertura', 'Data fechamento']
for col in date_cols:
    df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')

# Exibir informaÃ§Ãµes sobre o DataFrame apÃ³s a limpeza (opcional no script final, mas Ãºtil para depuraÃ§Ã£o)
# st.write("InformaÃ§Ãµes do DataFrame apÃ³s limpeza:")
# st.write(df.info())
# st.write("Valores ausentes apÃ³s limpeza:")
# st.write(df.isnull().sum())

import streamlit as st
from google.colab import auth
import gspread
from google.auth import default
import pandas as pd

# Authenticate to Google Cloud
auth.authenticate_user()

# Authorize gspread
creds, _ = default()
gc = gspread.authorize(creds)

# Open the spreadsheet and select the first sheet
try:
    B2sheet = gc.open('BI_B2')
    page = B2sheet.sheet1
    all_data = page.get_all_values()
    df = pd.DataFrame(all_data[1:], columns=all_data[0])
    st.write("Dados carregados com sucesso!")
    st.dataframe(df.head())

except Exception as e:
    st.error(f"Erro ao carregar dados do Google Sheet: {e}")

"""## TransformaÃ§Ã£o de dados para bi

### Subtask:
Preparar os dados para visualizaÃ§Ã£o, o que pode envolver agregaÃ§Ã£o, criaÃ§Ã£o de novas colunas relevantes para o BI (como mÃªs/ano de abertura, etc.), e seleÃ§Ã£o das colunas que serÃ£o usadas no painel.

**Reasoning**:
Create new date-related columns and aggregate the data as per instructions 1, 2, 3, and 4. Then, select the specified columns for the BI dashboard in instruction 5.
"""

# Access the already created date-related columns from the DataFrame

# Ensure the DataFrame 'df' is up-to-date by potentially re-running load_data() if needed
# df, df_timeline = load_data() # Uncomment and run this line if the DataFrame is not updated

df_agg_responsavel = df.groupby('ResponsÃ¡vel')['Valor'].sum().reset_index()

# Ensure 'MonthYear_Abertura' is not None before grouping
if 'MonthYear_Abertura' in df.columns and df['MonthYear_Abertura'] is not None:
    df_agg_estado_mes = df.groupby(['Estado', 'MonthYear_Abertura']).agg(
        Total_Deals=('OC_Identifier', 'nunique') # Use OC_Identifier and nunique for unique count
    ).reset_index()
else:
    # Handle case where MonthYear_Abertura could not be created
    df_agg_estado_mes = pd.DataFrame(columns=['Estado', 'MonthYear_Abertura', 'Total_Deals'])
    print("Warning: 'MonthYear_Abertura' column could not be created. Skipping aggregation by month.")


df_bi = df[['Mes de Abertura', 'Ano de Abertura', 'Mes de Fechamento', 'Ano de Fechamento', 'Estado', 'EstÃ¡gio', 'ResponsÃ¡vel', 'Valor']]

display(df_bi.head())
display(df_agg_responsavel.head())
display(df_agg_estado_mes.head())

import streamlit as st # Keep import for type hints if needed, but won't be used in execution
from google.colab import auth
import gspread
from google.auth import default
import pandas as pd
import re
from datetime import datetime
import os

# Define the load_data function directly in the Colab notebook
# @st.cache_data # Remove Streamlit cache decorator
def load_data():
    try:
        # Authenticate to Google Cloud
        auth.authenticate_user()
        creds, _ = default()
        gc = gspread.authorize(creds)
        B2sheet = gc.open('BI_B2')
        page = B2sheet.sheet1
        all_data = page.get_all_values()
        df = pd.DataFrame(all_data[1:], columns=all_data[0])

        # Limpeza e prÃ©-processing
        value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
        for col in value_cols:
            # Use regex=False as we are not using special regex characters
            df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
            df[col] = pd.to_numeric(df[col], errors='coerce')

        date_cols = ['Data de abertura', 'Data fechamento']
        for col in date_cols:
            # Use errors='coerce' to turn invalid date formats into NaT
            df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')

        # Extract unique opportunity identifier (OC + number) from 'TÃ­tulo'
        def extract_oc_identifier(title):
            if isinstance(title, str):
                match = re.search(r'(OC\s*\d+)', title, re.IGNORECASE)
                if match:
                    return match.group(1).replace(" ", "") # Remove space in "OC "
                else:
                    # Also check for "CTE" as it appears in titles and might be identifiers
                    match_cte = re.search(r'(CTE\s*\d+)', title, re.IGNORECASE)
                    if match_cte:
                         return match_cte.group(1).replace(" ", "")
            return None # Return None if no pattern is found

        df['OC_Identifier'] = df['TÃ­tulo'].apply(extract_oc_identifier)

        # Transformation and Feature Engineering for BI
        # Ensure date columns are not NaT before accessing dt properties
        df['Mes de Abertura'] = df['Data de abertura'].dt.month.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) else 0
        df['Ano de Abertura'] = df['Data de abertura'].dt.year.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) else 0
        df['Mes de Fechamento'] = df['Data fechamento'].dt.month.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data fechamento']) else 0
        df['Ano de Fechamento'] = df['Data fechamento'].dt.year.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data fechamento']) else 0

        # Create a 'MonthYear' column for time series analysis
        df['MonthYear_Abertura'] = df['Data de abertura'].dt.to_period('M') if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) else None
        df['MonthYear_Fechamento'] = df['Data fechamento'].dt.to_period('M') if pd.api.types.is_datetime64_any_dtype(df['Data fechamento']) else None

        # Extract Hour of Day from 'Data de abertura'
        # Ensure 'Data de abertura' is not NaT before extracting hour
        # Only extract hour if Data de abertura is a valid datetime
        df['Hour_of_Day_Abertura'] = df['Data de abertura'].apply(lambda x: x.hour if pd.notna(x) else -1).astype(int)


        # Calculate Time in Stage for timeline analysis using OC_Identifier
        df_timeline = df[['OC_Identifier', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()
        # Drop rows where OC_Identifier could not be extracted or Data de abertura is missing
        df_timeline.dropna(subset=['OC_Identifier', 'Data de abertura'], inplace=True)
        df_timeline = df_timeline.sort_values(by=['OC_Identifier', 'Data de abertura'])

        current_time = pd.to_datetime('now') # Timezone-naive for consistency
        df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600 # Time in hours

        df_timeline['Time_in_Stage'] = df_timeline.apply(
            lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
            axis=1
        )

        # Format 'Time_in_Stage' for display to include minutes
        def format_time_in_stage(hours):
            if pd.isna(hours):
                return "N/A"
            total_minutes = int(hours * 60)
            days = total_minutes // (24 * 60)
            remaining_minutes_after_days = total_minutes % (24 * 60)
            hours = remaining_minutes_after_days // 60
            minutes = remaining_minutes_after_days % 60
            return f"{days} days, {hours} hours, {minutes} minutes"

        df_timeline['Time_in_Stage_Formatted'] = df_timeline['Time_in_Stage'].apply(format_time_in_stage)


        return df, df_timeline

    except Exception as e:
        # st.error(f"Erro ao carregar dados do Google Sheet: {e}") # Remove Streamlit error display
        print(f"Error loading data from Google Sheet: {e}") # Use print instead
        return pd.DataFrame(), pd.DataFrame() # Return empty dataframes on error

# Re-run load_data to update the DataFrame with the new columns
df, df_timeline = load_data()

"""## CriaÃ§Ã£o do painel de bi

### Subtask:
Utilizar bibliotecas de visualizaÃ§Ã£o (como Matplotlib, Seaborn, Plotly ou atÃ© mesmo integrar com ferramentas de BI externas) para criar grÃ¡ficos e tabelas que apresentem insights sobre os dados (por exemplo, valor total por responsÃ¡vel, nÃºmero de negÃ³cios ganhos por mÃªs, etc.).

**Reasoning**:
Import the necessary libraries for visualization and create the requested bar plots to visualize the aggregated data.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Re-create the aggregated dataframes for plotting
df_agg_responsavel = df.groupby('ResponsÃ¡vel')['Valor'].sum().reset_index()

# Ensure 'MonthYear_Abertura' is not None before grouping
if 'MonthYear_Abertura' in df.columns and df['MonthYear_Abertura'] is not None:
    df_agg_estado_mes = df.groupby(['Estado', 'MonthYear_Abertura']).agg(
        Total_Deals=('OC_Identifier', 'nunique') # Use OC_Identifier and nunique for unique count
    ).reset_index()
else:
    # Handle case where MonthYear_Abertura could not be created
    df_agg_estado_mes = pd.DataFrame(columns=['Estado', 'MonthYear_Abertura', 'Total_Deals'])
    print("Warning: 'MonthYear_Abertura' column could not be created. Skipping aggregation by month for plotting.")


# Now create the plots
plt.figure(figsize=(14, 7))
sns.barplot(x='ResponsÃ¡vel', y='Valor', data=df_agg_responsavel)
plt.title('Total Valor per ResponsÃ¡vel')
plt.xlabel('ResponsÃ¡vel')
plt.ylabel('Total Valor')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 7))
sns.barplot(x='MonthYear_Abertura', y='Total_Deals', hue='Estado', data=df_agg_estado_mes) # Use MonthYear_Abertura for plotting
plt.title('Total Deals per Estado and Mes de Abertura')
plt.xlabel('Mes de Abertura')
plt.ylabel('Total Deals')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

"""## Refinamento e iteraÃ§Ã£o

### Subtask:
Ajustar as visualizaÃ§Ãµes e a anÃ¡lise com base nos insights obtidos e nas necessidades especÃ­ficas do seu BI.

**Reasoning**:
Analyze the current visualizations and consider potential areas for improvement based on the initial EDA and BI goals, then list specific ideas for new visualizations or modifications.
"""

# Analyze the current visualizations
print("Analysis of current visualizations:")
print("- 'Total Valor per ResponsÃ¡vel': This chart shows the total value handled by each responsible party. It clearly highlights which individuals or teams are associated with the highest total deal values.")
print("- 'Total Deals per Estado and Mes de Abertura': This chart shows the count of deals by their state ('Aberta', 'Ganha', 'Perdida') for each month of opening. It provides an overview of the deal flow and success rate over time, broken down by state.")

print("\nPotential areas for improvement and further exploration based on EDA and BI goals:")
print("- The initial EDA showed a significant number of deals in the 'Ganha' state. A deeper look into the distribution of 'Valor' specifically for 'Ganha' deals could provide insights into the typical value of successful deals.")
print("- The 'EstÃ¡gio' column has many different values. Analyzing the breakdown of 'EstÃ¡gio' within the 'Ganha' state could help understand the common stages at which deals are won.")
print("- While 'Total Deals per Estado and Mes de Abertura' provides some time trend, analyzing the *total value* over time (e.g., monthly total value for 'Ganha' deals) would be crucial for financial performance insights.")
print("- Exploring the relationship between 'ResponsÃ¡vel' and the 'Estado' of deals could reveal which responsible parties are most successful in closing deals (not just total value, but also win rate or distribution across states).")
print("- The 'Valor Rec.' and 'Valor rec. fechamento' columns had all zero values in the EDA. If these columns are intended to represent recovered value, their current state suggests they are not being populated correctly or are not relevant. Further investigation or exclusion from analysis might be needed, but for now, focus on 'Valor'.")

print("\nSpecific ideas for new visualizations or modifications:")
print("1.  **Visualization:** Histogram or box plot of 'Valor' for deals in the 'Ganha' state.")
print("    **Insight:** Understand the typical value range and distribution of successful deals.")
print("2.  **Visualization:** Bar plot showing the count of deals for each 'EstÃ¡gio' filtered by 'Estado' == 'Ganha'.")
print("    **Insight:** Identify the most common stages where deals are won.")
print("3.  **Visualization:** Line plot showing the monthly total 'Valor' for deals in the 'Ganha' state over time.")
print("    **Insight:** Track the revenue trend from won deals.")
print("4.  **Visualization:** Stacked bar plot showing the distribution of 'Estado' for the top N 'ResponsÃ¡vel' (based on deal count or total value).")
print("    **Insight:** Compare the success rates (distribution of states) across different responsible parties.")
print("5.  **Modification:** Refine the 'Total Valor per ResponsÃ¡vel' plot to exclude or highlight the responsible party with a significantly large value (Iran (OP-PB)) if it's an outlier that distorts the view of others.")

"""## Summary:

### Data Analysis Key Findings

*   The dataset was successfully loaded into a pandas DataFrame with the correct column headers.
*   Value columns ('Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento') were converted to numeric type after removing currency symbols and commas.
*   Date columns ('Data de abertura', 'Data fechamento') were converted to datetime objects.
*   Significant missing values were observed in the value-related columns, and a small number in 'Data fechamento'.
*   Descriptive statistics revealed a wide range in the 'Valor' column, suggesting potential outliers. 'Valor Rec.', 'Valor fechamento', and 'Valor rec. fechamento' had all zero values.
*   The majority of deals are in the 'Ganha' state.
*   The 'EstÃ¡gio' column contains many unique values with inconsistent formatting.
*   Deal distribution varies among responsible parties, with some handling significantly more deals than others.
*   New columns for month and year of opening and closing were successfully created.
*   Aggregated dataframes were created to show total 'Valor' per 'ResponsÃ¡vel' and total deals per 'Estado' and 'Mes de Abertura'.
*   Two bar plots were generated: 'Total Valor per ResponsÃ¡vel' and 'Total Deals per Estado and Mes de Abertura'.

### Insights or Next Steps

*   Investigate the 'Valor Rec.', 'Valor fechamento', and 'Valor rec. fechamento' columns as they currently contain only zero values, which might indicate data collection issues or irrelevance.
*   Refine the 'EstÃ¡gio' column by standardizing entries to reduce the number of unique values and enable more meaningful analysis of deal stages.
*   Create additional visualizations focusing on 'Ganha' deals, such as the distribution of 'Valor' for won deals and the monthly trend of total 'Valor' for won deals, to gain deeper insights into successful outcomes.

# Task
Create a dynamic BI dashboard using Streamlit that connects to a Google Sheet, loads and preprocesses the data, and displays interactive visualizations and key metrics for an operational team.

## Configurar o ambiente para streamlit

### Subtask:
Instalar a biblioteca Streamlit e outras dependÃªncias necessÃ¡rias.

**Reasoning**:
Install the necessary libraries for the Streamlit dashboard.
"""

!pip install streamlit gspread google-auth

!pip install pyngrok

!ngrok config add-authtoken 32sMP0W2ONOEmfbQ2mt5XLn2pkI_455pirg5RRFtSKiHF1ncM

import os
from google.colab import userdata

# Set your OpenAI API key as an environment variable
# Retrieve from Colab secrets
try:
    api_key = userdata.get('OPENAI_API_KEY')
    os.environ["OPENAI_API_KEY"] = api_key
    print("OpenAI API key successfully loaded from Colab secrets and set as environment variable.")
    # Add a print statement to check the environment variable value
    print(f"OPENAI_API_KEY environment variable value (first 5 chars): {os.getenv('OPENAI_API_KEY')[:5]}...")
except userdata.SecretNotFoundError:
    print("OpenAI API key not found in Colab secrets. Please add it.")
    print("Go to the left sidebar (ğŸ”‘), expand 'Secrets', and add a secret named OPENAI_API_KEY.")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from google.colab import auth
# import gspread
# from google.auth import default
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# import plotly.express as px # Import Plotly Express
# import plotly.graph_objects as go # Import Plotly Graph Objects
# import re # Import regex for extracting OC + number
# from datetime import datetime # Import datetime for current time
# import openai # Import OpenAI library
# import os # Import os for environment variables
# 
# 
# st.set_page_config(layout="wide") # Set wide layout for better use of space
# 
# # Configure OpenAI API key and initialize client
# # Make OpenAI client initialization conditional
# openai_api_key = os.getenv("OPENAI_API_KEY") # Try to get the key from environment variables first
# if openai_api_key is None:
#     try:
#         # If not in environment, try to get from Streamlit secrets
#         # This might still raise an error if secrets.toml is missing/empty,
#         # but the outer try-except will catch it.
#         openai_api_key = st.secrets["OPENAI_API_KEY"]
#     except Exception:
#         openai_api_key = None # Set to None if not found in secrets either
# 
# if openai_api_key:
#     openai.api_key = openai_api_key
#     client = openai.OpenAI(api_key=openai.api_key)
#     st.sidebar.success("OpenAI client initialized.")
# else:
#     st.sidebar.warning("OpenAI API key not found. OpenAI features will be disabled.")
#     client = None # Set client to None if key is not available
# 
# 
# @st.cache_data # Cache the data loading for performance
# def load_data():
#     try:
#         # Authenticate to Google Cloud
#         auth.authenticate_user()
#         creds, _ = default()
#         gc = gspread.authorize(creds)
#         B2sheet = gc.open('BI_B2')
#         page = B2sheet.sheet1
#         all_data = page.get_all_values()
#         df = pd.DataFrame(all_data[1:], columns=all_data[0])
# 
#         # Limpeza e prÃ©-processing
#         value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
#         for col in value_cols:
#             df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
#             df[col] = pd.to_numeric(df[col], errors='coerce')
# 
#         date_cols = ['Data de abertura', 'Data fechamento']
#         for col in date_cols:
#             df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')
# 
#         # Extract unique opportunity identifier (OC + number) from 'TÃ­tulo'
#         def extract_oc_identifier(title):
#             if isinstance(title, str):
#                 match = re.search(r'(OC\s*\d+)', title, re.IGNORECASE)
#                 if match:
#                     return match.group(1).replace(" ", "") # Remove space in "OC "
#                 else:
#                     # Also check for "CTE" as it appears in titles and might be identifiers
#                     match_cte = re.search(r'(CTE\s*\d+)', title, re.IGNORECASE)
#                     if match_cte:
#                          return match_cte.group(1).replace(" ", "")
#             return None # Return None if no pattern is found
# 
#         df['OC_Identifier'] = df['TÃ­tulo'].apply(extract_oc_identifier)
# 
# 
#         # Transformation and Feature Engineering for BI
#         # Ensure date columns are not NaT before accessing dt properties
#         df['Mes de Abertura'] = df['Data de abertura'].dt.month.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) else 0
#         df['Ano de Abertura'] = df['Data de abertura'].dt.year.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) else 0
#         df['Mes de Fechamento'] = df['Data fechamento'].dt.month.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data fechamento']) else 0
#         df['Ano de Fechamento'] = df['Data fechamento'].dt.year.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data fechamento']) else 0
# 
#         # Create a 'MonthYear' column for time series analysis
#         df['MonthYear_Abertura'] = df['Data de abertura'].dt.to_period('M') if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) else None
#         df['MonthYear_Fechamento'] = df['Data fechamento'].dt.to_period('M') if pd.api.types.is_datetime64_any_dtype(df['Data fechamento']) else None
# 
#         # Extract Hour of Day from 'Data de abertura'
#         # Ensure 'Data de abertura' is not NaT before extracting hour
#         # Only extract hour if Data de abertura is a valid datetime
#         df['Hour_of_Day_Abertura'] = df['Data de abertura'].apply(lambda x: x.hour if pd.notna(x) else -1).astype(int)
# 
# 
#         # Calculate Time in Stage for timeline analysis using OC_Identifier
#         df_timeline = df[['OC_Identifier', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()
#         # Drop rows where OC_Identifier could not be extracted or Data de abertura is missing
#         df_timeline.dropna(subset=['OC_Identifier', 'Data de abertura'], inplace=True)
#         df_timeline = df_timeline.sort_values(by=['OC_Identifier', 'Data de abertura'])
# 
#         current_time = pd.to_datetime('now') # Timezone-naive for consistency
#         df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600 # Time in hours
# 
#         df_timeline['Time_in_Stage'] = df_timeline.apply(
#             lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
#             axis=1
#         )
# 
#         # Format 'Time_in_Stage' for display to include minutes
#         def format_time_in_stage(hours):
#             if pd.isna(hours):
#                 return "N/A"
#             total_minutes = int(hours * 60)
#             days = total_minutes // (24 * 60)
#             remaining_minutes_after_days = total_minutes % (24 * 60)
#             hours = remaining_minutes_after_days // 60
#             minutes = remaining_minutes_after_days % 60
#             return f"{days} days, {hours} hours, {minutes} minutes"
# 
#         df_timeline['Time_in_Stage_Formatted'] = df_timeline['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#         return df, df_timeline
# 
#     except Exception as e:
#         st.error(f"Erro ao carregar dados do Google Sheet: {e}")
#         return pd.DataFrame(), pd.DataFrame() # Return empty dataframes on error
# 
# 
# # Load data
# df, df_timeline = load_data()
# 
# # --- Multi-page Navigation ---
# st.sidebar.title("NavegaÃ§Ã£o")
# page = st.sidebar.radio("Ir para:", ["Painel Geral", "RelatÃ³rio de Oportunidade"])
# 
# # --- Refresh Button ---
# if st.sidebar.button("Atualizar Dados"):
#     st.cache_data.clear() # Clear the cache for load_data
#     st.rerun() # Rerun the app to load fresh data
# 
# 
# # Main app logic within a try-except block for general errors
# try:
#     if page == "Painel Geral":
#         # --- Painel Geral ---
#         st.title("Painel de BI Operacional - Geral")
# 
#         st.markdown("""
#         Este painel apresenta insights gerais sobre os dados operacionais,
#         permitindo acompanhar o desempenho e a distribuiÃ§Ã£o dos negÃ³cios.
#         """)
# 
#         if not df.empty:
#             # Add filters to the sidebar
#             st.sidebar.subheader("Filtros do Painel Geral")
# 
#             # Date range filter for 'Data de abertura'
#             if not df['Data de abertura'].empty:
#                 min_date_abertura = df['Data de abertura'].min().date()
#                 max_date_abertura = df['Data de abertura'].max().date()
# 
#                 start_date = st.sidebar.date_input("Data de Abertura (InÃ­cio)", min_value=min_date_abertura, max_value=max_date_abertura, value=min_date_abertura)
#                 end_date = st.sidebar.date_input("Data de Abertura (Fim)", min_value=min_date_abertura, max_value=max_date_abertura, value=max_date_abertura)
# 
#                 # Convert selected dates to datetime objects for filtering
#                 start_datetime = pd.to_datetime(start_date)
#                 end_datetime = pd.to_datetime(end_date)
# 
#                 # Apply date filter
#                 filtered_df = df[(df['Data de abertura'] >= start_datetime) & (df['Data de abertura'] <= end_datetime)].copy()
#             else:
#                 filtered_df = df.copy() # If date column is empty, use the original df
# 
# 
#             # Filter by Estado
#             selected_estados = st.sidebar.multiselect("Selecionar Estado:", filtered_df['Estado'].unique(), filtered_df['Estado'].unique())
#             filtered_df = filtered_df[filtered_df['Estado'].isin(selected_estados)]
# 
#             # Filter by ResponsÃ¡vel (Initial filter)
#             selected_responsaveis_sidebar = st.sidebar.multiselect("Selecionar ResponsÃ¡vel (Filtro Inicial):", filtered_df['ResponsÃ¡vel'].unique(), filtered_df['ResponsÃ¡vel'].unique())
#             filtered_df = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_sidebar)]
# 
#             # Filter by EstÃ¡gio
#             selected_estagios = st.sidebar.multiselect("Selecionar EstÃ¡gio:", filtered_df['EstÃ¡gio'].unique(), filtered_df['EstÃ¡gio'].unique())
#             filtered_df = filtered_df[filtered_df['EstÃ¡gio'].isin(selected_estagios)]
# 
#             # Add filter by OC_Identifier for the general dashboard
#             opportunity_identifiers = filtered_df['OC_Identifier'].dropna().unique()
#             selected_opportunity_identifier_general = st.sidebar.selectbox(
#                 "Filtrar por Oportunidade (OC + NÃºmero ou CTE + NÃºmero):",
#                 ['Todos'] + list(opportunity_identifiers)
#             )
# 
#             if selected_opportunity_identifier_general != 'Todos':
#                  filtered_df = filtered_df[filtered_df['OC_Identifier'] == selected_opportunity_identifier_general].copy()
# 
# 
#             # AgregaÃ§Ãµes para visualizaÃ§Ãµes (using filtered_df)
#             # Aggregate to count unique OC_Identifier per ResponsÃ¡vel
#             df_agg_responsavel_count = filtered_df.groupby('ResponsÃ¡vel')['OC_Identifier'].nunique().reset_index()
#             df_agg_responsavel_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
# 
# 
#             # Aggregate to count unique OC_Identifier per Estado and MonthYear_Abertura
#             df_agg_estado_mes_count = filtered_df.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#             df_agg_estado_mes_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#             df_agg_estado_mes_count['MonthYear_Abertura'] = df_agg_estado_mes_count['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
# 
#             # Filter for 'Ganha' deals for specific visualizations
#             ganha_df_filtered = filtered_df[filtered_df['Estado'] == 'Ganha'].copy()
# 
#             # --- Display Key Metrics (KPIs) ---
#             st.subheader("Resumo Geral")
#             col_kpi1, col_kpi2, col_kpi3 = st.columns(3)
# 
#             total_opportunities = filtered_df['OC_Identifier'].nunique() if not filtered_df.empty else 0
#             total_won_value = ganha_df_filtered['Valor'].sum() if not ganha_df_filtered.empty else 0
#             win_rate = (len(ganha_df_filtered) / total_opportunities * 100) if total_opportunities > 0 else 0
# 
# 
#             col_kpi1.metric("Total Oportunidades Ãšnicas", total_opportunities)
#             col_kpi2.metric("Valor Total Ganho", f"R$ {total_won_value:,.2f}")
#             col_kpi3.metric("Taxa de Sucesso", f"{win_rate:.2f}%")
# 
# 
#             # --- Visualizations ---
#             st.subheader("AnÃ¡lise de Oportunidades e Valor")
# 
#             # Row 1: Value/Count by Responsible and Deals by State/Month
#             col1, col2 = st.columns(2)
# 
#             with col1:
#                 st.subheader("Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel")
#                 if not df_agg_responsavel_count.empty:
#                     # Using Plotly Express for interactivity
#                     fig1 = px.bar(df_agg_responsavel_count, x='ResponsÃ¡vel', y='Unique Opportunity Count',
#                                   title='Unique Opportunity Count per ResponsÃ¡vel')
# 
#                     # Capture selection
#                     selected_points = st.plotly_chart(fig1, use_container_width=True, on_select="rerun")
# 
#                     # Filter based on selection (using the original filtered_df and the selection from the chart)
#                     if selected_points and selected_points.selection and selected_points.selection.points:
#                          selected_responsaveis_chart = [point['x'] for point in selected_points.selection.points]
#                          filtered_df_chart_selection = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_chart)].copy()
#                     else:
#                          filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart selection
# 
#                 else:
#                     st.info("Nenhum dado disponÃ­vel para 'Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel' com os filtros selecionados.")
#                     filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart data
# 
# 
#             with col2:
#                  # Recalculate aggregated data based on chart selection
#                  df_agg_estado_mes_count_filtered = filtered_df_chart_selection.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#                  df_agg_estado_mes_count_filtered.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#                  df_agg_estado_mes_count_filtered['MonthYear_Abertura'] = df_agg_estado_mes_count_filtered['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
#                  st.subheader("Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura")
#                  if not df_agg_estado_mes_count_filtered.empty:
#                      # Using Plotly Express for the improved chart
#                      fig2 = px.bar(df_agg_estado_mes_count_filtered, x='MonthYear_Abertura', y='Unique Opportunity Count', color='Estado',
#                                    title='Unique Opportunity Count per Estado and Month of Opening (Filtered)',
#                                    barmode='group') # Use 'group' or 'stack'
# 
#                      fig2.update_layout(xaxis_title="MÃªs/Ano de Abertura", yaxis_title="Quantidade de Oportunidades Ãšnicas")
#                      st.plotly_chart(fig2, use_container_width=True)
# 
#                  else:
#                       st.info("Nenhum dado disponÃ­vel para 'Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura' com os filtros selecionados.")
# 
# 
#             # Row 2: Heatmap of Opportunities by Stage and Hour
#             st.subheader("Heatmap: Oportunidades por Etapa e Hora de Abertura")
#             if not df_timeline.empty:
#                 # Filter timeline data based on the date range selected for the main dashboard
#                 df_timeline_filtered_for_heatmap = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#                 # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#                 if selected_points and selected_points.selection and selected_points.selection.points:
#                      selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                      df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#                 df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
#                 # Filter heatmap data by selected OC_Identifier for the general dashboard
#                 if selected_opportunity_identifier_general != 'Todos':
#                      df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['OC_Identifier'] == selected_opportunity_identifier_general].copy()
# 
# 
#                 # Aggregate data for the heatmap (count unique OC_Identifier per EstÃ¡gio and Hour_of_Day_Abertura)
#                 # Ensure 'Hour_of_Day_Abertura' is extracted for the filtered timeline data
#                 df_timeline_filtered_for_heatmap['Hour_of_Day_Abertura'] = df_timeline_filtered_for_heatmap['Data de abertura'].dt.hour
# 
#                 if not df_timeline_filtered_for_heatmap.empty:
#                      heatmap_data = df_timeline_filtered_for_heatmap.groupby(['EstÃ¡gio', 'Hour_of_Day_Abertura'])['OC_Identifier'].nunique().unstack(fill_value=0)
# 
#                      if not heatmap_data.empty:
#                         # Use go.Heatmap to create the heatmap
#                         fig_heatmap = go.Figure(data=go.Heatmap(
#                                z=heatmap_data.values,
#                                x=heatmap_data.columns,
#                                y=heatmap_data.index,
#                                colorscale='Viridis')) # Use a color scale
# 
#                         fig_heatmap.update_layout(title='Opportunity Count by Stage and Hour of Opening (Filtered)',
#                                                   xaxis_title='Hour of Day',
#                                                   yaxis_title='Stage')
# 
#                         st.plotly_chart(fig_heatmap, use_container_width=True)
#                      else:
#                           st.info("Nenhum dado agregado disponÃ­vel para o Heatmap com os filtros selecionados.")
#                 else:
#                      st.info("Nenhum dado de timeline disponÃ­vel para o Heatmap com os filtros selecionados.")
#             else:
#                  st.info("Dados de timeline nÃ£o disponÃ­veis para o Heatmap.")
# 
# 
#         # Row 3: Distribution of EstÃ¡gios
#         st.subheader("AnÃ¡lise de EstÃ¡gios")
#         col5, col6 = st.columns(2)
# 
#         with col5:
#              # New Visualizations for 'Ganha' deals
#             if not ganha_df_filtered.empty:
#                 st.subheader("DistribuiÃ§Ã£o do Valor para NegÃ³cios Ganhos (Filtered)")
#                 fig3, ax3 = plt.subplots(figsize=(10, 6))
#                 sns.set_palette("viridis") # Set a color palette
#                 ax3.hist(ganha_df_filtered['Valor'].dropna(), bins=50)
#                 ax3.set_title('Distribution of Valor for Won Deals (Filtered)')
#                 ax3.set_xlabel('Valor')
#                 ax3.set_ylabel('Frequency')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig3)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para visualizaÃ§Ãµes de NegÃ³cios Ganhos com os filtros selecionados.")
# 
#         with col6:
#             if not filtered_df_chart_selection.empty:
#                 # Distribution of all stages (not just 'Ganha')
#                 st.subheader("DistribuiÃ§Ã£o de Todos os EstÃ¡gios (Filtered)")
#                 fig4, ax4 = plt.subplots(figsize=(12, 6))
#                 sns.set_palette("viridis") # Set a color palette
#                 filtered_df_chart_selection['EstÃ¡gio'].value_counts().plot(kind='bar', ax=ax4)
#                 ax4.set_title('Distribution of All Stages (Filtered)')
#                 ax4.set_xlabel('EstÃ¡gio')
#                 ax4.set_ylabel('Count')
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig4)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para 'DistribuiÃ§Ã£o de Todos os EstÃ¡gios' com os filtros selecionados.")
# 
# 
#         # Row 4: Timeline Analysis Metrics
#         st.subheader("AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio (Filtered)")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  # Filter timeline data based on selected OC_Identifiers from the chart selection
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
#             # Filter timeline data by selected OC_Identifier for the general dashboard
#             if selected_opportunity_identifier_general != 'Todos':
#                  df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['OC_Identifier'] == selected_opportunity_identifier_general].copy()
# 
# 
#             if not df_timeline_filtered.empty:
#                 df_agg_time_per_stage_avg = df_timeline_filtered.groupby('EstÃ¡gio')['Time_in_Stage'].mean().reset_index()
#                 df_agg_time_per_stage_avg = df_agg_time_per_stage_avg.sort_values(by='Time_in_Stage', ascending=False)
# 
#                 # Format 'Time_in_Stage' for display to include minutes
#                 def format_time_in_stage(hours):
#                     if pd.isna(hours):
#                         return "N/A"
#                     total_minutes = int(hours * 60)
#                     days = total_minutes // (24 * 60)
#                     remaining_minutes_after_days = total_minutes % (24 * 60)
#                     hours = remaining_minutes_after_days // 60
#                     minutes = remaining_minutes_after_days % 60
#                     return f"{days} days, {hours} hours, {minutes} minutes"
# 
#                 df_agg_time_per_stage_avg['Average Time in Stage'] = df_agg_time_per_stage_avg['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#                 st.write("Tempo MÃ©dio em Cada EstÃ¡gio:")
#                 st.dataframe(df_agg_time_per_stage_avg[['EstÃ¡gio', 'Average Time in Stage']])
# 
#                 # Visualization of average time per stage (using the numerical value)
#                 st.subheader("Tempo MÃ©dio por EstÃ¡gio VisualizaÃ§Ã£o (Filtered)")
#                 fig5, ax5 = plt.subplots(figsize=(14, 7))
#                 sns.set_palette("viridis") # Set a color palette
#                 sns.barplot(x='EstÃ¡gio', y='Time_in_Stage', data=df_agg_time_per_stage_avg, ax=ax5)
#                 ax5.set_title('Average Time per Stage (Filtered by Data de Abertura and Selection)')
#                 ax5.set_xlabel('EstÃ¡gio')
#                 ax5.set_ylabel('Average Time (hours)') # Keep y-axis as hours for the plot to work directly with numerical data
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig5)
#             else:
#                  st.info("Nenhum dado disponÃ­vel para AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio com os filtros selecionados.")
#         else:
#             st.info("Dados de timeline nÃ£o disponÃ­veis.")
# 
# 
#     elif page == "RelatÃ³rio de Oportunidade":
#         # --- RelatÃ³rio de Oportunidade Individual ---
#         st.title("RelatÃ³rio de Oportunidade Individual")
# 
#         st.markdown("""
#         Selecione um identificador de Oportunidade (OC + NÃºmero ou CTE + NÃºmero)
#         para visualizar sua linha do tempo e detalhes.
#         """)
# 
#         # Error handling for empty df or df_timeline
#         if df.empty or df_timeline.empty:
#              st.warning("Dados de oportunidade ou linha do tempo nÃ£o disponÃ­veis. Por favor, verifique a conexÃ£o com o Google Sheet.")
#         else:
#             try:
#                 # Use OC_Identifier for selection
#                 opportunity_identifiers = df['OC_Identifier'].dropna().unique()
# 
#                 if len(opportunity_identifiers) == 0:
#                      st.info("Nenhum identificador de oportunidade Ãºnico encontrado nos dados.")
#                 else:
#                     selected_opportunity_identifier = st.selectbox("Selecionar Oportunidade (OC + NÃºmero ou CTE + NÃºmero):", opportunity_identifiers)
# 
#                     st.subheader(f"Detalhes e Linha do Tempo para: {selected_opportunity_identifier}")
# 
#                     try:
#                         # Filter main df for the selected opportunity identifier
#                         opportunity_details_df = df[df['OC_Identifier'] == selected_opportunity_identifier]
# 
#                         if opportunity_details_df.empty:
#                             st.warning(f"Nenhum detalhe encontrado para: {selected_opportunity_identifier} no DataFrame principal.")
#                         else:
#                             opportunity_details = opportunity_details_df.iloc[0]
# 
#                             # Use columns for a structured layout
#                             col_info1, col_info2 = st.columns(2)
# 
#                             with col_info1:
#                                 st.write("**ID:**", opportunity_details.get('ID', 'N/A'))
#                                 st.write("**TÃ­tulo:**", opportunity_details.get('TÃ­tulo', 'N/A'))
#                                 st.write("**ResponsÃ¡vel:**", opportunity_details.get('ResponsÃ¡vel', 'N/A'))
#                                 st.write("**Estado:**", opportunity_details.get('Estado', 'N/A'))
#                                 st.write("**EstÃ¡gio Atual:**", opportunity_details.get('EstÃ¡gio', 'N/A')) # Display current stage
# 
#                             with col_info2:
#                                  # Add error handling for potential non-numeric 'Valor' before formatting
#                                  valor_display = "N/A"
#                                  if pd.notna(opportunity_details.get('Valor')) and pd.api.types.is_numeric_dtype(opportunity_details.get('Valor')):
#                                      valor_display = f"R$ {opportunity_details['Valor']:,.2f}"
#                                  st.write("**Valor:**", valor_display)
# 
#                                  st.write("**Origem:**", opportunity_details.get('Origem', 'N/A'))
#                                  st.write("**Prob %:**", opportunity_details.get('Prob %', 'N/A'))
#                                  st.write("**OC:**", opportunity_details.get('OC', 'N/A')) # Use .get for safer access
# 
# 
#                             st.subheader("Datas Principais")
#                             col_dates1, col_dates2 = st.columns(2)
#                             with col_dates1:
#                                  st.write("**Data de Abertura:**", opportunity_details['Data de abertura'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data de abertura']) else "N/A")
#                             with col_dates2:
#                                  st.write("**Data de Fechamento:**", opportunity_details['Data fechamento'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data fechamento']) else "N/A")
# 
#                             # Use an expander for Closing Details (if available)
#                             if pd.notna(opportunity_details.get('Data fechamento')):
#                                 with st.expander("Detalhes de Fechamento"):
#                                     # Add error handling for potential non-numeric closing values
#                                     valor_fechamento_display = "N/A"
#                                     if pd.notna(opportunity_details.get('Valor fechamento')) and pd.api.types.is_numeric_dtype(opportunity_details.get('Valor fechamento')):
#                                          valor_fechamento_display = f"R$ {opportunity_details['Valor fechamento']:,.2f}"
#                                     st.write("**Valor Fechamento:**", valor_fechamento_display)
# 
#                                     valor_rec_fechamento_display = "N/A"
#                                     if pd.notna(opportunity_details.get('Valor rec. fechamento')) and pd.api.types.is_numeric_dtype(opportunity_details.get('Valor rec. fechamento')):
#                                          valor_rec_fechamento_display = f"R$ {opportunity_details['Valor rec. fechamento']:,.2f}"
#                                     st.write("**Valor Rec. Fechamento:**", valor_rec_fechamento_display)
# 
#                                     st.write("**RazÃ£o de Fechamento:**", opportunity_details.get('RazÃ£o de fechamento', 'N/A')) # Use .get
#                                     st.write("**ObservaÃ§Ã£o de Fechamento:**", opportunity_details.get('ObservaÃ§Ã£o de fechamento', 'N/A')) # Use .get
# 
# 
#                             try:
#                                 # Filter timeline for the selected opportunity identifier
#                                 opportunity_timeline = df_timeline[df_timeline['OC_Identifier'] == selected_opportunity_identifier].copy()
# 
#                                 if not opportunity_timeline.empty:
#                                     st.subheader("Linha do Tempo da Oportunidade")
# 
#                                     # Display timeline using a more visually appealing table format
#                                     display_timeline_cols = ['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']
#                                     st.dataframe(opportunity_timeline[display_timeline_cols], key=f"timeline_data_{selected_opportunity_identifier}") # Add unique key
#                                 else:
#                                     st.info(f"Nenhum dado de linha do tempo encontrado para: {selected_opportunity_identifier}")
# 
#                             except Exception as e:
#                                 st.error(f"Erro ao processar ou exibir dados de linha do tempo para {selected_opportunity_identifier}: {e}")
# 
# 
#                             # --- AI Agent Interaction Section ---
#                             st.subheader("Assistente de IA para Oportunidade")
#                             # Removed the OpenAI client check here to always show the input area
#                             # Add a placeholder message if client is not initialized
#                             if st.session_state.get('ai_response') is None: # Check if session state has a response
#                                st.session_state['ai_response'] = "" # Initialize if not present
# 
#                             if client:
#                                 user_query = st.text_area(f"FaÃ§a uma pergunta sobre a oportunidade {selected_opportunity_identifier}:", height=100, key='user_query')
#                                 col_ai_button1, col_ai_button2 = st.columns(2)
# 
#                                 with col_ai_button1:
#                                     if st.button("Obter Resposta da IA", use_container_width=True):
#                                         if user_query:
#                                             with st.spinner("Obtendo resposta da IA..."):
#                                                 try:
#                                                     # Construct the prompt
#                                                     prompt = f"""
#                                                     VocÃª Ã© um assistente de BI focado em analisar dados de oportunidades de negÃ³cios.
#                                                     Sua tarefa Ã© responder a perguntas sobre uma oportunidade especÃ­fica com base nos dados fornecidos.
#                                                     Seja conciso e Ãºtil, focando em insights de BI e na progressÃ£o da oportunidade.
#                                                     **Use APENAS os dados fornecidos abaixo.**
#                                                     Se a pergunta do usuÃ¡rio nÃ£o puder ser respondida com os dados disponÃ­veis, diga isso de forma educada.
# 
#                                                     Dados da Oportunidade com identificador {selected_opportunity_identifier}:
# 
#                                                     Detalhes Principais:
#                                                     - ID: {opportunity_details.get('ID', 'N/A')}
#                                                     - TÃ­tulo: {opportunity_details.get('TÃ­tulo', 'N/A')}
#                                                     - ResponsÃ¡vel: {opportunity_details.get('ResponsÃ¡vel', 'N/A')}
#                                                     - Estado: {opportunity_details.get('Estado', 'N/A')}
#                                                     - EstÃ¡gio Atual: {opportunity_details.get('EstÃ¡gio', 'N/A')}
#                                                     - Valor: R$ {opportunity_details.get('Valor', 'N/A')}
#                                                     - Origem: {opportunity_details.get('Origem', 'N/A')}
#                                                     - Prob %: {opportunity_details.get('Prob %', 'N/A')}
#                                                     - OC: {opportunity_details.get('OC', 'N/A')}
#                                                     - Data de Abertura: {opportunity_details.get('Data de abertura', 'N/A')}
#                                                     - Data de Fechamento: {opportunity_details.get('Data fechamento', 'N/A')}
# 
#                                                     Detalhes de Fechamento (se aplicÃ¡vel):
#                                                     - Valor Fechamento: R$ {opportunity_details.get('Valor fechamento', 'N/A')}
#                                                     - Valor Rec. Fechamento: R$ {opportunity_details.get('Valor rec. fechamento', 'N/A')}
#                                                     - RazÃ£o de Fechamento: {opportunity_details.get('RazÃ£o de fechamento', 'N/A')}
#                                                     - ObservaÃ§Ã£o de Fechamento: {opportunity_details.get('ObservaÃ§Ã£o de fechamento', 'N/A')}
# 
#                                                     Linha do Tempo da Oportunidade (Progresso pelos EstÃ¡gios):
#                                                     {opportunity_timeline[['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']].to_string(index=False)}
# 
#                                                     Campos disponÃ­veis para anÃ¡lise nos Detalhes Principais: 'ID', 'TÃ­tulo', 'ResponsÃ¡vel', 'Estado', 'EstÃ¡gio', 'Valor', 'Origem', 'Prob %', 'OC', 'Data de Abertura', 'Data de Fechamento'.
#                                                     Campos disponÃ­veis para anÃ¡lise nos Detalhes de Fechamento: 'Valor Fechamento', 'Valor Rec. Fechamento', 'RazÃ£o de Fechamento', 'ObservaÃ§Ã£o de Fechamento'.
#                                                     Campos disponÃ­veis na Linha do Tempo: 'EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted'.
# 
#                                                     Pergunta do UsuÃ¡rio: {user_query}
#                                                     """
#                                                     response = client.chat.completions.create(
#                                                         model="gpt-4o-mini", # Or another suitable model
#                                                         messages=[
#                                                             {"role": "system", "content": "VocÃª Ã© um assistente de BI Ãºtil. Responda em PortuguÃªs."},
#                                                             {"role": "user", "content": prompt}
#                                                         ],
#                                                         max_tokens=500
#                                                     )
#                                                     st.session_state['ai_response'] = response.choices[0].message.content # Store response in session state
#                                                     st.rerun() # Rerun to display response immediately
#                                                 except Exception as e:
#                                                     st.error(f"Ocorreu um erro ao chamar a API da OpenAI: {e}")
#                                                     st.session_state['ai_response'] = f"Erro: {e}" # Store error message
#                                                     st.rerun()
#                                         else:
#                                             st.warning("Por favor, insira uma pergunta.")
# 
#                                 with col_ai_button2:
#                                     if st.button("Gerar Resumo da Oportunidade", use_container_width=True):
#                                          with st.spinner("Gerando resumo da oportunidade..."):
#                                             try:
#                                                 prompt = f"""
#                                                 VocÃª Ã© um assistente de BI focado em analisar dados de oportunidades de negÃ³cios.
#                                                 Sua tarefa Ã© gerar um resumo conciso e Ãºtil da oportunidade com o identificador {selected_opportunity_identifier}
#                                                 com base nos dados fornecidos abaixo.
#                                                 **Use APENAS os dados fornecidos.**
#                                                 Inclua detalhes importantes como o estado atual, valor, responsÃ¡vel e uma breve anÃ¡lise da linha do tempo, se aplicÃ¡vel.
# 
#                                                 Dados da Oportunidade com identificador {selected_opportunity_identifier}:
# 
#                                                 Detalhes Principais:
#                                                 - ID: {opportunity_details.get('ID', 'N/A')}
#                                                 - TÃ­tulo: {opportunity_details.get('TÃ­tulo', 'N/A')}
#                                                 - ResponsÃ¡vel: {opportunity_details.get('ResponsÃ¡vel', 'N/A')}
#                                                 - Estado: {opportunity_details.get('Estado', 'N/A')}
#                                                 - EstÃ¡gio Atual: {opportunity_details.get('EstÃ¡gio', 'N/A')}
#                                                 - Valor: R$ {opportunity_details.get('Valor', 'N/A')}
#                                                 - Origem: {opportunity_details.get('Origem', 'N/A')}
#                                                 - Prob %: {opportunity_details.get('Prob %', 'N/A')}
#                                                 - OC: {opportunity_details.get('OC', 'N/A')}
#                                                 - Data de Abertura: {opportunity_details.get('Data de abertura', 'N/A')}
#                                                 - Data de Fechamento: {opportunity_details.get('Data fechamento', 'N/A')}
# 
#                                                 Detalhes de Fechamento (se aplicÃ¡vel):
#                                                 - Valor Fechamento: R$ {opportunity_details.get('Valor fechamento', 'N/A')}
#                                                 - Valor Rec. Fechamento: R$ {opportunity_details.get('Valor rec. fechamento', 'N/A')}
#                                                 - RazÃ£o de Fechamento: {opportunity_details.get('RazÃ£o de fechamento', 'N/A')}
#                                                 - ObservaÃ§Ã£o de Fechamento: {opportunity_details.get('ObservaÃ§Ã£o de fechamento', 'N/A')}
# 
#                                                 Linha do Tempo da Oportunidade (Progresso pelos EstÃ¡gios):
#                                                 {opportunity_timeline[['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']].to_string(index=False)}
# 
#                                                 Gere o resumo agora.
#                                                 """
#                                                 response = client.chat.completions.create(
#                                                     model="gpt-4o-mini",
#                                                     messages=[
#                                                         {"role": "system", "content": "VocÃª Ã© um assistente de BI Ãºtil. Responda em PortuguÃªs."},
#                                                         {"role": "user", "content": prompt}
#                                                     ],
#                                                     max_tokens=500
#                                                 )
#                                                 st.session_state['ai_response'] = response.choices[0].message.content # Store response
#                                                 st.rerun() # Rerun to display
#                                             except Exception as e:
#                                                 st.error(f"Ocorreu um erro ao chamar a API da OpenAI para gerar o resumo: {e}")
#                                                 st.session_state['ai_response'] = f"Erro: {e}" # Store error message
#                                                 st.rerun()
#                             else:
#                                 st.warning("O cliente OpenAI nÃ£o estÃ¡ inicializado. A funcionalidade do assistente de IA estÃ¡ desativada.")
#                                 st.write(st.session_state.get('ai_response', '')) # Display any previous error/message
# 
#                             # Display AI response
#                             if st.session_state.get('ai_response'): # Check if there's a response to display
#                                  st.markdown("---") # Add a separator
#                                  st.markdown("**Resposta/Resumo do Assistente de IA:**")
#                                  st.markdown(st.session_state['ai_response'])
# 
#                     except Exception as e:
#                          st.error(f"Erro ao processar os detalhes da oportunidade {selected_opportunity_identifier}: {e}")
# 
#             except Exception as e:
#                  st.error(f"Erro ao carregar identificadores de oportunidade ou selecionar: {e}")
# 
# 
# except Exception as e:
#     st.error(f"Erro geral ao carregar ou processar dados: {e}")

"""# Task
Develop a modern BI dashboard and individual opportunity reports to track the timeline and progression of opportunities through different stages, including the time spent in each stage.

## Enhance the bi dashboard

### Subtask:
Improve the existing Streamlit dashboard with more sophisticated visualizations and potentially more interactive elements.

## Analyze opportunity timelines

### Subtask:
Develop a method to track and analyze the time spent in each 'EstÃ¡gio' for every opportunity, using the 'Data de abertura' and 'Data fechamento' columns and potentially inferring transitions based on changes in 'EstÃ¡gio' over time (if historical data is available or can be structured).

**Reasoning**:
Create a new DataFrame with relevant columns, sort it by ID and opening date, and calculate the time spent in each stage for each opportunity.
"""

df_timeline = df[['ID', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()

# Convert date columns to datetime objects
date_cols_timeline = ['Data de abertura', 'Data fechamento']
for col in date_cols_timeline:
    df_timeline[col] = pd.to_datetime(df_timeline[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')

df_timeline = df_timeline.sort_values(by=['ID', 'Data de abertura'])

# Calculate Time in Stage, handling potential NaT values after conversion
df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600 # Time in hours

display(df_timeline.head())
display(df_timeline.tail())

"""## Summary:

### Data Analysis Key Findings

* The dataset contains information on opportunities, including status, stage, value, and dates, but has data quality issues such as missing values in value columns and inconsistent formatting in the 'EstÃ¡gio' column.
* Value columns like 'Valor Rec.', 'Valor fechamento', and 'Valor rec. fechamento' contain mostly zero values, suggesting they might not be consistently populated or relevant for current value tracking.
* The majority of opportunities are in the 'Ganha' (Won) state.
* The timeline analysis successfully calculated the time spent in each stage for individual opportunities and identified the average time per stage, highlighting potential bottlenecks.
* The 'ResponsÃ¡vel' (Responsible) column shows varied activity levels among responsible parties.
* The stage with the highest average time in stage was identified as a potential bottleneck.
* The unique identifier for opportunities was confirmed to be the "OC + number" (or "CTE + number") found within the 'TÃ­tulo' column, which has been successfully extracted and used for timeline analysis and reporting.

### Capabilities of the BI Solution

* **Data Loading and Preprocessing:** The solution can connect to a Google Sheet, load data, and perform basic cleaning (handling currency symbols, converting data types, addressing missing values) to prepare it for analysis.
* **Interactive Dashboard:** Provides a dynamic dashboard with filters for date range ('Data de Abertura'), 'Estado', 'ResponsÃ¡vel', and 'EstÃ¡gio', allowing users to focus on specific subsets of data.
* **Interactive Visualizations:** Includes several charts (Total Value per Responsible, Deals per Estado and Month, Distribution of Value and Stages for Won Deals, Average Time per Stage) that update based on the selected filters and the interactive selection on the 'Total Value per Responsible' chart.
* **Multi-page Structure:** The dashboard is organized into a 'Painel Geral' for overall insights and a 'RelatÃ³rio de Oportunidade' for individual deal details, enhancing navigation and organization.
* **Individual Opportunity Reports:** Allows users to select a specific opportunity using its unique "OC + number" identifier and view its detailed progression through stages, including the time spent in each stage.
* **Timeline Analysis Integration:** Incorporates the analysis of time spent in each stage into the main dashboard, showing the average time per stage and highlighting potential bottlenecks.
* **Web-based Access:** Utilizes Streamlit to provide a user-friendly, web-based interface for accessing the BI insights via an ngrok tunnel with a wide layout.

### Next Steps & Potential Improvements

* **Standardize 'EstÃ¡gio' column:** Implement more robust data cleaning for the 'EstÃ¡gio' column to group similar stages and improve the accuracy of stage-based analysis and visualizations.
* **Investigate Value Columns:** Further investigate the 'Valor Rec.', 'Valor fechamento', and 'Valor rec. fechamento' columns to understand the reason for predominantly zero values and determine their relevance or the need for improved data collection.
* **Refine Appearance:** Further customize the appearance (e.g., color schemes, fonts, tooltips) to align with specific branding or aesthetic preferences.
* **Enhanced Timeline Visualization:** Explore more advanced ways to visualize the individual opportunity timelines (e.g., Gantt charts or similar representations).
* **Add More Key Metrics:** Include additional summary statistics or key performance indicators (KPIs) on the dashboard.

## Summary:

### Data Analysis Key Findings

* The dataset contains information on opportunities, including status, stage, value, and dates, but has data quality issues such as missing values in value columns and inconsistent formatting in the 'EstÃ¡gio' column.
* Value columns like 'Valor Rec.', 'Valor fechamento', and 'Valor rec. fechamento' contain mostly zero values, suggesting they might not be consistently populated or relevant for current value tracking.
* The majority of opportunities are in the 'Ganha' (Won) state.
* The timeline analysis successfully calculated the time spent in each stage for individual opportunities and identified the average time per stage, highlighting potential bottlenecks.
* The 'ResponsÃ¡vel' (Responsible) column shows varied activity levels among responsible parties.
* The stage with the highest average time in stage was identified as a potential bottleneck.

### Capabilities of the BI Solution

* **Data Loading and Preprocessing:** The solution can connect to a Google Sheet, load data, and perform basic cleaning (handling currency symbols, converting data types, addressing missing values) to prepare it for analysis.
* **Interactive Dashboard:** Provides a dynamic dashboard with filters for date range ('Data de Abertura'), 'Estado', and 'ResponsÃ¡vel', allowing users to focus on specific subsets of data.
* **Interactive Visualizations:** Includes several charts (Total Value per Responsible, Deals per Estado and Month, Distribution of Value and Stages for Won Deals, Average Time per Stage) that update based on the selected filters and the interactive selection on the 'Total Value per Responsible' chart.
* **Multi-page Structure:** The dashboard is organized into a 'Painel Geral' for overall insights and a 'RelatÃ³rio de Oportunidade' for individual deal details, enhancing navigation and organization.
* **Individual Opportunity Reports:** Allows users to select a specific opportunity ID and view its detailed progression through stages, including the time spent in each stage.
* **Timeline Analysis Integration:** Incorporates the analysis of time spent in each stage into the main dashboard, showing the average time per stage and highlighting potential bottlenecks.
* **Web-based Access:** Utilizes Streamlit to provide a user-friendly, web-based interface for accessing the BI insights via an ngrok tunnel.

### Next Steps & Potential Improvements

* **Standardize 'EstÃ¡gio' column:** Implement more robust data cleaning for the 'EstÃ¡gio' column to group similar stages and improve the accuracy of stage-based analysis and visualizations.
* **Investigate Value Columns:** Further investigate the 'Valor Rec.', 'Valor fechamento', and 'Valor rec. fechamento' columns to understand the reason for predominantly zero values and determine their relevance or the need for improved data collection.
* **Enhanced Interactivity:** Explore more advanced interactive features, such as linking selections on other charts or adding tooltips with more detailed information.
* **Additional Visualizations:** Consider adding more visualizations to explore other aspects of the data, such as the distribution of deals by 'Origem' or a heatmap showing deal volume by responsible and stage.
* **Refine Appearance:** Further customize the appearance (e.g., color schemes, fonts) to align with specific branding or aesthetic preferences.

**Reasoning**:
The previous step calculated the duration for each stage entry. Now, aggregate the time spent in each stage per opportunity ID by summing the 'Time_in_Stage' and handle missing 'Data de fechamento' values, assuming the opportunity is still in that stage.
"""

# For open opportunities (missing Data fechamento), calculate time in current stage until the current date
current_time = pd.to_datetime('now')
df_timeline['Time_in_Stage'] = df_timeline.apply(
    lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
    axis=1
)

# Aggregate time spent per stage for each opportunity
df_agg_time_per_stage = df_timeline.groupby(['ID', 'EstÃ¡gio'])['Time_in_Stage'].sum().reset_index()

display(df_agg_time_per_stage.head())

"""## Create individual opportunity reports

### Subtask:
Design a way to generate a detailed report for each opportunity, showing its progression through stages, the time spent in each stage, and relevant associated data.

**Reasoning**:
Incorporate the df_timeline DataFrame into the Streamlit app, add a select box for opportunity ID, filter the timeline data based on the selection, and display the filtered timeline.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from google.colab import auth
# import gspread
# from google.auth import default
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# 
# st.title("Painel de BI Operacional")
# 
# st.markdown("""
# Este painel apresenta insights sobre os dados operacionais,
# permitindo acompanhar o desempenho e a distribuiÃ§Ã£o dos negÃ³cios.
# """)
# 
# # Authenticate to Google Cloud
# # This part might behave differently in a deployed Streamlit app vs Colab
# # For Colab, authentication was handled in previous cells.
# # For deployment, you would typically use Streamlit Secrets.
# try:
#     auth.authenticate_user()
#     creds, _ = default()
#     gc = gspread.authorize(creds)
#     B2sheet = gc.open('BI_B2')
#     page = B2sheet.sheet1
#     all_data = page.get_all_values()
#     df = pd.DataFrame(all_data[1:], columns=all_data[0])
#     st.write("Dados carregados com sucesso!")
#     # st.dataframe(df.head()) # Optional: display head of raw data
# 
#     # Limpeza e prÃ©-processamento de dados
#     value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
#     for col in value_cols:
#         # Handle potential non-string values before applying string methods
#         df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
#         df[col] = pd.to_numeric(df[col], errors='coerce')
# 
#     date_cols = ['Data de abertura', 'Data fechamento']
#     for col in date_cols:
#         df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')
# 
#     # TransformaÃ§Ã£o de dados para BI
#     df['Mes de Abertura'] = df['Data de abertura'].dt.month
#     df['Ano de Abertura'] = df['Data de abertura'].dt.year
#     df['Mes de Fechamento'] = df['Data fechamento'].dt.month
#     df['Ano de Fechamento'] = df['Data fechamento'].dt.year
# 
# 
#     # Add filters to the sidebar
#     st.sidebar.subheader("Filtros do Painel")
# 
#     # Filter by Estado
#     selected_estados = st.sidebar.multiselect("Selecionar Estado:", df['Estado'].unique(), df['Estado'].unique())
#     filtered_df = df[df['Estado'].isin(selected_estados)]
# 
#     # Filter by ResponsÃ¡vel
#     selected_responsaveis = st.sidebar.multiselect("Selecionar ResponsÃ¡vel:", filtered_df['ResponsÃ¡vel'].unique(), filtered_df['ResponsÃ¡vel'].unique())
#     filtered_df = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis)]
# 
# 
#     # AgregaÃ§Ãµes para visualizaÃ§Ãµes (using filtered_df)
#     df_agg_responsavel = filtered_df.groupby('ResponsÃ¡vel')['Valor'].sum().reset_index()
# 
#     df_agg_estado_mes = filtered_df.groupby(['Estado', 'Mes de Abertura']).agg(
#         Total_Deals=('ID', 'count')
#     ).reset_index()
# 
#     # Filter for 'Ganha' deals for specific visualizations
#     ganha_df = filtered_df[filtered_df['Estado'] == 'Ganha'].copy()
# 
# 
#     # CriaÃ§Ã£o do painel de BI - VisualizaÃ§Ãµes
#     st.subheader("Valor Total por ResponsÃ¡vel")
#     fig1, ax1 = plt.subplots(figsize=(14, 7))
#     sns.barplot(x='ResponsÃ¡vel', y='Valor', data=df_agg_responsavel, ax=ax1)
#     ax1.set_title('Total Valor per ResponsÃ¡vel')
#     ax1.set_xlabel('ResponsÃ¡vel')
#     ax1.set_ylabel('Total Valor')
#     plt.xticks(rotation=45, ha='right')
#     st.pyplot(fig1)
# 
#     st.subheader("NegÃ³cios por Estado e MÃªs de Abertura")
#     fig2, ax2 = plt.subplots(figsize=(12, 7))
#     sns.barplot(x='Mes de Abertura', y='Total_Deals', hue='Estado', data=df_agg_estado_mes, ax=ax2)
#     ax2.set_title('Total Deals per Estado and Mes de Abertura')
#     ax2.set_xlabel('Mes de Abertura')
#     ax2.set_ylabel('Total Deals')
#     plt.xticks(rotation=0)
#     st.pyplot(fig2)
# 
#     # New Visualizations for 'Ganha' deals
#     if not ganha_df.empty:
#         st.subheader("DistribuiÃ§Ã£o do Valor para NegÃ³cios Ganhos")
#         fig3, ax3 = plt.subplots(figsize=(10, 6))
#         ax3.hist(ganha_df['Valor'].dropna(), bins=50)
#         ax3.set_title('Distribution of Valor for Won Deals')
#         ax3.set_xlabel('Valor')
#         ax3.set_ylabel('Frequency')
#         st.pyplot(fig3)
# 
#         st.subheader("DistribuiÃ§Ã£o de EstÃ¡gios para NegÃ³cios Ganhos")
#         fig4, ax4 = plt.subplots(figsize=(12, 6))
#         ganha_df['EstÃ¡gio'].value_counts().plot(kind='bar', ax=ax4)
#         ax4.set_title('Distribution of EstÃ¡gio for Won Deals')
#         ax4.set_xlabel('EstÃ¡gio')
#         ax4.set_ylabel('Count')
#         plt.xticks(rotation=45, ha='right')
#         plt.tight_layout()
#         st.pyplot(fig4)
# 
#     # Opportunity Report Section
#     st.sidebar.subheader("RelatÃ³rio de Oportunidade Individual")
#     opportunity_ids = df['ID'].unique()
#     selected_opportunity_id = st.sidebar.selectbox("Selecionar ID da Oportunidade:", opportunity_ids)
# 
#     st.subheader(f"RelatÃ³rio para Oportunidade ID: {selected_opportunity_id}")
# 
#     # Create df_timeline within the Streamlit app logic
#     df_timeline = df[['ID', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()
#     df_timeline = df_timeline.sort_values(by=['ID', 'Data de abertura'])
# 
#     # Calculate Time in Stage
#     current_time = pd.to_datetime('now', utc=True) # Use UTC for consistency if not specified
#     df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600 # Time in hours
# 
#     # Handle open opportunities (missing Data fechamento)
#     df_timeline['Time_in_Stage'] = df_timeline.apply(
#         lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
#         axis=1
#     )
# 
#     # Filter timeline for the selected opportunity ID
#     opportunity_timeline = df_timeline[df_timeline['ID'] == selected_opportunity_id].copy()
# 
#     if not opportunity_timeline.empty:
#         st.write("Progresso da Oportunidade:")
#         # Display relevant columns for the timeline
#         display_cols = ['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage']
#         st.dataframe(opportunity_timeline[display_cols])
# 
#         # Optionally, display other relevant details of the selected opportunity
#         opportunity_details = df[df['ID'] == selected_opportunity_id].iloc[0]
#         st.write("Detalhes da Oportunidade:")
#         st.write(opportunity_details[['TÃ­tulo', 'ResponsÃ¡vel', 'Estado', 'Valor']])
# 
#     else:
#         st.write("Nenhum dado de timeline encontrado para o ID selecionado.")
# 
# 
# except Exception as e:
#     st.error(f"Erro ao carregar dados ou processar: {e}")

"""## Integrate timeline insights into bi

### Subtask:
Incorporate key metrics derived from the timeline analysis (e.g., average time in each stage, bottlenecks) back into the main BI dashboard.

**Reasoning**:
Calculate the average time spent in each unique 'EstÃ¡gio' using the `df_agg_time_per_stage` DataFrame and identify the stage with the highest average time spent. Then, update the `app.py` to display these metrics and a visualization of average time per stage within the Streamlit dashboard.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from google.colab import auth
# import gspread
# from google.auth import default
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# 
# st.title("Painel de BI Operacional")
# 
# st.markdown("""
# Este painel apresenta insights sobre os dados operacionais,
# permitindo acompanhar o desempenho e a distribuiÃ§Ã£o dos negÃ³cios.
# """)
# 
# # Authenticate to Google Cloud
# # This part might behave differently in a deployed Streamlit app vs Colab
# # For Colab, authentication was handled in previous cells.
# # For deployment, you would typically use Streamlit Secrets.
# try:
#     auth.authenticate_user()
#     creds, _ = default()
#     gc = gspread.authorize(creds)
#     B2sheet = gc.open('BI_B2')
#     page = B2sheet.sheet1
#     all_data = page.get_all_values()
#     df = pd.DataFrame(all_data[1:], columns=all_data[0])
#     st.write("Dados carregados com sucesso!")
#     # st.dataframe(df.head()) # Optional: display head of raw data
# 
#     # Limpeza e prÃ©-processamento de dados
#     value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
#     for col in value_cols:
#         # Handle potential non-string values before applying string methods
#         df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
#         df[col] = pd.to_numeric(df[col], errors='coerce')
# 
#     date_cols = ['Data de abertura', 'Data fechamento']
#     for col in date_cols:
#         df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')
# 
#     # TransformaÃ§Ã£o de dados para BI
#     df['Mes de Abertura'] = df['Data de abertura'].dt.month
#     df['Ano de Abertura'] = df['Data de abertura'].dt.year
#     df['Mes de Fechamento'] = df['Data fechamento'].dt.month
#     df['Ano de Fechamento'] = df['Data fechamento'].dt.year
# 
# 
#     # Add filters to the sidebar
#     st.sidebar.subheader("Filtros do Painel")
# 
#     # Filter by Estado
#     selected_estados = st.sidebar.multiselect("Selecionar Estado:", df['Estado'].unique(), df['Estado'].unique())
#     filtered_df = df[df['Estado'].isin(selected_estados)]
# 
#     # Filter by ResponsÃ¡vel
#     selected_responsaveis = st.sidebar.multiselect("Selecionar ResponsÃ¡vel:", filtered_df['ResponsÃ¡vel'].unique(), filtered_df['ResponsÃ¡vel'].unique())
#     filtered_df = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis)]
# 
# 
#     # AgregaÃ§Ãµes para visualizaÃ§Ãµes (using filtered_df)
#     df_agg_responsavel = filtered_df.groupby('ResponsÃ¡vel')['Valor'].sum().reset_index()
# 
#     df_agg_estado_mes = filtered_df.groupby(['Estado', 'Mes de Abertura']).agg(
#         Total_Deals=('ID', 'count')
#     ).reset_index()
# 
#     # Filter for 'Ganha' deals for specific visualizations
#     ganha_df = filtered_df[filtered_df['Estado'] == 'Ganha'].copy()
# 
# 
#     # CriaÃ§Ã£o do painel de BI - VisualizaÃ§Ãµes
#     st.subheader("Valor Total por ResponsÃ¡vel")
#     fig1, ax1 = plt.subplots(figsize=(14, 7))
#     sns.barplot(x='ResponsÃ¡vel', y='Valor', data=df_agg_responsavel, ax=ax1)
#     ax1.set_title('Total Valor per ResponsÃ¡vel')
#     ax1.set_xlabel('ResponsÃ¡vel')
#     ax1.set_ylabel('Total Valor')
#     plt.xticks(rotation=45, ha='right')
#     st.pyplot(fig1)
# 
#     st.subheader("NegÃ³cios por Estado e MÃªs de Abertura")
#     fig2, ax2 = plt.subplots(figsize=(12, 7))
#     sns.barplot(x='Mes de Abertura', y='Total_Deals', hue='Estado', data=df_agg_estado_mes, ax=ax2)
#     ax2.set_title('Total Deals per Estado and Mes de Abertura')
#     ax2.set_xlabel('Mes de Abertura')
#     ax2.set_ylabel('Total Deals')
#     plt.xticks(rotation=0)
#     st.pyplot(fig2)
# 
#     # New Visualizations for 'Ganha' deals
#     if not ganha_df.empty:
#         st.subheader("DistribuiÃ§Ã£o do Valor para NegÃ³cios Ganhos")
#         fig3, ax3 = plt.subplots(figsize=(10, 6))
#         ax3.hist(ganha_df['Valor'].dropna(), bins=50)
#         ax3.set_title('Distribution of Valor for Won Deals')
#         ax3.set_xlabel('Valor')
#         ax3.set_ylabel('Frequency')
#         st.pyplot(fig3)
# 
#         st.subheader("DistribuiÃ§Ã£o de EstÃ¡gios para NegÃ³cios Ganhos")
#         fig4, ax4 = plt.subplots(figsize=(12, 6))
#         ganha_df['EstÃ¡gio'].value_counts().plot(kind='bar', ax=ax4)
#         ax4.set_title('Distribution of EstÃ¡gio for Won Deals')
#         ax4.set_xlabel('EstÃ¡gio')
#         ax4.set_ylabel('Count')
#         plt.xticks(rotation=45, ha='right')
#         plt.tight_layout()
#         st.pyplot(fig4)
# 
#     # Opportunity Report Section
#     st.sidebar.subheader("RelatÃ³rio de Oportunidade Individual")
#     opportunity_ids = df['ID'].unique()
#     selected_opportunity_id = st.sidebar.selectbox("Selecionar ID da Oportunidade:", opportunity_ids)
# 
#     st.subheader(f"RelatÃ³rio para Oportunidade ID: {selected_opportunity_id}")
# 
#     # Create df_timeline within the Streamlit app logic
#     df_timeline = df[['ID', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()
#     df_timeline = df_timeline.sort_values(by=['ID', 'Data de abertura'])
# 
#     # Calculate Time in Stage
#     current_time = pd.to_datetime('now', utc=True) # Use UTC for consistency if not specified
#     df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600 # Time in hours
# 
#     # Handle open opportunities (missing Data fechamento)
#     df_timeline['Time_in_Stage'] = df_timeline.apply(
#         lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
#         axis=1
#     )
# 
#     # Filter timeline for the selected opportunity ID
#     opportunity_timeline = df_timeline[df_timeline['ID'] == selected_opportunity_id].copy()
# 
#     if not opportunity_timeline.empty:
#         st.write("Progresso da Oportunidade:")
#         # Display relevant columns for the timeline
#         display_cols = ['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage']
#         st.dataframe(opportunity_timeline[display_cols])
# 
#         # Optionally, display other relevant details of the selected opportunity
#         opportunity_details = df[df['ID'] == selected_opportunity_id].iloc[0]
#         st.write("Detalhes da Oportunidade:")
#         st.write(opportunity_details[['TÃ­tulo', 'ResponsÃ¡vel', 'Estado', 'Valor']])
# 
#     else:
#         st.write("Nenhum dado de timeline encontrado para o ID selecionado.")
# 
#     # Timeline Analysis Metrics
#     st.subheader("AnÃ¡lise de Tempo por EstÃ¡gio")
# 
#     # Calculate average time per stage
#     df_agg_time_per_stage_avg = df_timeline.groupby('EstÃ¡gio')['Time_in_Stage'].mean().reset_index()
#     df_agg_time_per_stage_avg = df_agg_time_per_stage_avg.sort_values(by='Time_in_Stage', ascending=False)
# 
#     st.write("Tempo MÃ©dio em Cada EstÃ¡gio (em horas):")
#     st.dataframe(df_agg_time_per_stage_avg)
# 
#     # Identify bottleneck stage
#     if not df_agg_time_per_stage_avg.empty:
#         bottleneck_stage = df_agg_time_per_stage_avg.iloc[0]
#         st.warning(f"Potencial Gargalo (EstÃ¡gio com maior tempo mÃ©dio): **{bottleneck_stage['EstÃ¡gio']}** com tempo mÃ©dio de **{bottleneck_stage['Time_in_Stage']:.2f} horas**.")
# 
#         # Visualization of average time per stage
#         st.subheader("Tempo MÃ©dio por EstÃ¡gio")
#         fig5, ax5 = plt.subplots(figsize=(14, 7))
#         sns.barplot(x='EstÃ¡gio', y='Time_in_Stage', data=df_agg_time_per_stage_avg, ax=ax5)
#         ax5.set_title('Average Time per Stage')
#         ax5.set_xlabel('EstÃ¡gio')
#         ax5.set_ylabel('Average Time (hours)')
#         plt.xticks(rotation=45, ha='right')
#         plt.tight_layout()
#         st.pyplot(fig5)
# 
# 
# except Exception as e:
#     st.error(f"Erro ao carregar dados ou processar: {e}")

"""## Refine and present

### Subtask:
Review the dashboard and reports, make any necessary adjustments, and summarize the key findings and the capabilities of the new BI solution.

**Reasoning**:
Review the Streamlit app file (`app.py`) to evaluate the dashboard and reports, consider any necessary adjustments based on the previous analysis and the subtask requirements, and then summarize the key findings and capabilities of the BI solution.
"""

# Examine the content of app.py
with open('app.py', 'r') as f:
    app_code = f.read()

print("--- Content of app.py ---")
print(app_code)
print("-------------------------")

# Based on the code and previous analysis, summarize findings and capabilities.
print("\n--- Summary of Key Findings and BI Solution Capabilities ---")

print("\nKey Findings from the overall analysis:")
print("- The dataset contains information on opportunities, including their status, stage, value, and relevant dates.")
print("- There are missing values in the value columns, and these columns also contain non-numeric characters that require cleaning.")
print("- The 'EstÃ¡gio' column has a variety of entries, some with inconsistent formatting, requiring potential standardization for more detailed stage analysis.")
print("- The 'Valor Rec.', 'Valor fechamento', and 'Valor rec. fechamento' columns appear to have mostly zero values, suggesting they might not be consistently populated or relevant for current value tracking.")
print("- The majority of opportunities are in the 'Ganha' state.")
print("- The timeline analysis successfully calculated the time spent in each stage for individual opportunities and identified the average time per stage, highlighting potential bottlenecks.")
print("- The 'ResponsÃ¡vel' column shows a varied distribution of opportunities, indicating different levels of activity among responsible parties.")

print("\nCapabilities of the new BI solution:")
print("- **Data Loading and Preprocessing:** The solution can connect to a Google Sheet, load data, and perform basic cleaning (handling currency symbols, converting data types, addressing missing values) to prepare it for analysis.")
print("- **Interactive Dashboard:** Provides a dynamic dashboard with filters for 'Estado' and 'ResponsÃ¡vel', allowing users to focus on specific subsets of data.")
print("- **Key Metrics and Visualizations:** Displays visualizations such as total value per responsible party, deal count by state and month, distribution of value for won deals, and distribution of stages for won deals, offering insights into overall performance and trends.")
print("- **Timeline Analysis Integration:** Incorporates the analysis of time spent in each stage, showing the average time per stage and highlighting potential bottlenecks in the process.")
print("- **Individual Opportunity Reports:** Allows users to select a specific opportunity ID and view its detailed progression through stages, including the time spent in each stage, along with other relevant opportunity details.")
print("- **Reproducible Analysis:** The solution is implemented in a Python script, making the data processing and analysis steps transparent and reproducible.")
print("- **Web-based Access:** Utilizes Streamlit to provide a user-friendly, web-based interface for accessing the BI insights.")

print("\nPotential Adjustments (based on review):")
print("- **Standardize 'EstÃ¡gio' column:** Implement more robust cleaning for the 'EstÃ¡gio' column to group similar stages and improve the accuracy of stage-based analysis and visualizations.")
print("- **Investigate Value Columns:** Further investigate the 'Valor Rec.', 'Valor fechamento', and 'Valor rec. fechamento' columns to understand why they are mostly zero and determine if they should be excluded or if data collection needs improvement.")
print("- **Time Unit Flexibility:** Consider adding an option to display time in stage in different units (e.g., days, weeks) in addition to hours.")
print("- **More Granular Time Analysis:** If historical stage change data were available, a more precise timeline analysis tracking transitions between stages would be possible.")
print("- **Enhanced Filtering for Reports:** Allow filtering the individual opportunity report by criteria other than just ID (e.g., filter by responsible and then select from their opportunities).")

!streamlit run app.py &>/dev/null&

"""## Summary:

### Data Analysis Key Findings

*   The dataset contains information on opportunities, including status, stage, value, and dates, but has data quality issues such as missing values in value columns and inconsistent formatting in the 'EstÃ¡gio' column.
*   Value columns like 'Valor Rec.', 'Valor fechamento', and 'Valor rec. fechamento' contain mostly zero values.
*   The majority of opportunities are in the 'Ganha' (Won) state.
*   The timeline analysis successfully calculated the time spent in each stage for individual opportunities and identified the average time per stage.
*   The 'ResponsÃ¡vel' (Responsible) column shows varied activity levels among responsible parties.
*   The stage with the highest average time in stage was identified as a potential bottleneck.

### Insights or Next Steps

*   Implement robust data cleaning for the 'EstÃ¡gio' column to standardize entries and improve the accuracy of stage-based analysis and visualizations.
*   Investigate the value columns with predominantly zero values to understand the reason and determine their relevance or the need for improved data collection.
*   Consider adding flexibility to display time in stage in different units (e.g., days, weeks) to cater to different analysis needs.

# Task
Enhance the "RelatÃ³rio de Oportunidade" page, integrate an OpenAI agent using the provided API key to answer questions and generate reports about individual opportunities, and improve the overall user experience.

## Enhance "relatÃ³rio de oportunidade" page

### Subtask:
Improve the layout and presentation of the existing information for individual opportunities, potentially adding more relevant details if available in the data.

**Reasoning**:
Update the Streamlit app with improved layout, include additional relevant columns, format the output, and display the timeline data for individual opportunity reports.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from google.colab import auth
# import gspread
# from google.auth import default
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# import plotly.express as px # Import Plotly Express
# import plotly.graph_objects as go # Import Plotly Graph Objects
# import re # Import regex for extracting OC + number
# from datetime import datetime # Import datetime for current time
# 
# st.set_page_config(layout="wide") # Set wide layout for better use of space
# 
# # Authenticate to Google Cloud (handled outside this script in Colab)
# # For deployment, use Streamlit Secrets
# 
# @st.cache_data # Cache the data loading for performance
# def load_data():
#     try:
#         # Authenticate to Google Cloud
#         auth.authenticate_user()
#         creds, _ = default()
#         gc = gspread.authorize(creds)
#         B2sheet = gc.open('BI_B2')
#         page = B2sheet.sheet1
#         all_data = page.get_all_values()
#         df = pd.DataFrame(all_data[1:], columns=all_data[0])
# 
#         # Limpeza e prÃ©-processing
#         value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
#         for col in value_cols:
#             df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
#             df[col] = pd.to_numeric(df[col], errors='coerce')
# 
#         date_cols = ['Data de abertura', 'Data fechamento']
#         for col in date_cols:
#             df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')
# 
#         # Extract unique opportunity identifier (OC + number) from 'TÃ­tulo'
#         def extract_oc_identifier(title):
#             if isinstance(title, str):
#                 match = re.search(r'(OC\s*\d+)', title, re.IGNORECASE)
#                 if match:
#                     return match.group(1).replace(" ", "") # Remove space in "OC "
#                 else:
#                     # Also check for "CTE" as it appears in titles and might be identifiers
#                     match_cte = re.search(r'(CTE\s*\d+)', title, re.IGNORECASE)
#                     if match_cte:
#                          return match_cte.group(1).replace(" ", "")
#             return None # Return None if no pattern is found
# 
#         df['OC_Identifier'] = df['TÃ­tulo'].apply(extract_oc_identifier)
# 
# 
#         # Transformation and Feature Engineering for BI
#         # Ensure date columns are not NaT before accessing dt properties
#         df['Mes de Abertura'] = df['Data de abertura'].dt.month.fillna(0).astype(int)
#         df['Ano de Abertura'] = df['Data de abertura'].dt.year.fillna(0).astype(int)
#         df['Mes de Fechamento'] = df['Data fechamento'].dt.month.fillna(0).astype(int)
#         df['Ano de Fechamento'] = df['Data fechamento'].dt.year.fillna(0).astype(int)
# 
#         # Create a 'MonthYear' column for time series analysis
#         df['MonthYear_Abertura'] = df['Data de abertura'].dt.to_period('M')
#         df['MonthYear_Fechamento'] = df['Data fechamento'].dt.to_period('M')
# 
#         # Extract Hour of Day from 'Data de abertura'
#         df['Hour_of_Day_Abertura'] = df['Data de abertura'].dt.hour
# 
# 
#         # Calculate Time in Stage for timeline analysis using OC_Identifier
#         df_timeline = df[['OC_Identifier', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()
#         # Drop rows where OC_Identifier could not be extracted or Data de abertura is missing
#         df_timeline.dropna(subset=['OC_Identifier', 'Data de abertura'], inplace=True)
#         df_timeline = df_timeline.sort_values(by=['OC_Identifier', 'Data de abertura'])
# 
#         current_time = pd.to_datetime('now') # Timezone-naive for consistency
#         df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600 # Time in hours
# 
#         df_timeline['Time_in_Stage'] = df_timeline.apply(
#             lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
#             axis=1
#         )
# 
#         # Format 'Time_in_Stage' for display to include minutes
#         def format_time_in_stage(hours):
#             if pd.isna(hours):
#                 return "N/A"
#             total_minutes = int(hours * 60)
#             days = total_minutes // (24 * 60)
#             remaining_minutes_after_days = total_minutes % (24 * 60)
#             hours = remaining_minutes_after_days // 60
#             minutes = remaining_minutes_after_days % 60
#             return f"{days} days, {hours} hours, {minutes} minutes"
# 
#         df_timeline['Time_in_Stage_Formatted'] = df_timeline['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#         return df, df_timeline
# 
#     except Exception as e:
#         st.error(f"Erro ao carregar dados do Google Sheet: {e}")
#         return pd.DataFrame(), pd.DataFrame() # Return empty dataframes on error
# 
# 
# # Load data
# df, df_timeline = load_data()
# 
# # --- Multi-page Navigation ---
# st.sidebar.title("NavegaÃ§Ã£o")
# page = st.sidebar.radio("Ir para:", ["Painel Geral", "RelatÃ³rio de Oportunidade"])
# 
# # --- Refresh Button ---
# if st.sidebar.button("Atualizar Dados"):
#     st.cache_data.clear() # Clear the cache for load_data
#     st.rerun() # Rerun the app to load fresh data
# 
# 
# if page == "Painel Geral":
#     # --- Painel Geral ---
#     st.title("Painel de BI Operacional - Geral")
# 
#     st.markdown("""
#     Este painel apresenta insights gerais sobre os dados operacionais,
#     permitindo acompanhar o desempenho e a distribuiÃ§Ã£o dos negÃ³cios.
#     """)
# 
#     if not df.empty:
#         # Add filters to the sidebar
#         st.sidebar.subheader("Filtros do Painel Geral")
# 
#         # Date range filter for 'Data de abertura'
#         if not df['Data de abertura'].empty:
#             min_date_abertura = df['Data de abertura'].min().date()
#             max_date_abertura = df['Data de abertura'].max().date()
# 
#             start_date = st.sidebar.date_input("Data de Abertura (InÃ­cio)", min_value=min_date_abertura, max_value=max_date_abertura, value=min_date_abertura)
#             end_date = st.sidebar.date_input("Data de Abertura (Fim)", min_value=min_date_abertura, max_value=max_date_abertura, value=max_date_abertura)
# 
#             # Convert selected dates to datetime objects for filtering
#             start_datetime = pd.to_datetime(start_date)
#             end_datetime = pd.to_datetime(end_date)
# 
#             # Apply date filter
#             filtered_df = df[(df['Data de abertura'] >= start_datetime) & (df['Data de abertura'] <= end_datetime)].copy()
#         else:
#             filtered_df = df.copy() # If date column is empty, use the original df
# 
# 
#         # Filter by Estado
#         selected_estados = st.sidebar.multiselect("Selecionar Estado:", filtered_df['Estado'].unique(), filtered_df['Estado'].unique())
#         filtered_df = filtered_df[filtered_df['Estado'].isin(selected_estados)]
# 
#         # Filter by ResponsÃ¡vel (Initial filter)
#         selected_responsaveis_sidebar = st.sidebar.multiselect("Selecionar ResponsÃ¡vel (Filtro Inicial):", filtered_df['ResponsÃ¡vel'].unique(), filtered_df['ResponsÃ¡vel'].unique())
#         filtered_df = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_sidebar)]
# 
#         # Filter by EstÃ¡gio
#         selected_estagios = st.sidebar.multiselect("Selecionar EstÃ¡gio:", filtered_df['EstÃ¡gio'].unique(), filtered_df['EstÃ¡gio'].unique())
#         filtered_df = filtered_df[filtered_df['EstÃ¡gio'].isin(selected_estagios)]
# 
# 
#         # AgregaÃ§Ãµes para visualizaÃ§Ãµes (using filtered_df)
#         # Aggregate to count unique OC_Identifier per ResponsÃ¡vel
#         df_agg_responsavel_count = filtered_df.groupby('ResponsÃ¡vel')['OC_Identifier'].nunique().reset_index()
#         df_agg_responsavel_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
# 
# 
#         # Aggregate to count unique OC_Identifier per Estado and MonthYear_Abertura
#         df_agg_estado_mes_count = filtered_df.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#         df_agg_estado_mes_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#         df_agg_estado_mes_count['MonthYear_Abertura'] = df_agg_estado_mes_count['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
# 
#         # Filter for 'Ganha' deals for specific visualizations
#         ganha_df_filtered = filtered_df[filtered_df['Estado'] == 'Ganha'].copy()
# 
#         # --- Display Key Metrics (KPIs) ---
#         st.subheader("Resumo Geral")
#         col_kpi1, col_kpi2, col_kpi3 = st.columns(3)
# 
#         total_opportunities = filtered_df['OC_Identifier'].nunique() if not filtered_df.empty else 0
#         total_won_value = ganha_df_filtered['Valor'].sum() if not ganha_df_filtered.empty else 0
#         win_rate = (len(ganha_df_filtered) / total_opportunities * 100) if total_opportunities > 0 else 0
# 
# 
#         col_kpi1.metric("Total Oportunidades Ãšnicas", total_opportunities)
#         col_kpi2.metric("Valor Total Ganho", f"R$ {total_won_value:,.2f}")
#         col_kpi3.metric("Taxa de Sucesso", f"{win_rate:.2f}%")
# 
# 
#         # --- Visualizations ---
#         st.subheader("AnÃ¡lise de Oportunidades e Valor")
# 
#         # Row 1: Value/Count by Responsible and Deals by State/Month
#         col1, col2 = st.columns(2)
# 
#         with col1:
#             st.subheader("Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel")
#             if not df_agg_responsavel_count.empty:
#                 # Using Plotly Express for interactivity
#                 fig1 = px.bar(df_agg_responsavel_count, x='ResponsÃ¡vel', y='Unique Opportunity Count',
#                               title='Unique Opportunity Count per ResponsÃ¡vel')
# 
#                 # Capture selection
#                 selected_points = st.plotly_chart(fig1, use_container_width=True, on_select="rerun")
# 
#                 # Filter based on selection (using the original filtered_df and the selection from the chart)
#                 if selected_points and selected_points.selection and selected_points.selection.points:
#                      selected_responsaveis_chart = [point['x'] for point in selected_points.selection.points]
#                      filtered_df_chart_selection = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_chart)].copy()
#                 else:
#                      filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart selection
# 
#             else:
#                 st.info("Nenhum dado disponÃ­vel para 'Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel' com os filtros selecionados.")
#                 filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart data
# 
# 
#         with col2:
#              # Recalculate aggregated data based on chart selection
#              df_agg_estado_mes_count_filtered = filtered_df_chart_selection.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#              df_agg_estado_mes_count_filtered.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#              df_agg_estado_mes_count_filtered['MonthYear_Abertura'] = df_agg_estado_mes_count_filtered['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
#              st.subheader("Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura")
#              if not df_agg_estado_mes_count_filtered.empty:
#                  # Using Plotly Express for the improved chart
#                  fig2 = px.bar(df_agg_estado_mes_count_filtered, x='MonthYear_Abertura', y='Unique Opportunity Count', color='Estado',
#                                title='Unique Opportunity Count per Estado and Month of Opening (Filtered)',
#                                barmode='group') # Use 'group' or 'stack'
# 
#                  fig2.update_layout(xaxis_title="MÃªs/Ano de Abertura", yaxis_title="Quantidade de Oportunidades Ãšnicas")
#                  st.plotly_chart(fig2, use_container_width=True)
# 
#              else:
#                   st.info("Nenhum dado disponÃ­vel para 'Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura' com os filtros selecionados.")
# 
# 
#         # Row 2: Heatmap of Opportunities by Stage and Hour
#         st.subheader("Heatmap: Oportunidades por Etapa e Hora de Abertura")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered_for_heatmap = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
# 
#             # Aggregate data for the heatmap (count unique OC_Identifier per EstÃ¡gio and Hour_of_Day_Abertura)
#             # Ensure 'Hour_of_Day_Abertura' is extracted for the filtered timeline data
#             df_timeline_filtered_for_heatmap['Hour_of_Day_Abertura'] = df_timeline_filtered_for_heatmap['Data de abertura'].dt.hour
# 
#             if not df_timeline_filtered_for_heatmap.empty:
#                  heatmap_data = df_timeline_filtered_for_heatmap.groupby(['EstÃ¡gio', 'Hour_of_Day_Abertura'])['OC_Identifier'].nunique().unstack(fill_value=0)
# 
#                  if not heatmap_data.empty:
#                     # Use go.Heatmap to create the heatmap
#                     fig_heatmap = go.Figure(data=go.Heatmap(
#                            z=heatmap_data.values,
#                            x=heatmap_data.columns,
#                            y=heatmap_data.index,
#                            colorscale='Viridis')) # Use a color scale
# 
#                     fig_heatmap.update_layout(title='Opportunity Count by Stage and Hour of Opening (Filtered)',
#                                               xaxis_title='Hour of Day',
#                                               yaxis_title='Stage')
# 
#                     st.plotly_chart(fig_heatmap, use_container_width=True)
#                  else:
#                       st.info("Nenhum dado agregado disponÃ­vel para o Heatmap com os filtros selecionados.")
#             else:
#                  st.info("Nenhum dado de timeline disponÃ­vel para o Heatmap com os filtros selecionados.")
#         else:
#              st.info("Dados de timeline nÃ£o disponÃ­veis para o Heatmap.")
# 
# 
#         # Row 3: Distribution of EstÃ¡gios
#         st.subheader("AnÃ¡lise de EstÃ¡gios")
#         col5, col6 = st.columns(2)
# 
#         with col5:
#              # New Visualizations for 'Ganha' deals
#             if not ganha_df_filtered.empty:
#                 st.subheader("DistribuiÃ§Ã£o do Valor para NegÃ³cios Ganhos (Filtered)")
#                 fig3, ax3 = plt.subplots(figsize=(10, 6))
#                 sns.set_palette("viridis") # Set a color palette
#                 ax3.hist(ganha_df_filtered['Valor'].dropna(), bins=50)
#                 ax3.set_title('Distribution of Valor for Won Deals (Filtered)')
#                 ax3.set_xlabel('Valor')
#                 ax3.set_ylabel('Frequency')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig3)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para visualizaÃ§Ãµes de NegÃ³cios Ganhos com os filtros selecionados.")
# 
#         with col6:
#             if not filtered_df_chart_selection.empty:
#                 # Distribution of all stages (not just 'Ganha')
#                 st.subheader("DistribuiÃ§Ã£o de Todos os EstÃ¡gios (Filtered)")
#                 fig4, ax4 = plt.subplots(figsize=(12, 6))
#                 sns.set_palette("viridis") # Set a color palette
#                 filtered_df_chart_selection['EstÃ¡gio'].value_counts().plot(kind='bar', ax=ax4)
#                 ax4.set_title('Distribution of All Stages (Filtered)')
#                 ax4.set_xlabel('EstÃ¡gio')
#                 ax4.set_ylabel('Count')
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig4)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para 'DistribuiÃ§Ã£o de Todos os EstÃ¡gios' com os filtros selecionados.")
# 
# 
#         # Row 4: Timeline Analysis Metrics
#         st.subheader("AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio (Filtered)")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  # Filter timeline data based on selected OC_Identifiers from the chart selection
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
# 
#             if not df_timeline_filtered.empty:
#                 df_agg_time_per_stage_avg = df_timeline_filtered.groupby('EstÃ¡gio')['Time_in_Stage'].mean().reset_index()
#                 df_agg_time_per_stage_avg = df_agg_time_per_stage_avg.sort_values(by='Time_in_Stage', ascending=False)
# 
#                 # Format 'Time_in_Stage' for display to include minutes
#                 def format_time_in_stage(hours):
#                     if pd.isna(hours):
#                         return "N/A"
#                     total_minutes = int(hours * 60)
#                     days = total_minutes // (24 * 60)
#                     remaining_minutes_after_days = total_minutes % (24 * 60)
#                     hours = remaining_minutes_after_days // 60
#                     minutes = remaining_minutes_after_days % 60
#                     return f"{days} days, {hours} hours, {minutes} minutes"
# 
#                 df_agg_time_per_stage_avg['Average Time in Stage'] = df_agg_time_per_stage_avg['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#                 st.write("Tempo MÃ©dio em Cada EstÃ¡gio:")
#                 st.dataframe(df_agg_time_per_stage_avg[['EstÃ¡gio', 'Average Time in Stage']])
# 
#                 # Visualization of average time per stage (using the numerical value)
#                 st.subheader("Tempo MÃ©dio por EstÃ¡gio VisualizaÃ§Ã£o (Filtered)")
#                 fig5, ax5 = plt.subplots(figsize=(14, 7))
#                 sns.set_palette("viridis") # Set a color palette
#                 sns.barplot(x='EstÃ¡gio', y='Time_in_Stage', data=df_agg_time_per_stage_avg, ax=ax5)
#                 ax5.set_title('Average Time per Stage (Filtered by Data de Abertura and Selection)')
#                 ax5.set_xlabel('EstÃ¡gio')
#                 ax5.set_ylabel('Average Time (hours)') # Keep y-axis as hours for the plot to work directly with numerical data
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig5)
#             else:
#                  st.info("Nenhum dado disponÃ­vel para AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio com os filtros selecionados.")
#         else:
#             st.info("Dados de timeline nÃ£o disponÃ­veis.")
# 
# 
# elif page == "RelatÃ³rio de Oportunidade":
#     # --- RelatÃ³rio de Oportunidade Individual ---
#     st.title("RelatÃ³rio de Oportunidade Individual")
# 
#     st.markdown("""
#     Selecione um identificador de Oportunidade (OC + NÃºmero ou CTE + NÃºmero)
#     para visualizar sua linha do tempo e detalhes.
#     """)
# 
#     if not df.empty and not df_timeline.empty:
#         # Use OC_Identifier for selection
#         opportunity_identifiers = df['OC_Identifier'].dropna().unique()
#         selected_opportunity_identifier = st.selectbox("Selecionar Oportunidade (OC + NÃºmero ou CTE + NÃºmero):", opportunity_identifiers)
# 
#         st.subheader(f"Detalhes e Linha do Tempo para: {selected_opportunity_identifier}")
# 
#         # Filter main df for the selected opportunity identifier
#         opportunity_details_df = df[df['OC_Identifier'] == selected_opportunity_identifier]
# 
#         if not opportunity_details_df.empty:
#             opportunity_details = opportunity_details_df.iloc[0]
# 
#             # Use columns for a structured layout
#             col_info1, col_info2 = st.columns(2)
# 
#             with col_info1:
#                 st.write("**ID:**", opportunity_details['ID'])
#                 st.write("**TÃ­tulo:**", opportunity_details['TÃ­tulo'])
#                 st.write("**ResponsÃ¡vel:**", opportunity_details['ResponsÃ¡vel'])
#                 st.write("**Estado:**", opportunity_details['Estado'])
#                 st.write("**EstÃ¡gio Atual:**", opportunity_details['EstÃ¡gio']) # Display current stage
# 
#             with col_info2:
#                  st.write("**Valor:**", f"R$ {opportunity_details['Valor']:,.2f}" if pd.notna(opportunity_details['Valor']) else "N/A")
#                  st.write("**Origem:**", opportunity_details['Origem'])
#                  st.write("**Prob %:**", opportunity_details['Prob %'])
#                  st.write("**OC:**", opportunity_details['OC']) # Display OC column
# 
#             # Display Dates
#             st.subheader("Datas Principais")
#             col_dates1, col_dates2 = st.columns(2)
#             with col_dates1:
#                  st.write("**Data de Abertura:**", opportunity_details['Data de abertura'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data de abertura']) else "N/A")
#             with col_dates2:
#                  st.write("**Data de Fechamento:**", opportunity_details['Data fechamento'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data fechamento']) else "N/A")
# 
#             # Use an expander for Closing Details (if available)
#             if pd.notna(opportunity_details['Data fechamento']):
#                 with st.expander("Detalhes de Fechamento"):
#                     st.write("**Valor Fechamento:**", f"R$ {opportunity_details['Valor fechamento']:,.2f}" if pd.notna(opportunity_details['Valor fechamento']) else "N/A")
#                     st.write("**Valor Rec. Fechamento:**", f"R$ {opportunity_details['Valor rec. fechamento']:,.2f}" if pd.notna(opportunity_details['Valor rec. fechamento']) else "N/A")
#                     st.write("**RazÃ£o de Fechamento:**", opportunity_details['RazÃ£o de fechamento'])
#                     st.write("**ObservaÃ§Ã£o de Fechamento:**", opportunity_details['ObservaÃ§Ã£o de fechamento'])
# 
# 
#             # Filter timeline for the selected opportunity identifier
#             opportunity_timeline = df_timeline[df_timeline['OC_Identifier'] == selected_opportunity_identifier].copy()
# 
#             if not opportunity_timeline.empty:
#                 st.subheader("Linha do Tempo da Oportunidade")
# 
#                 # Display timeline using a more visually appealing table format
#                 display_timeline_cols = ['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']
#                 st.dataframe(opportunity_timeline[display_timeline_cols])
# 
# 
#             else:
#                 st.info(f"Nenhum dado de linha do tempo encontrado para: {selected_opportunity_identifier}")
# 
#         else:
#              st.info(f"Nenhum detalhe encontrado para: {selected_opportunity_identifier} no DataFrame principal.")
# 
#     elif df.empty:
#          st.warning("NÃ£o foi possÃ­vel carregar os dados. Verifique a conexÃ£o com o Google Sheet.")
#     else:
#          st.info("Dados de linha do tempo nÃ£o disponÃ­veis.")
# 
# except Exception as e:
#     st.error(f"Erro ao carregar dados ou processar: {e}")

"""## Set up openai api integration

### Subtask:
Add the necessary code to the Streamlit app to securely use your OpenAI API key and initialize the OpenAI client.

**Reasoning**:
Add the necessary imports and code to initialize the OpenAI client using Streamlit secrets in the app.py file.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from google.colab import auth
# import gspread
# from google.auth import default
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# import plotly.express as px # Import Plotly Express
# import plotly.graph_objects as go # Import Plotly Graph Objects
# import re # Import regex for extracting OC + number
# from datetime import datetime # Import datetime for current time
# import openai # Import the openai library
# 
# st.set_page_config(layout="wide") # Set wide layout for better use of space
# 
# # Configure OpenAI API key and initialize client
# if "OPENAI_API_KEY" in st.secrets:
#     openai.api_key = st.secrets["OPENAI_API_KEY"]
#     client = openai.OpenAI(api_key=openai.api_key)
#     st.sidebar.success("OpenAI client initialized.")
# else:
#     st.sidebar.warning("OpenAI API key not found in Streamlit secrets. OpenAI features will be disabled.")
#     client = None # Set client to None if key is not available
# 
# 
# @st.cache_data # Cache the data loading for performance
# def load_data():
#     try:
#         # Authenticate to Google Cloud
#         auth.authenticate_user()
#         creds, _ = default()
#         gc = gspread.authorize(creds)
#         B2sheet = gc.open('BI_B2')
#         page = B2sheet.sheet1
#         all_data = page.get_all_values()
#         df = pd.DataFrame(all_data[1:], columns=all_data[0])
# 
#         # Limpeza e prÃ©-processing
#         value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
#         for col in value_cols:
#             df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
#             df[col] = pd.to_numeric(df[col], errors='coerce')
# 
#         date_cols = ['Data de abertura', 'Data fechamento']
#         for col in date_cols:
#             df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')
# 
#         # Extract unique opportunity identifier (OC + number) from 'TÃ­tulo'
#         def extract_oc_identifier(title):
#             if isinstance(title, str):
#                 match = re.search(r'(OC\s*\d+)', title, re.IGNORECASE)
#                 if match:
#                     return match.group(1).replace(" ", "") # Remove space in "OC "
#                 else:
#                     # Also check for "CTE" as it appears in titles and might be identifiers
#                     match_cte = re.search(r'(CTE\s*\d+)', title, re.IGNORECASE)
#                     if match_cte:
#                          return match_cte.group(1).replace(" ", "")
#             return None # Return None if no pattern is found
# 
#         df['OC_Identifier'] = df['TÃ­tulo'].apply(extract_oc_identifier)
# 
# 
#         # Transformation and Feature Engineering for BI
#         # Ensure date columns are not NaT before accessing dt properties
#         df['Mes de Abertura'] = df['Data de abertura'].dt.month.fillna(0).astype(int)
#         df['Ano de Abertura'] = df['Data de abertura'].dt.year.fillna(0).astype(int)
#         df['Mes de Fechamento'] = df['Data fechamento'].dt.month.fillna(0).astype(int)
#         df['Ano de Fechamento'] = df['Data fechamento'].dt.year.fillna(0).astype(int)
# 
#         # Create a 'MonthYear' column for time series analysis
#         df['MonthYear_Abertura'] = df['Data de abertura'].dt.to_period('M')
#         df['MonthYear_Fechamento'] = df['Data fechamento'].dt.to_period('M')
# 
#         # Extract Hour of Day from 'Data de abertura'
#         df['Hour_of_Day_Abertura'] = df['Data de abertura'].dt.hour
# 
# 
#         # Calculate Time in Stage for timeline analysis using OC_Identifier
#         df_timeline = df[['OC_Identifier', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()
#         # Drop rows where OC_Identifier could not be extracted or Data de abertura is missing
#         df_timeline.dropna(subset=['OC_Identifier', 'Data de abertura'], inplace=True)
#         df_timeline = df_timeline.sort_values(by=['OC_Identifier', 'Data de abertura'])
# 
#         current_time = pd.to_datetime('now') # Timezone-naive for consistency
#         df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600 # Time in hours
# 
#         df_timeline['Time_in_Stage'] = df_timeline.apply(
#             lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
#             axis=1
#         )
# 
#         # Format 'Time_in_Stage' for display to include minutes
#         def format_time_in_stage(hours):
#             if pd.isna(hours):
#                 return "N/A"
#             total_minutes = int(hours * 60)
#             days = total_minutes // (24 * 60)
#             remaining_minutes_after_days = total_minutes % (24 * 60)
#             hours = remaining_minutes_after_days // 60
#             minutes = remaining_minutes_after_days % 60
#             return f"{days} days, {hours} hours, {minutes} minutes"
# 
#         df_timeline['Time_in_Stage_Formatted'] = df_timeline['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#         return df, df_timeline
# 
#     except Exception as e:
#         st.error(f"Erro ao carregar dados do Google Sheet: {e}")
#         return pd.DataFrame(), pd.DataFrame() # Return empty dataframes on error
# 
# 
# # Load data
# df, df_timeline = load_data()
# 
# # --- Multi-page Navigation ---
# st.sidebar.title("NavegaÃ§Ã£o")
# page = st.sidebar.radio("Ir para:", ["Painel Geral", "RelatÃ³rio de Oportunidade"])
# 
# # --- Refresh Button ---
# if st.sidebar.button("Atualizar Dados"):
#     st.cache_data.clear() # Clear the cache for load_data
#     st.rerun() # Rerun the app to load fresh data
# 
# 
# if page == "Painel Geral":
#     # --- Painel Geral ---
#     st.title("Painel de BI Operacional - Geral")
# 
#     st.markdown("""
#     Este painel apresenta insights gerais sobre os dados operacionais,
#     permitindo acompanhar o desempenho e a distribuiÃ§Ã£o dos negÃ³cios.
#     """)
# 
#     if not df.empty:
#         # Add filters to the sidebar
#         st.sidebar.subheader("Filtros do Painel Geral")
# 
#         # Date range filter for 'Data de abertura'
#         if not df['Data de abertura'].empty:
#             min_date_abertura = df['Data de abertura'].min().date()
#             max_date_abertura = df['Data de abertura'].max().date()
# 
#             start_date = st.sidebar.date_input("Data de Abertura (InÃ­cio)", min_value=min_date_abertura, max_value=max_date_abertura, value=min_date_abertura)
#             end_date = st.sidebar.date_input("Data de Abertura (Fim)", min_value=min_date_abertura, max_value=max_date_abertura, value=max_date_abertura)
# 
#             # Convert selected dates to datetime objects for filtering
#             start_datetime = pd.to_datetime(start_date)
#             end_datetime = pd.to_datetime(end_date)
# 
#             # Apply date filter
#             filtered_df = df[(df['Data de abertura'] >= start_datetime) & (df['Data de abertura'] <= end_datetime)].copy()
#         else:
#             filtered_df = df.copy() # If date column is empty, use the original df
# 
# 
#         # Filter by Estado
#         selected_estados = st.sidebar.multiselect("Selecionar Estado:", filtered_df['Estado'].unique(), filtered_df['Estado'].unique())
#         filtered_df = filtered_df[filtered_df['Estado'].isin(selected_estados)]
# 
#         # Filter by ResponsÃ¡vel (Initial filter)
#         selected_responsaveis_sidebar = st.sidebar.multiselect("Selecionar ResponsÃ¡vel (Filtro Inicial):", filtered_df['ResponsÃ¡vel'].unique(), filtered_df['ResponsÃ¡vel'].unique())
#         filtered_df = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_sidebar)]
# 
#         # Filter by EstÃ¡gio
#         selected_estagios = st.sidebar.multiselect("Selecionar EstÃ¡gio:", filtered_df['EstÃ¡gio'].unique(), filtered_df['EstÃ¡gio'].unique())
#         filtered_df = filtered_df[filtered_df['EstÃ¡gio'].isin(selected_estagios)]
# 
# 
#         # AgregaÃ§Ãµes para visualizaÃ§Ãµes (using filtered_df)
#         # Aggregate to count unique OC_Identifier per ResponsÃ¡vel
#         df_agg_responsavel_count = filtered_df.groupby('ResponsÃ¡vel')['OC_Identifier'].nunique().reset_index()
#         df_agg_responsavel_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
# 
# 
#         # Aggregate to count unique OC_Identifier per Estado and MonthYear_Abertura
#         df_agg_estado_mes_count = filtered_df.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#         df_agg_estado_mes_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#         df_agg_estado_mes_count['MonthYear_Abertura'] = df_agg_estado_mes_count['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
# 
#         # Filter for 'Ganha' deals for specific visualizations
#         ganha_df_filtered = filtered_df[filtered_df['Estado'] == 'Ganha'].copy()
# 
#         # --- Display Key Metrics (KPIs) ---
#         st.subheader("Resumo Geral")
#         col_kpi1, col_kpi2, col_kpi3 = st.columns(3)
# 
#         total_opportunities = filtered_df['OC_Identifier'].nunique() if not filtered_df.empty else 0
#         total_won_value = ganha_df_filtered['Valor'].sum() if not ganha_df_filtered.empty else 0
#         win_rate = (len(ganha_df_filtered) / total_opportunities * 100) if total_opportunities > 0 else 0
# 
# 
#         col_kpi1.metric("Total Oportunidades Ãšnicas", total_opportunities)
#         col_kpi2.metric("Valor Total Ganho", f"R$ {total_won_value:,.2f}")
#         col_kpi3.metric("Taxa de Sucesso", f"{win_rate:.2f}%")
# 
# 
#         # --- Visualizations ---
#         st.subheader("AnÃ¡lise de Oportunidades e Valor")
# 
#         # Row 1: Value/Count by Responsible and Deals by State/Month
#         col1, col2 = st.columns(2)
# 
#         with col1:
#             st.subheader("Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel")
#             if not df_agg_responsavel_count.empty:
#                 # Using Plotly Express for interactivity
#                 fig1 = px.bar(df_agg_responsavel_count, x='ResponsÃ¡vel', y='Unique Opportunity Count',
#                               title='Unique Opportunity Count per ResponsÃ¡vel')
# 
#                 # Capture selection
#                 selected_points = st.plotly_chart(fig1, use_container_width=True, on_select="rerun")
# 
#                 # Filter based on selection (using the original filtered_df and the selection from the chart)
#                 if selected_points and selected_points.selection and selected_points.selection.points:
#                      selected_responsaveis_chart = [point['x'] for point in selected_points.selection.points]
#                      filtered_df_chart_selection = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_chart)].copy()
#                 else:
#                      filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart selection
# 
#             else:
#                 st.info("Nenhum dado disponÃ­vel para 'Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel' com os filtros selecionados.")
#                 filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart data
# 
# 
#         with col2:
#              # Recalculate aggregated data based on chart selection
#              df_agg_estado_mes_count_filtered = filtered_df_chart_selection.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#              df_agg_estado_mes_count_filtered.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#              df_agg_estado_mes_count_filtered['MonthYear_Abertura'] = df_agg_estado_mes_count_filtered['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
#              st.subheader("Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura")
#              if not df_agg_estado_mes_count_filtered.empty:
#                  # Using Plotly Express for the improved chart
#                  fig2 = px.bar(df_agg_estado_mes_count_filtered, x='MonthYear_Abertura', y='Unique Opportunity Count', color='Estado',
#                                title='Unique Opportunity Count per Estado and Month of Opening (Filtered)',
#                                barmode='group') # Use 'group' or 'stack'
# 
#                  fig2.update_layout(xaxis_title="MÃªs/Ano de Abertura", yaxis_title="Quantidade de Oportunidades Ãšnicas")
#                  st.plotly_chart(fig2, use_container_width=True)
# 
#              else:
#                   st.info("Nenhum dado disponÃ­vel para 'Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura' com os filtros selecionados.")
# 
# 
#         # Row 2: Heatmap of Opportunities by Stage and Hour
#         st.subheader("Heatmap: Oportunidades por Etapa e Hora de Abertura")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered_for_heatmap = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
# 
#             # Aggregate data for the heatmap (count unique OC_Identifier per EstÃ¡gio and Hour_of_Day_Abertura)
#             # Ensure 'Hour_of_Day_Abertura' is extracted for the filtered timeline data
#             df_timeline_filtered_for_heatmap['Hour_of_Day_Abertura'] = df_timeline_filtered_for_heatmap['Data de abertura'].dt.hour
# 
#             if not df_timeline_filtered_for_heatmap.empty:
#                  heatmap_data = df_timeline_filtered_for_heatmap.groupby(['EstÃ¡gio', 'Hour_of_Day_Abertura'])['OC_Identifier'].nunique().unstack(fill_value=0)
# 
#                  if not heatmap_data.empty:
#                     # Use go.Heatmap to create the heatmap
#                     fig_heatmap = go.Figure(data=go.Heatmap(
#                            z=heatmap_data.values,
#                            x=heatmap_data.columns,
#                            y=heatmap_data.index,
#                            colorscale='Viridis')) # Use a color scale
# 
#                     fig_heatmap.update_layout(title='Opportunity Count by Stage and Hour of Opening (Filtered)',
#                                               xaxis_title='Hour of Day',
#                                               yaxis_title='Stage')
# 
#                     st.plotly_chart(fig_heatmap, use_container_width=True)
#                  else:
#                       st.info("Nenhum dado agregado disponÃ­vel para o Heatmap com os filtros selecionados.")
#             else:
#                  st.info("Nenhum dado de timeline disponÃ­vel para o Heatmap com os filtros selecionados.")
#         else:
#              st.info("Dados de timeline nÃ£o disponÃ­veis para o Heatmap.")
# 
# 
#         # Row 3: Distribution of EstÃ¡gios
#         st.subheader("AnÃ¡lise de EstÃ¡gios")
#         col5, col6 = st.columns(2)
# 
#         with col5:
#              # New Visualizations for 'Ganha' deals
#             if not ganha_df_filtered.empty:
#                 st.subheader("DistribuiÃ§Ã£o do Valor para NegÃ³cios Ganhos (Filtered)")
#                 fig3, ax3 = plt.subplots(figsize=(10, 6))
#                 sns.set_palette("viridis") # Set a color palette
#                 ax3.hist(ganha_df_filtered['Valor'].dropna(), bins=50)
#                 ax3.set_title('Distribution of Valor for Won Deals (Filtered)')
#                 ax3.set_xlabel('Valor')
#                 ax3.set_ylabel('Frequency')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig3)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para visualizaÃ§Ãµes de NegÃ³cios Ganhos com os filtros selecionados.")
# 
#         with col6:
#             if not filtered_df_chart_selection.empty:
#                 # Distribution of all stages (not just 'Ganha')
#                 st.subheader("DistribuiÃ§Ã£o de Todos os EstÃ¡gios (Filtered)")
#                 fig4, ax4 = plt.subplots(figsize=(12, 6))
#                 sns.set_palette("viridis") # Set a color palette
#                 filtered_df_chart_selection['EstÃ¡gio'].value_counts().plot(kind='bar', ax=ax4)
#                 ax4.set_title('Distribution of All Stages (Filtered)')
#                 ax4.set_xlabel('EstÃ¡gio')
#                 ax4.set_ylabel('Count')
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig4)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para 'DistribuiÃ§Ã£o de Todos os EstÃ¡gios' com os filtros selecionados.")
# 
# 
#         # Row 4: Timeline Analysis Metrics
#         st.subheader("AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio (Filtered)")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  # Filter timeline data based on selected OC_Identifiers from the chart selection
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
# 
#             if not df_timeline_filtered.empty:
#                 df_agg_time_per_stage_avg = df_timeline_filtered.groupby('EstÃ¡gio')['Time_in_Stage'].mean().reset_index()
#                 df_agg_time_per_stage_avg = df_agg_time_per_stage_avg.sort_values(by='Time_in_Stage', ascending=False)
# 
#                 # Format 'Time_in_Stage' for display to include minutes
#                 def format_time_in_stage(hours):
#                     if pd.isna(hours):
#                         return "N/A"
#                     total_minutes = int(hours * 60)
#                     days = total_minutes // (24 * 60)
#                     remaining_minutes_after_days = total_minutes % (24 * 60)
#                     hours = remaining_minutes_after_days // 60
#                     minutes = remaining_minutes_after_days % 60
#                     return f"{days} days, {hours} hours, {minutes} minutes"
# 
#                 df_agg_time_per_stage_avg['Average Time in Stage'] = df_agg_time_per_stage_avg['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#                 st.write("Tempo MÃ©dio em Cada EstÃ¡gio:")
#                 st.dataframe(df_agg_time_per_stage_avg[['EstÃ¡gio', 'Average Time in Stage']])
# 
#                 # Visualization of average time per stage (using the numerical value)
#                 st.subheader("Tempo MÃ©dio por EstÃ¡gio VisualizaÃ§Ã£o (Filtered)")
#                 fig5, ax5 = plt.subplots(figsize=(14, 7))
#                 sns.set_palette("viridis") # Set a color palette
#                 sns.barplot(x='EstÃ¡gio', y='Time_in_Stage', data=df_agg_time_per_stage_avg, ax=ax5)
#                 ax5.set_title('Average Time per Stage (Filtered by Data de Abertura and Selection)')
#                 ax5.set_xlabel('EstÃ¡gio')
#                 ax5.set_ylabel('Average Time (hours)') # Keep y-axis as hours for the plot to work directly with numerical data
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig5)
#             else:
#                  st.info("Nenhum dado disponÃ­vel para AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio com os filtros selecionados.")
#         else:
#             st.info("Dados de timeline nÃ£o disponÃ­veis.")
# 
# 
# elif page == "RelatÃ³rio de Oportunidade":
#     # --- RelatÃ³rio de Oportunidade Individual ---
#     st.title("RelatÃ³rio de Oportunidade Individual")
# 
#     st.markdown("""
#     Selecione um identificador de Oportunidade (OC + NÃºmero ou CTE + NÃºmero)
#     para visualizar sua linha do tempo e detalhes.
#     """)
# 
#     if not df.empty and not df_timeline.empty:
#         # Use OC_Identifier for selection
#         opportunity_identifiers = df['OC_Identifier'].dropna().unique()
#         selected_opportunity_identifier = st.selectbox("Selecionar Oportunidade (OC + NÃºmero ou CTE + NÃºmero):", opportunity_identifiers)
# 
#         st.subheader(f"Detalhes e Linha do Tempo para: {selected_opportunity_identifier}")
# 
#         # Filter main df for the selected opportunity identifier
#         opportunity_details_df = df[df['OC_Identifier'] == selected_opportunity_identifier]
# 
#         if not opportunity_details_df.empty:
#             opportunity_details = opportunity_details_df.iloc[0]
# 
#             # Use columns for a structured layout
#             col_info1, col_info2 = st.columns(2)
# 
#             with col_info1:
#                 st.write("**ID:**", opportunity_details['ID'])
#                 st.write("**TÃ­tulo:**", opportunity_details['TÃ­tulo'])
#                 st.write("**ResponsÃ¡vel:**", opportunity_details['ResponsÃ¡vel'])
#                 st.write("**Estado:**", opportunity_details['Estado'])
#                 st.write("**EstÃ¡gio Atual:**", opportunity_details['EstÃ¡gio']) # Display current stage
# 
#             with col_info2:
#                  st.write("**Valor:**", f"R$ {opportunity_details['Valor']:,.2f}" if pd.notna(opportunity_details['Valor']) else "N/A")
#                  st.write("**Origem:**", opportunity_details['Origem'])
#                  st.write("**Prob %:**", opportunity_details['Prob %'])
#                  st.write("**OC:**", opportunity_details['OC']) # Display OC column
# 
#             # Display Dates
#             st.subheader("Datas Principais")
#             col_dates1, col_dates2 = st.columns(2)
#             with col_dates1:
#                  st.write("**Data de Abertura:**", opportunity_details['Data de abertura'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data de abertura']) else "N/A")
#             with col_dates2:
#                  st.write("**Data de Fechamento:**", opportunity_details['Data fechamento'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data fechamento']) else "N/A")
# 
#             # Use an expander for Closing Details (if available)
#             if pd.notna(opportunity_details['Data fechamento']):
#                 with st.expander("Detalhes de Fechamento"):
#                     st.write("**Valor Fechamento:**", f"R$ {opportunity_details['Valor fechamento']:,.2f}" if pd.notna(opportunity_details['Valor fechamento']) else "N/A")
#                     st.write("**Valor Rec. Fechamento:**", f"R$ {opportunity_details['Valor rec. fechamento']:,.2f}" if pd.notna(opportunity_details['Valor rec. fechamento']) else "N/A")
#                     st.write("**RazÃ£o de Fechamento:**", opportunity_details['RazÃ£o de fechamento'])
#                     st.write("**ObservaÃ§Ã£o de Fechamento:**", opportunity_details['ObservaÃ§Ã£o de fechamento'])
# 
# 
#             # Filter timeline for the selected opportunity identifier
#             opportunity_timeline = df_timeline[df_timeline['OC_Identifier'] == selected_opportunity_identifier].copy()
# 
#             if not opportunity_timeline.empty:
#                 st.subheader("Linha do Tempo da Oportunidade")
# 
#                 # Display timeline using a more visually appealing table format
#                 display_timeline_cols = ['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']
#                 st.dataframe(opportunity_timeline[display_timeline_cols])
# 
# 
#             else:
#                 st.info(f"Nenhum dado de linha do tempo encontrado para: {selected_opportunity_identifier}")
# 
#         else:
#              st.info(f"Nenhum detalhe encontrado para: {selected_opportunity_identifier} no DataFrame principal.")
# 
#     elif df.empty:
#          st.warning("NÃ£o foi possÃ­vel carregar os dados. Verifique a conexÃ£o com o Google Sheet.")
#     else:
#          st.info("Dados de linha do tempo nÃ£o disponÃ­veis.")
# 
# except Exception as e:
#     st.error(f"Erro ao carregar dados ou processar: {e}")

"""## Design agent interaction ui

### Subtask:
Design agent interaction ui

**Reasoning**:
Add a text area for user input and a button to trigger the AI agent interaction on the "RelatÃ³rio de Oportunidade" page, ensuring they are only displayed if the OpenAI client is successfully initialized.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from google.colab import auth
# import gspread
# from google.auth import default
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# import plotly.express as px # Import Plotly Express
# import plotly.graph_objects as go # Import Plotly Graph Objects
# import re # Import regex for extracting OC + number
# from datetime import datetime # Import datetime for current time
# import openai # Import the openai library
# 
# st.set_page_config(layout="wide") # Set wide layout for better use of space
# 
# # Configure OpenAI API key and initialize client
# if "OPENAI_API_KEY" in st.secrets:
#     openai.api_key = st.secrets["OPENAI_API_KEY"]
#     client = openai.OpenAI(api_key=openai.api_key)
#     st.sidebar.success("OpenAI client initialized.")
# else:
#     st.sidebar.warning("OpenAI API key not found in Streamlit secrets. OpenAI features will be disabled.")
#     client = None # Set client to None if key is not available
# 
# 
# @st.cache_data # Cache the data loading for performance
# def load_data():
#     try:
#         # Authenticate to Google Cloud
#         auth.authenticate_user()
#         creds, _ = default()
#         gc = gspread.authorize(creds)
#         B2sheet = gc.open('BI_B2')
#         page = B2sheet.sheet1
#         all_data = page.get_all_values()
#         df = pd.DataFrame(all_data[1:], columns=all_data[0])
# 
#         # Limpeza e prÃ©-processing
#         value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
#         for col in value_cols:
#             df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
#             df[col] = pd.to_numeric(df[col], errors='coerce')
# 
#         date_cols = ['Data de abertura', 'Data fechamento']
#         for col in date_cols:
#             df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')
# 
#         # Extract unique opportunity identifier (OC + number) from 'TÃ­tulo'
#         def extract_oc_identifier(title):
#             if isinstance(title, str):
#                 match = re.search(r'(OC\s*\d+)', title, re.IGNORECASE)
#                 if match:
#                     return match.group(1).replace(" ", "") # Remove space in "OC "
#                 else:
#                     # Also check for "CTE" as it appears in titles and might be identifiers
#                     match_cte = re.search(r'(CTE\s*\d+)', title, re.IGNORECASE)
#                     if match_cte:
#                          return match_cte.group(1).replace(" ", "")
#             return None # Return None if no pattern is found
# 
#         df['OC_Identifier'] = df['TÃ­tulo'].apply(extract_oc_identifier)
# 
# 
#         # Transformation and Feature Engineering for BI
#         # Ensure date columns are not NaT before accessing dt properties
#         df['Mes de Abertura'] = df['Data de abertura'].dt.month.fillna(0).astype(int)
#         df['Ano de Abertura'] = df['Data de abertura'].dt.year.fillna(0).astype(int)
#         df['Mes de Fechamento'] = df['Data fechamento'].dt.month.fillna(0).astype(int)
#         df['Ano de Fechamento'] = df['Data fechamento'].dt.year.fillna(0).astype(int)
# 
#         # Create a 'MonthYear' column for time series analysis
#         df['MonthYear_Abertura'] = df['Data de abertura'].dt.to_period('M')
#         df['MonthYear_Fechamento'] = df['Data fechamento'].dt.to_period('M')
# 
#         # Extract Hour of Day from 'Data de abertura'
#         df['Hour_of_Day_Abertura'] = df['Data de abertura'].dt.hour
# 
# 
#         # Calculate Time in Stage for timeline analysis using OC_Identifier
#         df_timeline = df[['OC_Identifier', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()
#         # Drop rows where OC_Identifier could not be extracted or Data de abertura is missing
#         df_timeline.dropna(subset=['OC_Identifier', 'Data de abertura'], inplace=True)
#         df_timeline = df_timeline.sort_values(by=['OC_Identifier', 'Data de abertura'])
# 
#         current_time = pd.to_datetime('now') # Timezone-naive for consistency
#         df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600 # Time in hours
# 
#         df_timeline['Time_in_Stage'] = df_timeline.apply(
#             lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
#             axis=1
#         )
# 
#         # Format 'Time_in_Stage' for display to include minutes
#         def format_time_in_stage(hours):
#             if pd.isna(hours):
#                 return "N/A"
#             total_minutes = int(hours * 60)
#             days = total_minutes // (24 * 60)
#             remaining_minutes_after_days = total_minutes % (24 * 60)
#             hours = remaining_minutes_after_days // 60
#             minutes = remaining_minutes_after_days % 60
#             return f"{days} days, {hours} hours, {minutes} minutes"
# 
#         df_timeline['Time_in_Stage_Formatted'] = df_timeline['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#         return df, df_timeline
# 
#     except Exception as e:
#         st.error(f"Erro ao carregar dados do Google Sheet: {e}")
#         return pd.DataFrame(), pd.DataFrame() # Return empty dataframes on error
# 
# 
# # Load data
# df, df_timeline = load_data()
# 
# # --- Multi-page Navigation ---
# st.sidebar.title("NavegaÃ§Ã£o")
# page = st.sidebar.radio("Ir para:", ["Painel Geral", "RelatÃ³rio de Oportunidade"])
# 
# # --- Refresh Button ---
# if st.sidebar.button("Atualizar Dados"):
#     st.cache_data.clear() # Clear the cache for load_data
#     st.rerun() # Rerun the app to load fresh data
# 
# 
# if page == "Painel Geral":
#     # --- Painel Geral ---
#     st.title("Painel de BI Operacional - Geral")
# 
#     st.markdown("""
#     Este painel apresenta insights gerais sobre os dados operacionais,
#     permitindo acompanhar o desempenho e a distribuiÃ§Ã£o dos negÃ³cios.
#     """)
# 
#     if not df.empty:
#         # Add filters to the sidebar
#         st.sidebar.subheader("Filtros do Painel Geral")
# 
#         # Date range filter for 'Data de abertura'
#         if not df['Data de abertura'].empty:
#             min_date_abertura = df['Data de abertura'].min().date()
#             max_date_abertura = df['Data de abertura'].max().date()
# 
#             start_date = st.sidebar.date_input("Data de Abertura (InÃ­cio)", min_value=min_date_abertura, max_value=max_date_abertura, value=min_date_abertura)
#             end_date = st.sidebar.date_input("Data de Abertura (Fim)", min_value=min_date_abertura, max_value=max_date_abertura, value=max_date_abertura)
# 
#             # Convert selected dates to datetime objects for filtering
#             start_datetime = pd.to_datetime(start_date)
#             end_datetime = pd.to_datetime(end_date)
# 
#             # Apply date filter
#             filtered_df = df[(df['Data de abertura'] >= start_datetime) & (df['Data de abertura'] <= end_datetime)].copy()
#         else:
#             filtered_df = df.copy() # If date column is empty, use the original df
# 
# 
#         # Filter by Estado
#         selected_estados = st.sidebar.multiselect("Selecionar Estado:", filtered_df['Estado'].unique(), filtered_df['Estado'].unique())
#         filtered_df = filtered_df[filtered_df['Estado'].isin(selected_estados)]
# 
#         # Filter by ResponsÃ¡vel (Initial filter)
#         selected_responsaveis_sidebar = st.sidebar.multiselect("Selecionar ResponsÃ¡vel (Filtro Inicial):", filtered_df['ResponsÃ¡vel'].unique(), filtered_df['ResponsÃ¡vel'].unique())
#         filtered_df = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_sidebar)]
# 
#         # Filter by EstÃ¡gio
#         selected_estagios = st.sidebar.multiselect("Selecionar EstÃ¡gio:", filtered_df['EstÃ¡gio'].unique(), filtered_df['EstÃ¡gio'].unique())
#         filtered_df = filtered_df[filtered_df['EstÃ¡gio'].isin(selected_estagios)]
# 
# 
#         # AgregaÃ§Ãµes para visualizaÃ§Ãµes (using filtered_df)
#         # Aggregate to count unique OC_Identifier per ResponsÃ¡vel
#         df_agg_responsavel_count = filtered_df.groupby('ResponsÃ¡vel')['OC_Identifier'].nunique().reset_index()
#         df_agg_responsavel_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
# 
# 
#         # Aggregate to count unique OC_Identifier per Estado and MonthYear_Abertura
#         df_agg_estado_mes_count = filtered_df.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#         df_agg_estado_mes_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#         df_agg_estado_mes_count['MonthYear_Abertura'] = df_agg_estado_mes_count['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
# 
#         # Filter for 'Ganha' deals for specific visualizations
#         ganha_df_filtered = filtered_df[filtered_df['Estado'] == 'Ganha'].copy()
# 
#         # --- Display Key Metrics (KPIs) ---
#         st.subheader("Resumo Geral")
#         col_kpi1, col_kpi2, col_kpi3 = st.columns(3)
# 
#         total_opportunities = filtered_df['OC_Identifier'].nunique() if not filtered_df.empty else 0
#         total_won_value = ganha_df_filtered['Valor'].sum() if not ganha_df_filtered.empty else 0
#         win_rate = (len(ganha_df_filtered) / total_opportunities * 100) if total_opportunities > 0 else 0
# 
# 
#         col_kpi1.metric("Total Oportunidades Ãšnicas", total_opportunities)
#         col_kpi2.metric("Valor Total Ganho", f"R$ {total_won_value:,.2f}")
#         col_kpi3.metric("Taxa de Sucesso", f"{win_rate:.2f}%")
# 
# 
#         # --- Visualizations ---
#         st.subheader("AnÃ¡lise de Oportunidades e Valor")
# 
#         # Row 1: Value/Count by Responsible and Deals by State/Month
#         col1, col2 = st.columns(2)
# 
#         with col1:
#             st.subheader("Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel")
#             if not df_agg_responsavel_count.empty:
#                 # Using Plotly Express for interactivity
#                 fig1 = px.bar(df_agg_responsavel_count, x='ResponsÃ¡vel', y='Unique Opportunity Count',
#                               title='Unique Opportunity Count per ResponsÃ¡vel')
# 
#                 # Capture selection
#                 selected_points = st.plotly_chart(fig1, use_container_width=True, on_select="rerun")
# 
#                 # Filter based on selection (using the original filtered_df and the selection from the chart)
#                 if selected_points and selected_points.selection and selected_points.selection.points:
#                      selected_responsaveis_chart = [point['x'] for point in selected_points.selection.points]
#                      filtered_df_chart_selection = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_chart)].copy()
#                 else:
#                      filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart selection
# 
#             else:
#                 st.info("Nenhum dado disponÃ­vel para 'Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel' com os filtros selecionados.")
#                 filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart data
# 
# 
#         with col2:
#              # Recalculate aggregated data based on chart selection
#              df_agg_estado_mes_count_filtered = filtered_df_chart_selection.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#              df_agg_estado_mes_count_filtered.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#              df_agg_estado_mes_count_filtered['MonthYear_Abertura'] = df_agg_estado_mes_count_filtered['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
#              st.subheader("Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura")
#              if not df_agg_estado_mes_count_filtered.empty:
#                  # Using Plotly Express for the improved chart
#                  fig2 = px.bar(df_agg_estado_mes_count_filtered, x='MonthYear_Abertura', y='Unique Opportunity Count', color='Estado',
#                                title='Unique Opportunity Count per Estado and Month of Opening (Filtered)',
#                                barmode='group') # Use 'group' or 'stack'
# 
#                  fig2.update_layout(xaxis_title="MÃªs/Ano de Abertura", yaxis_title="Quantidade de Oportunidades Ãšnicas")
#                  st.plotly_chart(fig2, use_container_width=True)
# 
#              else:
#                   st.info("Nenhum dado disponÃ­vel para 'Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura' com os filtros selecionados.")
# 
# 
#         # Row 2: Heatmap of Opportunities by Stage and Hour
#         st.subheader("Heatmap: Oportunidades por Etapa e Hora de Abertura")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered_for_heatmap = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
# 
#             # Aggregate data for the heatmap (count unique OC_Identifier per EstÃ¡gio and Hour_of_Day_Abertura)
#             # Ensure 'Hour_of_Day_Abertura' is extracted for the filtered timeline data
#             df_timeline_filtered_for_heatmap['Hour_of_Day_Abertura'] = df_timeline_filtered_for_heatmap['Data de abertura'].dt.hour
# 
#             if not df_timeline_filtered_for_heatmap.empty:
#                  heatmap_data = df_timeline_filtered_for_heatmap.groupby(['EstÃ¡gio', 'Hour_of_Day_Abertura'])['OC_Identifier'].nunique().unstack(fill_value=0)
# 
#                  if not heatmap_data.empty:
#                     # Use go.Heatmap to create the heatmap
#                     fig_heatmap = go.Figure(data=go.Heatmap(
#                            z=heatmap_data.values,
#                            x=heatmap_data.columns,
#                            y=heatmap_data.index,
#                            colorscale='Viridis')) # Use a color scale
# 
#                     fig_heatmap.update_layout(title='Opportunity Count by Stage and Hour of Opening (Filtered)',
#                                               xaxis_title='Hour of Day',
#                                               yaxis_title='Stage')
# 
#                     st.plotly_chart(fig_heatmap, use_container_width=True)
#                  else:
#                       st.info("Nenhum dado agregado disponÃ­vel para o Heatmap com os filtros selecionados.")
#             else:
#                  st.info("Nenhum dado de timeline disponÃ­vel para o Heatmap com os filtros selecionados.")
#         else:
#              st.info("Dados de timeline nÃ£o disponÃ­veis para o Heatmap.")
# 
# 
#         # Row 3: Distribution of EstÃ¡gios
#         st.subheader("AnÃ¡lise de EstÃ¡gios")
#         col5, col6 = st.columns(2)
# 
#         with col5:
#              # New Visualizations for 'Ganha' deals
#             if not ganha_df_filtered.empty:
#                 st.subheader("DistribuiÃ§Ã£o do Valor para NegÃ³cios Ganhos (Filtered)")
#                 fig3, ax3 = plt.subplots(figsize=(10, 6))
#                 sns.set_palette("viridis") # Set a color palette
#                 ax3.hist(ganha_df_filtered['Valor'].dropna(), bins=50)
#                 ax3.set_title('Distribution of Valor for Won Deals (Filtered)')
#                 ax3.set_xlabel('Valor')
#                 ax3.set_ylabel('Frequency')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig3)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para visualizaÃ§Ãµes de NegÃ³cios Ganhos com os filtros selecionados.")
# 
#         with col6:
#             if not filtered_df_chart_selection.empty:
#                 # Distribution of all stages (not just 'Ganha')
#                 st.subheader("DistribuiÃ§Ã£o de Todos os EstÃ¡gios (Filtered)")
#                 fig4, ax4 = plt.subplots(figsize=(12, 6))
#                 sns.set_palette("viridis") # Set a color palette
#                 filtered_df_chart_selection['EstÃ¡gio'].value_counts().plot(kind='bar', ax=ax4)
#                 ax4.set_title('Distribution of All Stages (Filtered)')
#                 ax4.set_xlabel('EstÃ¡gio')
#                 ax4.set_ylabel('Count')
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig4)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para 'DistribuiÃ§Ã£o de Todos os EstÃ¡gios' com os filtros selecionados.")
# 
# 
#         # Row 4: Timeline Analysis Metrics
#         st.subheader("AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio (Filtered)")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  # Filter timeline data based on selected OC_Identifiers from the chart selection
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
# 
#             if not df_timeline_filtered.empty:
#                 df_agg_time_per_stage_avg = df_timeline_filtered.groupby('EstÃ¡gio')['Time_in_Stage'].mean().reset_index()
#                 df_agg_time_per_stage_avg = df_agg_time_per_stage_avg.sort_values(by='Time_in_Stage', ascending=False)
# 
#                 # Format 'Time_in_Stage' for display to include minutes
#                 def format_time_in_stage(hours):
#                     if pd.isna(hours):
#                         return "N/A"
#                     total_minutes = int(hours * 60)
#                     days = total_minutes // (24 * 60)
#                     remaining_minutes_after_days = total_minutes % (24 * 60)
#                     hours = remaining_minutes_after_days // 60
#                     minutes = remaining_minutes_after_days % 60
#                     return f"{days} days, {hours} hours, {minutes} minutes"
# 
#                 df_agg_time_per_stage_avg['Average Time in Stage'] = df_agg_time_per_stage_avg['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#                 st.write("Tempo MÃ©dio em Cada EstÃ¡gio:")
#                 st.dataframe(df_agg_time_per_stage_avg[['EstÃ¡gio', 'Average Time in Stage']])
# 
#                 # Visualization of average time per stage (using the numerical value)
#                 st.subheader("Tempo MÃ©dio por EstÃ¡gio VisualizaÃ§Ã£o (Filtered)")
#                 fig5, ax5 = plt.subplots(figsize=(14, 7))
#                 sns.set_palette("viridis") # Set a color palette
#                 sns.barplot(x='EstÃ¡gio', y='Time_in_Stage', data=df_agg_time_per_stage_avg, ax=ax5)
#                 ax5.set_title('Average Time per Stage (Filtered by Data de Abertura and Selection)')
#                 ax5.set_xlabel('EstÃ¡gio')
#                 ax5.set_ylabel('Average Time (hours)') # Keep y-axis as hours for the plot to work directly with numerical data
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig5)
#             else:
#                  st.info("Nenhum dado disponÃ­vel para AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio com os filtros selecionados.")
#         else:
#             st.info("Dados de timeline nÃ£o disponÃ­veis.")
# 
# 
# elif page == "RelatÃ³rio de Oportunidade":
#     # --- RelatÃ³rio de Oportunidade Individual ---
#     st.title("RelatÃ³rio de Oportunidade Individual")
# 
#     st.markdown("""
#     Selecione um identificador de Oportunidade (OC + NÃºmero ou CTE + NÃºmero)
#     para visualizar sua linha do tempo e detalhes.
#     """)
# 
#     if not df.empty and not df_timeline.empty:
#         # Use OC_Identifier for selection
#         opportunity_identifiers = df['OC_Identifier'].dropna().unique()
#         selected_opportunity_identifier = st.selectbox("Selecionar Oportunidade (OC + NÃºmero ou CTE + NÃºmero):", opportunity_identifiers)
# 
#         st.subheader(f"Detalhes e Linha do Tempo para: {selected_opportunity_identifier}")
# 
#         # Filter main df for the selected opportunity identifier
#         opportunity_details_df = df[df['OC_Identifier'] == selected_opportunity_identifier]
# 
#         if not opportunity_details_df.empty:
#             opportunity_details = opportunity_details_df.iloc[0]
# 
#             # Use columns for a structured layout
#             col_info1, col_info2 = st.columns(2)
# 
#             with col_info1:
#                 st.write("**ID:**", opportunity_details['ID'])
#                 st.write("**TÃ­tulo:**", opportunity_details['TÃ­tulo'])
#                 st.write("**ResponsÃ¡vel:**", opportunity_details['ResponsÃ¡vel'])
#                 st.write("**Estado:**", opportunity_details['Estado'])
#                 st.write("**EstÃ¡gio Atual:**", opportunity_details['EstÃ¡gio']) # Display current stage
# 
#             with col_info2:
#                  st.write("**Valor:**", f"R$ {opportunity_details['Valor']:,.2f}" if pd.notna(opportunity_details['Valor']) else "N/A")
#                  st.write("**Origem:**", opportunity_details['Origem'])
#                  st.write("**Prob %:**", opportunity_details['Prob %'])
#                  st.write("**OC:**", opportunity_details['OC']) # Display OC column
# 
#             # Display Dates
#             st.subheader("Datas Principais")
#             col_dates1, col_dates2 = st.columns(2)
#             with col_dates1:
#                  st.write("**Data de Abertura:**", opportunity_details['Data de abertura'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data de abertura']) else "N/A")
#             with col_dates2:
#                  st.write("**Data de Fechamento:**", opportunity_details['Data fechamento'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data fechamento']) else "N/A")
# 
#             # Use an expander for Closing Details (if available)
#             if pd.notna(opportunity_details['Data fechamento']):
#                 with st.expander("Detalhes de Fechamento"):
#                     st.write("**Valor Fechamento:**", f"R$ {opportunity_details['Valor fechamento']:,.2f}" if pd.notna(opportunity_details['Valor fechamento']) else "N/A")
#                     st.write("**Valor Rec. Fechamento:**", f"R$ {opportunity_details['Valor rec. fechamento']:,.2f}" if pd.notna(opportunity_details['Valor rec. fechamento']) else "N/A")
#                     st.write("**RazÃ£o de Fechamento:**", opportunity_details['RazÃ£o de fechamento'])
#                     st.write("**ObservaÃ§Ã£o de Fechamento:**", opportunity_details['ObservaÃ§Ã£o de fechamento'])
# 
# 
#             # Filter timeline for the selected opportunity identifier
#             opportunity_timeline = df_timeline[df_timeline['OC_Identifier'] == selected_opportunity_identifier].copy()
# 
#             if not opportunity_timeline.empty:
#                 st.subheader("Linha do Tempo da Oportunidade")
# 
#                 # Display timeline using a more visually appealing table format
#                 display_timeline_cols = ['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']
#                 st.dataframe(opportunity_timeline[display_timeline_cols])
# 
# 
#             else:
#                 st.info(f"Nenhum dado de linha do tempo encontrado para: {selected_opportunity_identifier}")
# 
#             # --- AI Agent Interaction Section ---
#             st.subheader("Assistente de IA para Oportunidade")
#             if client: # Only display if OpenAI client is initialized
#                 user_query = st.text_area(f"FaÃ§a uma pergunta sobre a oportunidade {selected_opportunity_identifier}:", height=100)
#                 if st.button("Obter Resposta da IA"):
#                     # This is where the agent call will be implemented in the next subtask
#                     st.info("Funcionalidade do agente de IA serÃ¡ implementada em seguida.")
#             else:
#                 st.warning("O cliente OpenAI nÃ£o estÃ¡ inicializado. A funcionalidade do assistente de IA estÃ¡ desativada.")
# 
# 
#         else:
#              st.info(f"Nenhum detalhe encontrado para: {selected_opportunity_identifier} no DataFrame principal.")
# 
#     elif df.empty:
#          st.warning("NÃ£o foi possÃ­vel carregar os dados. Verifique a conexÃ£o com o Google Sheet.")
#     else:
#          st.info("Dados de linha do tempo nÃ£o disponÃ­veis.")
# 
# 
# except Exception as e:
#     st.error(f"Erro ao carregar dados ou processar: {e}")

"""## Implement agent logic

### Subtask:
Implement the Python code that will take the user's input, interact with the OpenAI API (potentially using the loaded data as context), and display the agent's response or generated report on the page.

**Reasoning**:
Implement the logic to call the OpenAI API using the user's input and relevant opportunity data, handle the response, and display it, including basic error handling.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from google.colab import auth
# import gspread
# from google.auth import default
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# import plotly.express as px # Import Plotly Express
# import plotly.graph_objects as go # Import Plotly Graph Objects
# import re # Import regex for extracting OC + number
# from datetime import datetime # Import datetime for current time
# import openai # Import the openai library
# 
# st.set_page_config(layout="wide") # Set wide layout for better use of space
# 
# # Configure OpenAI API key and initialize client
# if "OPENAI_API_KEY" in st.secrets:
#     openai.api_key = st.secrets["OPENAI_API_KEY"]
#     client = openai.OpenAI(api_key=openai.api_key)
#     st.sidebar.success("OpenAI client initialized.")
# else:
#     st.sidebar.warning("OpenAI API key not found in Streamlit secrets. OpenAI features will be disabled.")
#     client = None # Set client to None if key is not available
# 
# 
# @st.cache_data # Cache the data loading for performance
# def load_data():
#     try:
#         # Authenticate to Google Cloud
#         auth.authenticate_user()
#         creds, _ = default()
#         gc = gspread.authorize(creds)
#         B2sheet = gc.open('BI_B2')
#         page = B2sheet.sheet1
#         all_data = page.get_all_values()
#         df = pd.DataFrame(all_data[1:], columns=all_data[0])
# 
#         # Limpeza e prÃ©-processing
#         value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
#         for col in value_cols:
#             df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
#             df[col] = pd.to_numeric(df[col], errors='coerce')
# 
#         date_cols = ['Data de abertura', 'Data fechamento']
#         for col in date_cols:
#             df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')
# 
#         # Extract unique opportunity identifier (OC + number) from 'TÃ­tulo'
#         def extract_oc_identifier(title):
#             if isinstance(title, str):
#                 match = re.search(r'(OC\s*\d+)', title, re.IGNORECASE)
#                 if match:
#                     return match.group(1).replace(" ", "") # Remove space in "OC "
#                 else:
#                     # Also check for "CTE" as it appears in titles and might be identifiers
#                     match_cte = re.search(r'(CTE\s*\d+)', title, re.IGNORECASE)
#                     if match_cte:
#                          return match_cte.group(1).replace(" ", "")
#             return None # Return None if no pattern is found
# 
#         df['OC_Identifier'] = df['TÃ­tulo'].apply(extract_oc_identifier)
# 
# 
#         # Transformation and Feature Engineering for BI
#         # Ensure date columns are not NaT before accessing dt properties
#         df['Mes de Abertura'] = df['Data de abertura'].dt.month.fillna(0).astype(int)
#         df['Ano de Abertura'] = df['Data de abertura'].dt.year.fillna(0).astype(int)
#         df['Mes de Fechamento'] = df['Data fechamento'].dt.month.fillna(0).astype(int)
#         df['Ano de Fechamento'] = df['Data fechamento'].dt.year.fillna(0).astype(int)
# 
#         # Create a 'MonthYear' column for time series analysis
#         df['MonthYear_Abertura'] = df['Data de abertura'].dt.to_period('M')
#         df['MonthYear_Fechamento'] = df['Data fechamento'].dt.to_period('M')
# 
#         # Extract Hour of Day from 'Data de abertura'
#         df['Hour_of_Day_Abertura'] = df['Data de abertura'].dt.hour
# 
# 
#         # Calculate Time in Stage for timeline analysis using OC_Identifier
#         df_timeline = df[['OC_Identifier', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()
#         # Drop rows where OC_Identifier could not be extracted or Data de abertura is missing
#         df_timeline.dropna(subset=['OC_Identifier', 'Data de abertura'], inplace=True)
#         df_timeline = df_timeline.sort_values(by=['OC_Identifier', 'Data de abertura'])
# 
#         current_time = pd.to_datetime('now') # Timezone-naive for consistency
#         df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600 # Time in hours
# 
#         df_timeline['Time_in_Stage'] = df_timeline.apply(
#             lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
#             axis=1
#         )
# 
#         # Format 'Time_in_Stage' for display to include minutes
#         def format_time_in_stage(hours):
#             if pd.isna(hours):
#                 return "N/A"
#             total_minutes = int(hours * 60)
#             days = total_minutes // (24 * 60)
#             remaining_minutes_after_days = total_minutes % (24 * 60)
#             hours = remaining_minutes_after_days // 60
#             minutes = remaining_minutes_after_days % 60
#             return f"{days} days, {hours} hours, {minutes} minutes"
# 
#         df_timeline['Time_in_Stage_Formatted'] = df_timeline['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#         return df, df_timeline
# 
#     except Exception as e:
#         st.error(f"Erro ao carregar dados do Google Sheet: {e}")
#         return pd.DataFrame(), pd.DataFrame() # Return empty dataframes on error
# 
# 
# # Load data
# df, df_timeline = load_data()
# 
# # --- Multi-page Navigation ---
# st.sidebar.title("NavegaÃ§Ã£o")
# page = st.sidebar.radio("Ir para:", ["Painel Geral", "RelatÃ³rio de Oportunidade"])
# 
# # --- Refresh Button ---
# if st.sidebar.button("Atualizar Dados"):
#     st.cache_data.clear() # Clear the cache for load_data
#     st.rerun() # Rerun the app to load fresh data
# 
# 
# if page == "Painel Geral":
#     # --- Painel Geral ---
#     st.title("Painel de BI Operacional - Geral")
# 
#     st.markdown("""
#     Este painel apresenta insights gerais sobre os dados operacionais,
#     permitindo acompanhar o desempenho e a distribuiÃ§Ã£o dos negÃ³cios.
#     """)
# 
#     if not df.empty:
#         # Add filters to the sidebar
#         st.sidebar.subheader("Filtros do Painel Geral")
# 
#         # Date range filter for 'Data de abertura'
#         if not df['Data de abertura'].empty:
#             min_date_abertura = df['Data de abertura'].min().date()
#             max_date_abertura = df['Data de abertura'].max().date()
# 
#             start_date = st.sidebar.date_input("Data de Abertura (InÃ­cio)", min_value=min_date_abertura, max_value=max_date_abertura, value=min_date_abertura)
#             end_date = st.sidebar.date_input("Data de Abertura (Fim)", min_value=min_date_abertura, max_value=max_date_abertura, value=max_date_abertura)
# 
#             # Convert selected dates to datetime objects for filtering
#             start_datetime = pd.to_datetime(start_date)
#             end_datetime = pd.to_datetime(end_date)
# 
#             # Apply date filter
#             filtered_df = df[(df['Data de abertura'] >= start_datetime) & (df['Data de abertura'] <= end_datetime)].copy()
#         else:
#             filtered_df = df.copy() # If date column is empty, use the original df
# 
# 
#         # Filter by Estado
#         selected_estados = st.sidebar.multiselect("Selecionar Estado:", filtered_df['Estado'].unique(), filtered_df['Estado'].unique())
#         filtered_df = filtered_df[filtered_df['Estado'].isin(selected_estados)]
# 
#         # Filter by ResponsÃ¡vel (Initial filter)
#         selected_responsaveis_sidebar = st.sidebar.multiselect("Selecionar ResponsÃ¡vel (Filtro Inicial):", filtered_df['ResponsÃ¡vel'].unique(), filtered_df['ResponsÃ¡vel'].unique())
#         filtered_df = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_sidebar)]
# 
#         # Filter by EstÃ¡gio
#         selected_estagios = st.sidebar.multiselect("Selecionar EstÃ¡gio:", filtered_df['EstÃ¡gio'].unique(), filtered_df['EstÃ¡gio'].unique())
#         filtered_df = filtered_df[filtered_df['EstÃ¡gio'].isin(selected_estagios)]
# 
# 
#         # AgregaÃ§Ãµes para visualizaÃ§Ãµes (using filtered_df)
#         # Aggregate to count unique OC_Identifier per ResponsÃ¡vel
#         df_agg_responsavel_count = filtered_df.groupby('ResponsÃ¡vel')['OC_Identifier'].nunique().reset_index()
#         df_agg_responsavel_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
# 
# 
#         # Aggregate to count unique OC_Identifier per Estado and MonthYear_Abertura
#         df_agg_estado_mes_count = filtered_df.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#         df_agg_estado_mes_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#         df_agg_estado_mes_count['MonthYear_Abertura'] = df_agg_estado_mes_count['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
# 
#         # Filter for 'Ganha' deals for specific visualizations
#         ganha_df_filtered = filtered_df[filtered_df['Estado'] == 'Ganha'].copy()
# 
#         # --- Display Key Metrics (KPIs) ---
#         st.subheader("Resumo Geral")
#         col_kpi1, col_kpi2, col_kpi3 = st.columns(3)
# 
#         total_opportunities = filtered_df['OC_Identifier'].nunique() if not filtered_df.empty else 0
#         total_won_value = ganha_df_filtered['Valor'].sum() if not ganha_df_filtered.empty else 0
#         win_rate = (len(ganha_df_filtered) / total_opportunities * 100) if total_opportunities > 0 else 0
# 
# 
#         col_kpi1.metric("Total Oportunidades Ãšnicas", total_opportunities)
#         col_kpi2.metric("Valor Total Ganho", f"R$ {total_won_value:,.2f}")
#         col_kpi3.metric("Taxa de Sucesso", f"{win_rate:.2f}%")
# 
# 
#         # --- Visualizations ---
#         st.subheader("AnÃ¡lise de Oportunidades e Valor")
# 
#         # Row 1: Value/Count by Responsible and Deals by State/Month
#         col1, col2 = st.columns(2)
# 
#         with col1:
#             st.subheader("Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel")
#             if not df_agg_responsavel_count.empty:
#                 # Using Plotly Express for interactivity
#                 fig1 = px.bar(df_agg_responsavel_count, x='ResponsÃ¡vel', y='Unique Opportunity Count',
#                               title='Unique Opportunity Count per ResponsÃ¡vel')
# 
#                 # Capture selection
#                 selected_points = st.plotly_chart(fig1, use_container_width=True, on_select="rerun")
# 
#                 # Filter based on selection (using the original filtered_df and the selection from the chart)
#                 if selected_points and selected_points.selection and selected_points.selection.points:
#                      selected_responsaveis_chart = [point['x'] for point in selected_points.selection.points]
#                      filtered_df_chart_selection = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_chart)].copy()
#                 else:
#                      filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart selection
# 
#             else:
#                 st.info("Nenhum dado disponÃ­vel para 'Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel' com os filtros selecionados.")
#                 filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart data
# 
# 
#         with col2:
#              # Recalculate aggregated data based on chart selection
#              df_agg_estado_mes_count_filtered = filtered_df_chart_selection.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#              df_agg_estado_mes_count_filtered.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#              df_agg_estado_mes_count_filtered['MonthYear_Abertura'] = df_agg_estado_mes_count_filtered['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
#              st.subheader("Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura")
#              if not df_agg_estado_mes_count_filtered.empty:
#                  # Using Plotly Express for the improved chart
#                  fig2 = px.bar(df_agg_estado_mes_count_filtered, x='MonthYear_Abertura', y='Unique Opportunity Count', color='Estado',
#                                title='Unique Opportunity Count per Estado and Month of Opening (Filtered)',
#                                barmode='group') # Use 'group' or 'stack'
# 
#                  fig2.update_layout(xaxis_title="MÃªs/Ano de Abertura", yaxis_title="Quantidade de Oportunidades Ãšnicas")
#                  st.plotly_chart(fig2, use_container_width=True)
# 
#              else:
#                   st.info("Nenhum dado disponÃ­vel para 'Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura' com os filtros selecionados.")
# 
# 
#         # Row 2: Heatmap of Opportunities by Stage and Hour
#         st.subheader("Heatmap: Oportunidades por Etapa e Hora de Abertura")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered_for_heatmap = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
# 
#             # Aggregate data for the heatmap (count unique OC_Identifier per EstÃ¡gio and Hour_of_Day_Abertura)
#             # Ensure 'Hour_of_Day_Abertura' is extracted for the filtered timeline data
#             df_timeline_filtered_for_heatmap['Hour_of_Day_Abertura'] = df_timeline_filtered_for_heatmap['Data de abertura'].dt.hour
# 
#             if not df_timeline_filtered_for_heatmap.empty:
#                  heatmap_data = df_timeline_filtered_for_heatmap.groupby(['EstÃ¡gio', 'Hour_of_Day_Abertura'])['OC_Identifier'].nunique().unstack(fill_value=0)
# 
#                  if not heatmap_data.empty:
#                     # Use go.Heatmap to create the heatmap
#                     fig_heatmap = go.Figure(data=go.Heatmap(
#                            z=heatmap_data.values,
#                            x=heatmap_data.columns,
#                            y=heatmap_data.index,
#                            colorscale='Viridis')) # Use a color scale
# 
#                     fig_heatmap.update_layout(title='Opportunity Count by Stage and Hour of Opening (Filtered)',
#                                               xaxis_title='Hour of Day',
#                                               yaxis_title='Stage')
# 
#                     st.plotly_chart(fig_heatmap, use_container_width=True)
#                  else:
#                       st.info("Nenhum dado agregado disponÃ­vel para o Heatmap com os filtros selecionados.")
#             else:
#                  st.info("Nenhum dado de timeline disponÃ­vel para o Heatmap com os filtros selecionados.")
#         else:
#              st.info("Dados de timeline nÃ£o disponÃ­veis para o Heatmap.")
# 
# 
#         # Row 3: Distribution of EstÃ¡gios
#         st.subheader("AnÃ¡lise de EstÃ¡gios")
#         col5, col6 = st.columns(2)
# 
#         with col5:
#              # New Visualizations for 'Ganha' deals
#             if not ganha_df_filtered.empty:
#                 st.subheader("DistribuiÃ§Ã£o do Valor para NegÃ³cios Ganhos (Filtered)")
#                 fig3, ax3 = plt.subplots(figsize=(10, 6))
#                 sns.set_palette("viridis") # Set a color palette
#                 ax3.hist(ganha_df_filtered['Valor'].dropna(), bins=50)
#                 ax3.set_title('Distribution of Valor for Won Deals (Filtered)')
#                 ax3.set_xlabel('Valor')
#                 ax3.set_ylabel('Frequency')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig3)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para visualizaÃ§Ãµes de NegÃ³cios Ganhos com os filtros selecionados.")
# 
#         with col6:
#             if not filtered_df_chart_selection.empty:
#                 # Distribution of all stages (not just 'Ganha')
#                 st.subheader("DistribuiÃ§Ã£o de Todos os EstÃ¡gios (Filtered)")
#                 fig4, ax4 = plt.subplots(figsize=(12, 6))
#                 sns.set_palette("viridis") # Set a color palette
#                 filtered_df_chart_selection['EstÃ¡gio'].value_counts().plot(kind='bar', ax=ax4)
#                 ax4.set_title('Distribution of All Stages (Filtered)')
#                 ax4.set_xlabel('EstÃ¡gio')
#                 ax4.set_ylabel('Count')
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig4)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para 'DistribuiÃ§Ã£o de Todos os EstÃ¡gios' com os filtros selecionados.")
# 
# 
#         # Row 4: Timeline Analysis Metrics
#         st.subheader("AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio (Filtered)")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  # Filter timeline data based on selected OC_Identifiers from the chart selection
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
# 
#             if not df_timeline_filtered.empty:
#                 df_agg_time_per_stage_avg = df_timeline_filtered.groupby('EstÃ¡gio')['Time_in_Stage'].mean().reset_index()
#                 df_agg_time_per_stage_avg = df_agg_time_per_stage_avg.sort_values(by='Time_in_Stage', ascending=False)
# 
#                 # Format 'Time_in_Stage' for display to include minutes
#                 def format_time_in_stage(hours):
#                     if pd.isna(hours):
#                         return "N/A"
#                     total_minutes = int(hours * 60)
#                     days = total_minutes // (24 * 60)
#                     remaining_minutes_after_days = total_minutes % (24 * 60)
#                     hours = remaining_minutes_after_days // 60
#                     minutes = remaining_minutes_after_days % 60
#                     return f"{days} days, {hours} hours, {minutes} minutes"
# 
#                 df_agg_time_per_stage_avg['Average Time in Stage'] = df_agg_time_per_stage_avg['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#                 st.write("Tempo MÃ©dio em Cada EstÃ¡gio:")
#                 st.dataframe(df_agg_time_per_stage_avg[['EstÃ¡gio', 'Average Time in Stage']])
# 
#                 # Visualization of average time per stage (using the numerical value)
#                 st.subheader("Tempo MÃ©dio por EstÃ¡gio VisualizaÃ§Ã£o (Filtered)")
#                 fig5, ax5 = plt.subplots(figsize=(14, 7))
#                 sns.set_palette("viridis") # Set a color palette
#                 sns.barplot(x='EstÃ¡gio', y='Time_in_Stage', data=df_agg_time_per_stage_avg, ax=ax5)
#                 ax5.set_title('Average Time per Stage (Filtered by Data de Abertura and Selection)')
#                 ax5.set_xlabel('EstÃ¡gio')
#                 ax5.set_ylabel('Average Time (hours)') # Keep y-axis as hours for the plot to work directly with numerical data
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig5)
#             else:
#                  st.info("Nenhum dado disponÃ­vel para AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio com os filtros selecionados.")
#         else:
#             st.info("Dados de timeline nÃ£o disponÃ­veis.")
# 
# 
# elif page == "RelatÃ³rio de Oportunidade":
#     # --- RelatÃ³rio de Oportunidade Individual ---
#     st.title("RelatÃ³rio de Oportunidade Individual")
# 
#     st.markdown("""
#     Selecione um identificador de Oportunidade (OC + NÃºmero ou CTE + NÃºmero)
#     para visualizar sua linha do tempo e detalhes.
#     """)
# 
#     if not df.empty and not df_timeline.empty:
#         # Use OC_Identifier for selection
#         opportunity_identifiers = df['OC_Identifier'].dropna().unique()
#         selected_opportunity_identifier = st.selectbox("Selecionar Oportunidade (OC + NÃºmero ou CTE + NÃºmero):", opportunity_identifiers)
# 
#         st.subheader(f"Detalhes e Linha do Tempo para: {selected_opportunity_identifier}")
# 
#         # Filter main df for the selected opportunity identifier
#         opportunity_details_df = df[df['OC_Identifier'] == selected_opportunity_identifier]
# 
#         if not opportunity_details_df.empty:
#             opportunity_details = opportunity_details_df.iloc[0]
# 
#             # Use columns for a structured layout
#             col_info1, col_info2 = st.columns(2)
# 
#             with col_info1:
#                 st.write("**ID:**", opportunity_details['ID'])
#                 st.write("**TÃ­tulo:**", opportunity_details['TÃ­tulo'])
#                 st.write("**ResponsÃ¡vel:**", opportunity_details['ResponsÃ¡vel'])
#                 st.write("**Estado:**", opportunity_details['Estado'])
#                 st.write("**EstÃ¡gio Atual:**", opportunity_details['EstÃ¡gio']) # Display current stage
# 
#             with col_info2:
#                  st.write("**Valor:**", f"R$ {opportunity_details['Valor']:,.2f}" if pd.notna(opportunity_details['Valor']) else "N/A")
#                  st.write("**Origem:**", opportunity_details['Origem'])
#                  st.write("**Prob %:**", opportunity_details['Prob %'])
#                  st.write("**OC:**", opportunity_details['OC']) # Display OC column
# 
#             # Display Dates
#             st.subheader("Datas Principais")
#             col_dates1, col_dates2 = st.columns(2)
#             with col_dates1:
#                  st.write("**Data de Abertura:**", opportunity_details['Data de abertura'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data de abertura']) else "N/A")
#             with col_dates2:
#                  st.write("**Data de Fechamento:**", opportunity_details['Data fechamento'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data fechamento']) else "N/A")
# 
#             # Use an expander for Closing Details (if available)
#             if pd.notna(opportunity_details['Data fechamento']):
#                 with st.expander("Detalhes de Fechamento"):
#                     st.write("**Valor Fechamento:**", f"R$ {opportunity_details['Valor fechamento']:,.2f}" if pd.notna(opportunity_details['Valor fechamento']) else "N/A")
#                     st.write("**Valor Rec. Fechamento:**", f"R$ {opportunity_details['Valor rec. fechamento']:,.2f}" if pd.notna(opportunity_details['Valor rec. fechamento']) else "N/A")
#                     st.write("**RazÃ£o de Fechamento:**", opportunity_details['RazÃ£o de fechamento'])
#                     st.write("**ObservaÃ§Ã£o de Fechamento:**", opportunity_details['ObservaÃ§Ã£o de fechamento'])
# 
# 
#             # Filter timeline for the selected opportunity identifier
#             opportunity_timeline = df_timeline[df_timeline['OC_Identifier'] == selected_opportunity_identifier].copy()
# 
#             if not opportunity_timeline.empty:
#                 st.subheader("Linha do Tempo da Oportunidade")
# 
#                 # Display timeline using a more visually appealing table format
#                 display_timeline_cols = ['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']
#                 st.dataframe(opportunity_timeline[display_timeline_cols])
# 
# 
#             else:
#                 st.info(f"Nenhum dado de linha do tempo encontrado para: {selected_opportunity_identifier}")
# 
#             # --- AI Agent Interaction Section ---
#             st.subheader("Assistente de IA para Oportunidade")
#             if client: # Only display if OpenAI client is initialized
#                 user_query = st.text_area(f"FaÃ§a uma pergunta sobre a oportunidade {selected_opportunity_identifier}:", height=100)
#                 if st.button("Obter Resposta da IA"):
#                     with st.spinner("Obtendo resposta da IA..."):
#                         try:
#                             # Construct the prompt
#                             prompt = f"""
#                             VocÃª Ã© um assistente de BI focado em analisar dados de oportunidades de negÃ³cios.
#                             A seguir estÃ£o os detalhes e a linha do tempo da oportunidade com o identificador {selected_opportunity_identifier}:
# 
#                             Detalhes da Oportunidade:
#                             ID: {opportunity_details.get('ID', 'N/A')}
#                             TÃ­tulo: {opportunity_details.get('TÃ­tulo', 'N/A')}
#                             ResponsÃ¡vel: {opportunity_details.get('ResponsÃ¡vel', 'N/A')}
#                             Estado: {opportunity_details.get('Estado', 'N/A')}
#                             EstÃ¡gio Atual: {opportunity_details.get('EstÃ¡gio', 'N/A')}
#                             Valor: R$ {opportunity_details.get('Valor', 'N/A')}
#                             Origem: {opportunity_details.get('Origem', 'N/A')}
#                             Prob %: {opportunity_details.get('Prob %', 'N/A')}
#                             OC: {opportunity_details.get('OC', 'N/A')}
#                             Data de Abertura: {opportunity_details.get('Data de abertura', 'N/A')}
#                             Data de Fechamento: {opportunity_details.get('Data fechamento', 'N/A')}
#                             Valor Fechamento: R$ {opportunity_details.get('Valor fechamento', 'N/A')}
#                             Valor Rec. Fechamento: R$ {opportunity_details.get('Valor rec. fechamento', 'N/A')}
#                             RazÃ£o de Fechamento: {opportunity_details.get('RazÃ£o de fechamento', 'N/A')}
#                             ObservaÃ§Ã£o de Fechamento: {opportunity_details.get('ObservaÃ§Ã£o de fechamento', 'N/A')}
# 
#                             Linha do Tempo da Oportunidade (Progresso pelos EstÃ¡gios):
#                             {opportunity_timeline[['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']].to_string(index=False)}
# 
#                             Com base nesses dados, responda Ã  seguinte pergunta do usuÃ¡rio de forma concisa e Ãºtil,
#                             focando em insights de BI e anÃ¡lise de oportunidades.
#                             Se a pergunta nÃ£o puder ser respondida com os dados fornecidos, diga isso.
# 
#                             Pergunta do UsuÃ¡rio: {user_query}
#                             """
#                             # Call the OpenAI API
#                             response = client.chat.completions.create(
#                                 model="gpt-4o-mini", # Or another suitable model
#                                 messages=[
#                                     {"role": "system", "content": "VocÃª Ã© um assistente de BI Ãºtil."},
#                                     {"role": "user", "content": prompt}
#                                 ],
#                                 max_tokens=500 # Limit response length to avoid excessive cost/output
#                             )
#                             ai_response = response.choices[0].message.content
#                             st.info(ai_response) # Display the response
#                         except Exception as e:
#                             st.error(f"Ocorreu um erro ao chamar a API da OpenAI: {e}")
#             else:
#                 st.warning("O cliente OpenAI nÃ£o estÃ¡ inicializado. A funcionalidade do assistente de IA estÃ¡ desativada.")
# 
# 
#         else:
#              st.info(f"Nenhum detalhe encontrado para: {selected_opportunity_identifier} no DataFrame principal.")
# 
#     elif df.empty:
#          st.warning("NÃ£o foi possÃ­vel carregar os dados. Verifique a conexÃ£o com o Google Sheet.")
#     else:
#          st.info("Dados de linha do tempo nÃ£o disponÃ­veis.")
# 
# 
# except Exception as e:
#     st.error(f"Erro ao carregar dados ou processar: {e}")

"""## Refine agent responses

### Subtask:
Iterate on the agent's prompts and response handling to ensure it provides accurate, relevant, and well-formatted information based on the opportunity data.

**Reasoning**:
Refine the prompt to be more concise, explicitly list available fields, and add instructions for handling irrelevant questions. Improve response display using markdown.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from google.colab import auth
# import gspread
# from google.auth import default
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# import plotly.express as px # Import Plotly Express
# import plotly.graph_objects as go # Import Plotly Graph Objects
# import re # Import regex for extracting OC + number
# from datetime import datetime # Import datetime for current time
# import openai # Import the openai library
# 
# st.set_page_config(layout="wide") # Set wide layout for better use of space
# 
# # Configure OpenAI API key and initialize client
# if "OPENAI_API_KEY" in st.secrets:
#     openai.api_key = st.secrets["OPENAI_API_KEY"]
#     client = openai.OpenAI(api_key=openai.api_key)
#     st.sidebar.success("OpenAI client initialized.")
# else:
#     st.sidebar.warning("OpenAI API key not found in Streamlit secrets. OpenAI features will be disabled.")
#     client = None # Set client to None if key is not available
# 
# 
# @st.cache_data # Cache the data loading for performance
# def load_data():
#     try:
#         # Authenticate to Google Cloud
#         auth.authenticate_user()
#         creds, _ = default()
#         gc = gspread.authorize(creds)
#         B2sheet = gc.open('BI_B2')
#         page = B2sheet.sheet1
#         all_data = page.get_all_values()
#         df = pd.DataFrame(all_data[1:], columns=all_data[0])
# 
#         # Limpeza e prÃ©-processing
#         value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
#         for col in value_cols:
#             df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
#             df[col] = pd.to_numeric(df[col], errors='coerce')
# 
#         date_cols = ['Data de abertura', 'Data fechamento']
#         for col in date_cols:
#             df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')
# 
#         # Extract unique opportunity identifier (OC + number) from 'TÃ­tulo'
#         def extract_oc_identifier(title):
#             if isinstance(title, str):
#                 match = re.search(r'(OC\s*\d+)', title, re.IGNORECASE)
#                 if match:
#                     return match.group(1).replace(" ", "") # Remove space in "OC "
#                 else:
#                     # Also check for "CTE" as it appears in titles and might be identifiers
#                     match_cte = re.search(r'(CTE\s*\d+)', title, re.IGNORECASE)
#                     if match_cte:
#                          return match_cte.group(1).replace(" ", "")
#             return None # Return None if no pattern is found
# 
#         df['OC_Identifier'] = df['TÃ­tulo'].apply(extract_oc_identifier)
# 
# 
#         # Transformation and Feature Engineering for BI
#         # Ensure date columns are not NaT before accessing dt properties
#         df['Mes de Abertura'] = df['Data de abertura'].dt.month.fillna(0).astype(int)
#         df['Ano de Abertura'] = df['Data de abertura'].dt.year.fillna(0).astype(int)
#         df['Mes de Fechamento'] = df['Data fechamento'].dt.month.fillna(0).astype(int)
#         df['Ano de Fechamento'] = df['Data fechamento'].dt.year.fillna(0).astype(int)
# 
#         # Create a 'MonthYear' column for time series analysis
#         df['MonthYear_Abertura'] = df['Data de abertura'].dt.to_period('M')
#         df['MonthYear_Fechamento'] = df['Data fechamento'].dt.to_period('M')
# 
#         # Extract Hour of Day from 'Data de abertura'
#         df['Hour_of_Day_Abertura'] = df['Data de abertura'].dt.hour
# 
# 
#         # Calculate Time in Stage for timeline analysis using OC_Identifier
#         df_timeline = df[['OC_Identifier', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()
#         # Drop rows where OC_Identifier could not be extracted or Data de abertura is missing
#         df_timeline.dropna(subset=['OC_Identifier', 'Data de abertura'], inplace=True)
#         df_timeline = df_timeline.sort_values(by=['OC_Identifier', 'Data de abertura'])
# 
#         current_time = pd.to_datetime('now') # Timezone-naive for consistency
#         df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600 # Time in hours
# 
#         df_timeline['Time_in_Stage'] = df_timeline.apply(
#             lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
#             axis=1
#         )
# 
#         # Format 'Time_in_Stage' for display to include minutes
#         def format_time_in_stage(hours):
#             if pd.isna(hours):
#                 return "N/A"
#             total_minutes = int(hours * 60)
#             days = total_minutes // (24 * 60)
#             remaining_minutes_after_days = total_minutes % (24 * 60)
#             hours = remaining_minutes_after_days // 60
#             minutes = remaining_minutes_after_days % 60
#             return f"{days} days, {hours} hours, {minutes} minutes"
# 
#         df_timeline['Time_in_Stage_Formatted'] = df_timeline['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#         return df, df_timeline
# 
#     except Exception as e:
#         st.error(f"Erro ao carregar dados do Google Sheet: {e}")
#         return pd.DataFrame(), pd.DataFrame() # Return empty dataframes on error
# 
# 
# # Load data
# df, df_timeline = load_data()
# 
# # --- Multi-page Navigation ---
# st.sidebar.title("NavegaÃ§Ã£o")
# page = st.sidebar.radio("Ir para:", ["Painel Geral", "RelatÃ³rio de Oportunidade"])
# 
# # --- Refresh Button ---
# if st.sidebar.button("Atualizar Dados"):
#     st.cache_data.clear() # Clear the cache for load_data
#     st.rerun() # Rerun the app to load fresh data
# 
# 
# if page == "Painel Geral":
#     # --- Painel Geral ---
#     st.title("Painel de BI Operacional - Geral")
# 
#     st.markdown("""
#     Este painel apresenta insights gerais sobre os dados operacionais,
#     permitindo acompanhar o desempenho e a distribuiÃ§Ã£o dos negÃ³cios.
#     """)
# 
#     if not df.empty:
#         # Add filters to the sidebar
#         st.sidebar.subheader("Filtros do Painel Geral")
# 
#         # Date range filter for 'Data de abertura'
#         if not df['Data de abertura'].empty:
#             min_date_abertura = df['Data de abertura'].min().date()
#             max_date_abertura = df['Data de abertura'].max().date()
# 
#             start_date = st.sidebar.date_input("Data de Abertura (InÃ­cio)", min_value=min_date_abertura, max_value=max_date_abertura, value=min_date_abertura)
#             end_date = st.sidebar.date_input("Data de Abertura (Fim)", min_value=min_date_abertura, max_value=max_date_abertura, value=max_date_abertura)
# 
#             # Convert selected dates to datetime objects for filtering
#             start_datetime = pd.to_datetime(start_date)
#             end_datetime = pd.to_datetime(end_date)
# 
#             # Apply date filter
#             filtered_df = df[(df['Data de abertura'] >= start_datetime) & (df['Data de abertura'] <= end_datetime)].copy()
#         else:
#             filtered_df = df.copy() # If date column is empty, use the original df
# 
# 
#         # Filter by Estado
#         selected_estados = st.sidebar.multiselect("Selecionar Estado:", filtered_df['Estado'].unique(), filtered_df['Estado'].unique())
#         filtered_df = filtered_df[filtered_df['Estado'].isin(selected_estados)]
# 
#         # Filter by ResponsÃ¡vel (Initial filter)
#         selected_responsaveis_sidebar = st.sidebar.multiselect("Selecionar ResponsÃ¡vel (Filtro Inicial):", filtered_df['ResponsÃ¡vel'].unique(), filtered_df['ResponsÃ¡vel'].unique())
#         filtered_df = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_sidebar)]
# 
#         # Filter by EstÃ¡gio
#         selected_estagios = st.sidebar.multiselect("Selecionar EstÃ¡gio:", filtered_df['EstÃ¡gio'].unique(), filtered_df['EstÃ¡gio'].unique())
#         filtered_df = filtered_df[filtered_df['EstÃ¡gio'].isin(selected_estagios)]
# 
# 
#         # AgregaÃ§Ãµes para visualizaÃ§Ãµes (using filtered_df)
#         # Aggregate to count unique OC_Identifier per ResponsÃ¡vel
#         df_agg_responsavel_count = filtered_df.groupby('ResponsÃ¡vel')['OC_Identifier'].nunique().reset_index()
#         df_agg_responsavel_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
# 
# 
#         # Aggregate to count unique OC_Identifier per Estado and MonthYear_Abertura
#         df_agg_estado_mes_count = filtered_df.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#         df_agg_estado_mes_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#         df_agg_estado_mes_count['MonthYear_Abertura'] = df_agg_estado_mes_count['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
# 
#         # Filter for 'Ganha' deals for specific visualizations
#         ganha_df_filtered = filtered_df[filtered_df['Estado'] == 'Ganha'].copy()
# 
#         # --- Display Key Metrics (KPIs) ---
#         st.subheader("Resumo Geral")
#         col_kpi1, col_kpi2, col_kpi3 = st.columns(3)
# 
#         total_opportunities = filtered_df['OC_Identifier'].nunique() if not filtered_df.empty else 0
#         total_won_value = ganha_df_filtered['Valor'].sum() if not ganha_df_filtered.empty else 0
#         win_rate = (len(ganha_df_filtered) / total_opportunities * 100) if total_opportunities > 0 else 0
# 
# 
#         col_kpi1.metric("Total Oportunidades Ãšnicas", total_opportunities)
#         col_kpi2.metric("Valor Total Ganho", f"R$ {total_won_value:,.2f}")
#         col_kpi3.metric("Taxa de Sucesso", f"{win_rate:.2f}%")
# 
# 
#         # --- Visualizations ---
#         st.subheader("AnÃ¡lise de Oportunidades e Valor")
# 
#         # Row 1: Value/Count by Responsible and Deals by State/Month
#         col1, col2 = st.columns(2)
# 
#         with col1:
#             st.subheader("Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel")
#             if not df_agg_responsavel_count.empty:
#                 # Using Plotly Express for interactivity
#                 fig1 = px.bar(df_agg_responsavel_count, x='ResponsÃ¡vel', y='Unique Opportunity Count',
#                               title='Unique Opportunity Count per ResponsÃ¡vel')
# 
#                 # Capture selection
#                 selected_points = st.plotly_chart(fig1, use_container_width=True, on_select="rerun")
# 
#                 # Filter based on selection (using the original filtered_df and the selection from the chart)
#                 if selected_points and selected_points.selection and selected_points.selection.points:
#                      selected_responsaveis_chart = [point['x'] for point in selected_points.selection.points]
#                      filtered_df_chart_selection = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_chart)].copy()
#                 else:
#                      filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart selection
# 
#             else:
#                 st.info("Nenhum dado disponÃ­vel para 'Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel' com os filtros selecionados.")
#                 filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart data
# 
# 
#         with col2:
#              # Recalculate aggregated data based on chart selection
#              df_agg_estado_mes_count_filtered = filtered_df_chart_selection.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#              df_agg_estado_mes_count_filtered.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#              df_agg_estado_mes_count_filtered['MonthYear_Abertura'] = df_agg_estado_mes_count_filtered['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
#              st.subheader("Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura")
#              if not df_agg_estado_mes_count_filtered.empty:
#                  # Using Plotly Express for the improved chart
#                  fig2 = px.bar(df_agg_estado_mes_count_filtered, x='MonthYear_Abertura', y='Unique Opportunity Count', color='Estado',
#                                title='Unique Opportunity Count per Estado and Month of Opening (Filtered)',
#                                barmode='group') # Use 'group' or 'stack'
# 
#                  fig2.update_layout(xaxis_title="MÃªs/Ano de Abertura", yaxis_title="Quantidade de Oportunidades Ãšnicas")
#                  st.plotly_chart(fig2, use_container_width=True)
# 
#              else:
#                   st.info("Nenhum dado disponÃ­vel para 'Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura' com os filtros selecionados.")
# 
# 
#         # Row 2: Heatmap of Opportunities by Stage and Hour
#         st.subheader("Heatmap: Oportunidades por Etapa e Hora de Abertura")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered_for_heatmap = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
# 
#             # Aggregate data for the heatmap (count unique OC_Identifier per EstÃ¡gio and Hour_of_Day_Abertura)
#             # Ensure 'Hour_of_Day_Abertura' is extracted for the filtered timeline data
#             df_timeline_filtered_for_heatmap['Hour_of_Day_Abertura'] = df_timeline_filtered_for_heatmap['Data de abertura'].dt.hour
# 
#             if not df_timeline_filtered_for_heatmap.empty:
#                  heatmap_data = df_timeline_filtered_for_heatmap.groupby(['EstÃ¡gio', 'Hour_of_Day_Abertura'])['OC_Identifier'].nunique().unstack(fill_value=0)
# 
#                  if not heatmap_data.empty:
#                     # Use go.Heatmap to create the heatmap
#                     fig_heatmap = go.Figure(data=go.Heatmap(
#                            z=heatmap_data.values,
#                            x=heatmap_data.columns,
#                            y=heatmap_data.index,
#                            colorscale='Viridis')) # Use a color scale
# 
#                     fig_heatmap.update_layout(title='Opportunity Count by Stage and Hour of Opening (Filtered)',
#                                               xaxis_title='Hour of Day',
#                                               yaxis_title='Stage')
# 
#                     st.plotly_chart(fig_heatmap, use_container_width=True)
#                  else:
#                       st.info("Nenhum dado agregado disponÃ­vel para o Heatmap com os filtros selecionados.")
#             else:
#                  st.info("Nenhum dado de timeline disponÃ­vel para o Heatmap com os filtros selecionados.")
#         else:
#              st.info("Dados de timeline nÃ£o disponÃ­veis para o Heatmap.")
# 
# 
#         # Row 3: Distribution of EstÃ¡gios
#         st.subheader("AnÃ¡lise de EstÃ¡gios")
#         col5, col6 = st.columns(2)
# 
#         with col5:
#              # New Visualizations for 'Ganha' deals
#             if not ganha_df_filtered.empty:
#                 st.subheader("DistribuiÃ§Ã£o do Valor para NegÃ³cios Ganhos (Filtered)")
#                 fig3, ax3 = plt.subplots(figsize=(10, 6))
#                 sns.set_palette("viridis") # Set a color palette
#                 ax3.hist(ganha_df_filtered['Valor'].dropna(), bins=50)
#                 ax3.set_title('Distribution of Valor for Won Deals (Filtered)')
#                 ax3.set_xlabel('Valor')
#                 ax3.set_ylabel('Frequency')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig3)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para visualizaÃ§Ãµes de NegÃ³cios Ganhos com os filtros selecionados.")
# 
#         with col6:
#             if not filtered_df_chart_selection.empty:
#                 # Distribution of all stages (not just 'Ganha')
#                 st.subheader("DistribuiÃ§Ã£o de Todos os EstÃ¡gios (Filtered)")
#                 fig4, ax4 = plt.subplots(figsize=(12, 6))
#                 sns.set_palette("viridis") # Set a color palette
#                 filtered_df_chart_selection['EstÃ¡gio'].value_counts().plot(kind='bar', ax=ax4)
#                 ax4.set_title('Distribution of All Stages (Filtered)')
#                 ax4.set_xlabel('EstÃ¡gio')
#                 ax4.set_ylabel('Count')
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig4)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para 'DistribuiÃ§Ã£o de Todos os EstÃ¡gios' com os filtros selecionados.")
# 
# 
#         # Row 4: Timeline Analysis Metrics
#         st.subheader("AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio (Filtered)")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  # Filter timeline data based on selected OC_Identifiers from the chart selection
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
# 
#             if not df_timeline_filtered.empty:
#                 df_agg_time_per_stage_avg = df_timeline_filtered.groupby('EstÃ¡gio')['Time_in_Stage'].mean().reset_index()
#                 df_agg_time_per_stage_avg = df_agg_time_per_stage_avg.sort_values(by='Time_in_Stage', ascending=False)
# 
#                 # Format 'Time_in_Stage' for display to include minutes
#                 def format_time_in_stage(hours):
#                     if pd.isna(hours):
#                         return "N/A"
#                     total_minutes = int(hours * 60)
#                     days = total_minutes // (24 * 60)
#                     remaining_minutes_after_days = total_minutes % (24 * 60)
#                     hours = remaining_minutes_after_days // 60
#                     minutes = remaining_minutes_after_days % 60
#                     return f"{days} days, {hours} hours, {minutes} minutes"
# 
#                 df_agg_time_per_stage_avg['Average Time in Stage'] = df_agg_time_per_stage_avg['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#                 st.write("Tempo MÃ©dio em Cada EstÃ¡gio:")
#                 st.dataframe(df_agg_time_per_stage_avg[['EstÃ¡gio', 'Average Time in Stage']])
# 
#                 # Visualization of average time per stage (using the numerical value)
#                 st.subheader("Tempo MÃ©dio por EstÃ¡gio VisualizaÃ§Ã£o (Filtered)")
#                 fig5, ax5 = plt.subplots(figsize=(14, 7))
#                 sns.set_palette("viridis") # Set a color palette
#                 sns.barplot(x='EstÃ¡gio', y='Time_in_Stage', data=df_agg_time_per_stage_avg, ax=ax5)
#                 ax5.set_title('Average Time per Stage (Filtered by Data de Abertura and Selection)')
#                 ax5.set_xlabel('EstÃ¡gio')
#                 ax5.set_ylabel('Average Time (hours)') # Keep y-axis as hours for the plot to work directly with numerical data
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig5)
#             else:
#                  st.info("Nenhum dado disponÃ­vel para AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio com os filtros selecionados.")
#         else:
#             st.info("Dados de timeline nÃ£o disponÃ­veis.")
# 
# 
# elif page == "RelatÃ³rio de Oportunidade":
#     # --- RelatÃ³rio de Oportunidade Individual ---
#     st.title("RelatÃ³rio de Oportunidade Individual")
# 
#     st.markdown("""
#     Selecione um identificador de Oportunidade (OC + NÃºmero ou CTE + NÃºmero)
#     para visualizar sua linha do tempo e detalhes.
#     """)
# 
#     if not df.empty and not df_timeline.empty:
#         # Use OC_Identifier for selection
#         opportunity_identifiers = df['OC_Identifier'].dropna().unique()
#         selected_opportunity_identifier = st.selectbox("Selecionar Oportunidade (OC + NÃºmero ou CTE + NÃºmero):", opportunity_identifiers)
# 
#         st.subheader(f"Detalhes e Linha do Tempo para: {selected_opportunity_identifier}")
# 
#         # Filter main df for the selected opportunity identifier
#         opportunity_details_df = df[df['OC_Identifier'] == selected_opportunity_identifier]
# 
#         if not opportunity_details_df.empty:
#             opportunity_details = opportunity_details_df.iloc[0]
# 
#             # Use columns for a structured layout
#             col_info1, col_info2 = st.columns(2)
# 
#             with col_info1:
#                 st.write("**ID:**", opportunity_details['ID'])
#                 st.write("**TÃ­tulo:**", opportunity_details['TÃ­tulo'])
#                 st.write("**ResponsÃ¡vel:**", opportunity_details['ResponsÃ¡vel'])
#                 st.write("**Estado:**", opportunity_details['Estado'])
#                 st.write("**EstÃ¡gio Atual:**", opportunity_details['EstÃ¡gio']) # Display current stage
# 
#             with col_info2:
#                  st.write("**Valor:**", f"R$ {opportunity_details['Valor']:,.2f}" if pd.notna(opportunity_details['Valor']) else "N/A")
#                  st.write("**Origem:**", opportunity_details['Origem'])
#                  st.write("**Prob %:**", opportunity_details['Prob %'])
#                  st.write("**OC:**", opportunity_details['OC']) # Display OC column
# 
#             # Display Dates
#             st.subheader("Datas Principais")
#             col_dates1, col_dates2 = st.columns(2)
#             with col_dates1:
#                  st.write("**Data de Abertura:**", opportunity_details['Data de abertura'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data de abertura']) else "N/A")
#             with col_dates2:
#                  st.write("**Data de Fechamento:**", opportunity_details['Data fechamento'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data fechamento']) else "N/A")
# 
#             # Use an expander for Closing Details (if available)
#             if pd.notna(opportunity_details['Data fechamento']):
#                 with st.expander("Detalhes de Fechamento"):
#                     st.write("**Valor Fechamento:**", f"R$ {opportunity_details['Valor fechamento']:,.2f}" if pd.notna(opportunity_details['Valor fechamento']) else "N/A")
#                     st.write("**Valor Rec. Fechamento:**", f"R$ {opportunity_details['Valor rec. fechamento']:,.2f}" if pd.notna(opportunity_details['Valor rec. fechamento']) else "N/A")
#                     st.write("**RazÃ£o de Fechamento:**", opportunity_details['RazÃ£o de fechamento'])
#                     st.write("**ObservaÃ§Ã£o de Fechamento:**", opportunity_details['ObservaÃ§Ã£o de fechamento'])
# 
# 
#             # Filter timeline for the selected opportunity identifier
#             opportunity_timeline = df_timeline[df_timeline['OC_Identifier'] == selected_opportunity_identifier].copy()
# 
#             if not opportunity_timeline.empty:
#                 st.subheader("Linha do Tempo da Oportunidade")
# 
#                 # Display timeline using a more visually appealing table format
#                 display_timeline_cols = ['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']
#                 st.dataframe(opportunity_timeline[display_timeline_cols])
# 
# 
#             else:
#                 st.info(f"Nenhum dado de linha do tempo encontrado para: {selected_opportunity_identifier}")
# 
#             # --- AI Agent Interaction Section ---
#             st.subheader("Assistente de IA para Oportunidade")
#             if client: # Only display if OpenAI client is initialized
#                 user_query = st.text_area(f"FaÃ§a uma pergunta sobre a oportunidade {selected_opportunity_identifier}:", height=100)
#                 if st.button("Obter Resposta da IA"):
#                     with st.spinner("Obtendo resposta da IA..."):
#                         try:
#                             # Construct the prompt
#                             prompt = f"""
#                             VocÃª Ã© um assistente de BI focado em analisar dados de oportunidades de negÃ³cios.
#                             Sua tarefa Ã© responder a perguntas sobre uma oportunidade especÃ­fica com base nos dados fornecidos.
#                             Seja conciso e Ãºtil, focando em insights de BI e na progressÃ£o da oportunidade.
#                             **Use APENAS os dados fornecidos abaixo.**
#                             Se a pergunta do usuÃ¡rio nÃ£o puder ser respondida com os dados disponÃ­veis, diga isso de forma educada.
# 
#                             Dados da Oportunidade com identificador {selected_opportunity_identifier}:
# 
#                             Detalhes Principais:
#                             - ID: {opportunity_details.get('ID', 'N/A')}
#                             - TÃ­tulo: {opportunity_details.get('TÃ­tulo', 'N/A')}
#                             - ResponsÃ¡vel: {opportunity_details.get('ResponsÃ¡vel', 'N/A')}
#                             - Estado: {opportunity_details.get('Estado', 'N/A')}
#                             - EstÃ¡gio Atual: {opportunity_details.get('EstÃ¡gio', 'N/A')}
#                             - Valor: R$ {opportunity_details.get('Valor', 'N/A')}
#                             - Origem: {opportunity_details.get('Origem', 'N/A')}
#                             - Prob %: {opportunity_details.get('Prob %', 'N/A')}
#                             - OC: {opportunity_details.get('OC', 'N/A')}
#                             - Data de Abertura: {opportunity_details.get('Data de abertura', 'N/A')}
#                             - Data de Fechamento: {opportunity_details.get('Data fechamento', 'N/A')}
# 
#                             Detalhes de Fechamento (se aplicÃ¡vel):
#                             - Valor Fechamento: R$ {opportunity_details.get('Valor fechamento', 'N/A')}
#                             - Valor Rec. Fechamento: R$ {opportunity_details.get('Valor rec. fechamento', 'N/A')}
#                             - RazÃ£o de Fechamento: {opportunity_details.get('RazÃ£o de fechamento', 'N/A')}
#                             - ObservaÃ§Ã£o de Fechamento: {opportunity_details.get('ObservaÃ§Ã£o de fechamento', 'N/A')}
# 
#                             Linha do Tempo da Oportunidade (Progresso pelos EstÃ¡gios):
#                             {opportunity_timeline[['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']].to_string(index=False)}
# 
#                             Campos disponÃ­veis para anÃ¡lise nos Detalhes Principais: 'ID', 'TÃ­tulo', 'ResponsÃ¡vel', 'Estado', 'EstÃ¡gio', 'Valor', 'Origem', 'Prob %', 'OC', 'Data de Abertura', 'Data de Fechamento'.
#                             Campos disponÃ­veis para anÃ¡lise nos Detalhes de Fechamento: 'Valor Fechamento', 'Valor Rec. Fechamento', 'RazÃ£o de Fechamento', 'ObservaÃ§Ã£o de Fechamento'.
#                             Campos disponÃ­veis na Linha do Tempo: 'EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted'.
# 
#                             Pergunta do UsuÃ¡rio: {user_query}
#                             """
#                             # Call the OpenAI API
#                             response = client.chat.completions.create(
#                                 model="gpt-4o-mini", # Or another suitable model
#                                 messages=[
#                                     {"role": "system", "content": "VocÃª Ã© um assistente de BI Ãºtil. Responda em PortuguÃªs."},
#                                     {"role": "user", "content": prompt}
#                                 ],
#                                 max_tokens=500 # Limit response length to avoid excessive cost/output
#                             )
#                             ai_response = response.choices[0].message.content
#                             # Display the response with markdown
#                             st.markdown("---") # Add a separator
#                             st.markdown("**Resposta do Assistente de IA:**")
#                             st.markdown(ai_response)
#                         except Exception as e:
#                             st.error(f"Ocorreu um erro ao chamar a API da OpenAI: {e}")
#             else:
#                 st.warning("O cliente OpenAI nÃ£o estÃ¡ inicializado. A funcionalidade do assistente de IA estÃ¡ desativada.")
# 
# 
#         else:
#              st.info(f"Nenhum detalhe encontrado para: {selected_opportunity_identifier} no DataFrame principal.")
# 
#     elif df.empty:
#          st.warning("NÃ£o foi possÃ­vel carregar os dados. Verifique a conexÃ£o com o Google Sheet.")
#     else:
#          st.info("Dados de linha do tempo nÃ£o disponÃ­veis.")
# 
# 
# except Exception as e:
#     st.error(f"Erro ao carregar dados ou processar: {e}")

"""## Integrate agent into workflow

### Subtask:
Ensure the agent seamlessly fits into the user's workflow on the report page, perhaps allowing it to summarize key aspects of the displayed opportunity or answer specific questions about its timeline.

**Reasoning**:
Implement the logic to trigger the AI agent with predefined prompts or automatically generate a summary when an opportunity is selected, and ensure the agent's response is displayed clearly.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from google.colab import auth
# import gspread
# from google.auth import default
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as se
# import numpy as np
# import plotly.express as px
# import plotly.graph_objects as go
# import re
# from datetime import datetime
# # import openai # Removido a importaÃ§Ã£o do openai
# 
# st.set_page_config(layout="wide")
# 
# # Configure OpenAI API key and initialize client - Removido esta seÃ§Ã£o
# # if "OPENAI_API_KEY" in st.secrets:
# #     openai.api_key = st.secrets["OPENAI_API_KEY"]
# #     client = openai.OpenAI(api_key=openai.api_key)
# #     st.sidebar.success("OpenAI client initialized.")
# # else:
# #     st.sidebar.warning("OpenAI API key not found in Streamlit secrets. OpenAI features will be disabled.")
# #     client = None
# 
# @st.cache_data
# def load_data():
#     try:
#         auth.authenticate_user()
#         creds, _ = default()
#         gc = gspread.authorize(creds)
#         B2sheet = gc.open('BI_B2')
#         page = B2sheet.sheet1
#         all_data = page.get_all_values()
#         df = pd.DataFrame(all_data[1:], columns=all_data[0])
# 
#         value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
#         for col in value_cols:
#             df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
#             df[col] = pd.to_numeric(df[col], errors='coerce')
# 
#         date_cols = ['Data de abertura', 'Data fechamento']
#         for col in date_cols:
#             df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')
# 
#         def extract_oc_identifier(title):
#             if isinstance(title, str):
#                 match = re.search(r'(OC\s*\d+)', title, re.IGNORECASE)
#                 if match:
#                     return match.group(1).replace(" ", "")
#                 else:
#                     match_cte = re.search(r'(CTE\s*\d+)', title, re.IGNORECASE)
#                     if match_cte:
#                          return match_cte.group(1).replace(" ", "")
#             return None
# 
#         df['OC_Identifier'] = df['TÃ­tulo'].apply(extract_oc_identifier)
# 
#         df['Mes de Abertura'] = df['Data de abertura'].dt.month.fillna(0).astype(int)
#         df['Ano de Abertura'] = df['Data de abertura'].dt.year.fillna(0).astype(int)
#         df['Mes de Fechamento'] = df['Data fechamento'].dt.month.fillna(0).astype(int)
#         df['Ano de Fechamento'] = df['Data fechamento'].dt.year.fillna(0).astype(int)
# 
#         df['MonthYear_Abertura'] = df['Data de abertura'].dt.to_period('M')
#         df['MonthYear_Fechamento'] = df['Data fechamento'].dt.to_period('M')
# 
#         df['Hour_of_Day_Abertura'] = df['Data de abertura'].dt.hour
# 
#         df_timeline = df[['OC_Identifier', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()
#         df_timeline.dropna(subset=['OC_Identifier', 'Data de abertura'], inplace=True)
#         df_timeline = df_timeline.sort_values(by=['OC_Identifier', 'Data de abertura'])
# 
#         current_time = pd.to_datetime('now')
#         df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600
# 
#         df_timeline['Time_in_Stage'] = df_timeline.apply(
#             lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
#             axis=1
#         )
# 
#         def format_time_in_stage(hours):
#             if pd.isna(hours):
#                 return "N/A"
#             total_minutes = int(hours * 60)
#             days = total_minutes // (24 * 60)
#             remaining_minutes_after_days = total_minutes % (24 * 60)
#             hours = remaining_minutes_after_days // 60
#             minutes = remaining_minutes_after_days % 60
#             return f"{days} days, {hours} hours, {minutes} minutes"
# 
#         df_timeline['Time_in_Stage_Formatted'] = df_timeline['Time_in_Stage'].apply(format_time_in_stage)
# 
#         return df, df_timeline
# 
#     except Exception as e:
#         st.error(f"Erro ao carregar dados do Google Sheet: {e}")
#         return pd.DataFrame(), pd.DataFrame()
# 
# df, df_timeline = load_data()
# 
# st.sidebar.title("NavegaÃ§Ã£o")
# page = st.sidebar.radio("Ir para:", ["Painel Geral", "RelatÃ³rio de Oportunidade"])
# 
# if st.sidebar.button("Atualizar Dados"):
#     st.cache_data.clear()
#     st.rerun()
# 
# # Corrected the try-except block structure for the main part of the script
# try:
#     if page == "Painel Geral":
#         st.title("Painel de BI Operacional - Geral")
#         st.markdown("""
#         Este painel apresenta insights gerais sobre os dados operacionais,
#         permitindo acompanhar o desempenho e a distribuiÃ§Ã£o dos negÃ³cios.
#         """)
# 
#         if not df.empty:
#             st.sidebar.subheader("Filtros do Painel Geral")
#             if not df['Data de abertura'].empty:
#                 min_date_abertura = df['Data de abertura'].min().date()
#                 max_date_abertura = df['Data de abertura'].max().date()
#                 start_date = st.sidebar.date_input("Data de Abertura (InÃ­cio)", min_value=min_date_abertura, max_value=max_date_abertura, value=min_date_abertura)
#                 end_date = st.sidebar.date_input("Data de Abertura (Fim)", min_value=min_date_abertura, max_value=max_date_abertura, value=max_date_abertura)
#                 start_datetime = pd.to_datetime(start_date)
#                 end_datetime = pd.to_datetime(end_date)
#                 filtered_df = df[(df['Data de abertura'] >= start_datetime) & (df['Data de abertura'] <= end_datetime)].copy()
#             else:
#                 filtered_df = df.copy()
# 
#             selected_estados = st.sidebar.multiselect("Selecionar Estado:", filtered_df['Estado'].unique(), filtered_df['Estado'].unique())
#             filtered_df = filtered_df[filtered_df['Estado'].isin(selected_estados)]
#             selected_responsaveis_sidebar = st.sidebar.multiselect("Selecionar ResponsÃ¡vel (Filtro Inicial):", filtered_df['ResponsÃ¡vel'].unique(), filtered_df['ResponsÃ¡vel'].unique())
#             filtered_df = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_sidebar)]
#             selected_estagios = st.sidebar.multiselect("Selecionar EstÃ¡gio:", filtered_df['EstÃ¡gio'].unique(), filtered_df['EstÃ¡gio'].unique())
#             filtered_df = filtered_df[filtered_df['EstÃ¡gio'].isin(selected_estagios)]
# 
#             df_agg_responsavel_count = filtered_df.groupby('ResponsÃ¡vel')['OC_Identifier'].nunique().reset_index()
#             df_agg_responsavel_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#             df_agg_estado_mes_count = filtered_df.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#             df_agg_estado_mes_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#             df_agg_estado_mes_count['MonthYear_Abertura'] = df_agg_estado_mes_count['MonthYear_Abertura'].astype(str)
# 
#             ganha_df_filtered = filtered_df[filtered_df['Estado'] == 'Ganha'].copy()
# 
#             st.subheader("Resumo Geral")
#             col_kpi1, col_kpi2, col_kpi3 = st.columns(3)
#             total_opportunities = filtered_df['OC_Identifier'].nunique() if not filtered_df.empty else 0
#             total_won_value = ganha_df_filtered['Valor'].sum() if not ganha_df_filtered.empty else 0
#             win_rate = (len(ganha_df_filtered) / total_opportunities * 100) if total_opportunities > 0 else 0
# 
#             col_kpi1.metric("Total Oportunidades Ãšnicas", total_opportunities)
#             col_kpi2.metric("Valor Total Ganho", f"R$ {total_won_value:,.2f}")
#             col_kpi3.metric("Taxa de Sucesso", f"{win_rate:.2f}%")
# 
#             st.subheader("AnÃ¡lise de Oportunidades e Valor")
#             col1, col2 = st.columns(2)
# 
#             with col1:
#                 st.subheader("Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel")
#                 if not df_agg_responsavel_count.empty:
#                     fig1 = px.bar(df_agg_responsavel_count, x='ResponsÃ¡vel', y='Unique Opportunity Count',
#                                   title='Unique Opportunity Count per ResponsÃ¡vel')
#                     selected_points = st.plotly_chart(fig1, use_container_width=True, on_select="rerun")
#                     if selected_points and selected_points.selection and selected_points.selection.points:
#                          selected_responsaveis_chart = [point['x'] for point in selected_points.selection.points]
#                          filtered_df_chart_selection = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_chart)].copy()
#                     else:
#                          filtered_df_chart_selection = filtered_df.copy()
#                 else:
#                     st.info("Nenhum dado disponÃ­vel para 'Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel' com os filtros selecionados.")
#                     filtered_df_chart_selection = filtered_df.copy()
# 
#             with col2:
#                  df_agg_estado_mes_count_filtered = filtered_df_chart_selection.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#                  df_agg_estado_mes_count_filtered.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#                  df_agg_estado_mes_count_filtered['MonthYear_Abertura'] = df_agg_estado_mes_count_filtered['MonthYear_Abertura'].astype(str)
#                  st.subheader("Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura")
#                  if not df_agg_estado_mes_count_filtered.empty:
#                      fig2 = px.bar(df_agg_estado_mes_count_filtered, x='MonthYear_Abertura', y='Unique Opportunity Count', color='Estado',
#                                    title='Unique Opportunity Count per Estado and Month of Opening (Filtered)',
#                                    barmode='group')
#                      fig2.update_layout(xaxis_title="MÃªs/Ano de Abertura", yaxis_title="Quantidade de Oportunidades Ãšnicas")
#                      st.plotly_chart(fig2, use_container_width=True)
#                  else:
#                       st.info("Nenhum dado disponÃ­vel para 'Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura' com os filtros selecionados.")
# 
#             st.subheader("Heatmap: Oportunidades por Etapa e Hora de Abertura")
#             if not df_timeline.empty:
#                 df_timeline_filtered_for_heatmap = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
#                 if selected_points and selected_points.selection and selected_points.selection.points:
#                      selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                      df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#                 df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
#                 df_timeline_filtered_for_heatmap['Hour_of_Day_Abertura'] = df_timeline_filtered_for_heatmap['Data de abertura'].dt.hour
# 
#                 if not df_timeline_filtered_for_heatmap.empty:
#                      heatmap_data = df_timeline_filtered_for_heatmap.groupby(['EstÃ¡gio', 'Hour_of_Day_Abertura'])['OC_Identifier'].nunique().unstack(fill_value=0)
#                      if not heatmap_data.empty:
#                         fig_heatmap = go.Figure(data=go.Heatmap(
#                                z=heatmap_data.values,
#                                x=heatmap_data.columns,
#                                y=heatmap_data.index,
#                                colorscale='Viridis'))
#                         fig_heatmap.update_layout(title='Opportunity Count by Stage and Hour of Opening (Filtered)',
#                                                   xaxis_title='Hour of Day',
#                                                   yaxis_title='Stage')
#                         st.plotly_chart(fig_heatmap, use_container_width=True)
#                      else:
#                           st.info("Nenhum dado agregado disponÃ­vel para o Heatmap com os filtros selecionados.")
#                 else:
#                      st.info("Nenhum dado de timeline disponÃ­vel para o Heatmap com os filtros selecionados.")
#             else:
#                  st.info("Dados de timeline nÃ£o disponÃ­veis para o Heatmap.")
# 
#             st.subheader("AnÃ¡lise de EstÃ¡gios")
#             col5, col6 = st.columns(2)
# 
#             with col5:
#                 if not ganha_df_filtered.empty:
#                     st.subheader("DistribuiÃ§Ã£o do Valor para NegÃ³cios Ganhos (Filtered)")
#                     fig3, ax3 = plt.subplots(figsize=(10, 6))
#                     se.set_palette("viridis")
#                     ax3.hist(ganha_df_filtered['Valor'].dropna(), bins=50)
#                     ax3.set_title('Distribution of Valor for Won Deals (Filtered)')
#                     ax3.set_xlabel('Valor')
#                     ax3.set_ylabel('Frequency')
#                     plt.tight_layout()
#                     st.pyplot(fig3)
#                 else:
#                     st.info("Nenhum dado disponÃ­vel para visualizaÃ§Ãµes de NegÃ³cios Ganhos com os filtros selecionados.")
# 
#             with col6:
#                 if not filtered_df_chart_selection.empty:
#                     st.subheader("DistribuiÃ§Ã£o de Todos os EstÃ¡gios (Filtered)")
#                     fig4, ax4 = plt.subplots(figsize=(12, 6))
#                     se.set_palette("viridis")
#                     filtered_df_chart_selection['EstÃ¡gio'].value_counts().plot(kind='bar', ax=ax4)
#                     ax4.set_title('Distribution of All Stages (Filtered)')
#                     ax4.set_xlabel('EstÃ¡gio')
#                     ax4.set_ylabel('Count')
#                     plt.xticks(rotation=45, ha='right')
#                     plt.tight_layout()
#                     st.pyplot(fig4)
#                 else:
#                     st.info("Nenhum dado disponÃ­vel para 'DistribuiÃ§Ã£o de Todos os EstÃ¡gios' com os filtros selecionados.")
# 
#             st.subheader("AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio (Filtered)")
#             if not df_timeline.empty:
#                 df_timeline_filtered = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
#                 if selected_points and selected_points.selection and selected_points.selection.points:
#                      selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                      df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#                 df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
#                 if not df_timeline_filtered.empty:
#                     df_agg_time_per_stage_avg = df_timeline_filtered.groupby('EstÃ¡gio')['Time_in_Stage'].mean().reset_index()
#                     df_agg_time_per_stage_avg = df_agg_time_per_stage_avg.sort_values(by='Time_in_Stage', ascending=False)
# 
#                     def format_time_in_stage(hours):
#                         if pd.isna(hours):
#                             return "N/A"
#                         total_minutes = int(hours * 60)
#                         days = total_minutes // (24 * 60)
#                         remaining_minutes_after_days = total_minutes % (24 * 60)
#                         hours = remaining_minutes_after_days // 60
#                         minutes = remaining_minutes_after_days % 60
#                         return f"{days} days, {hours} hours, {minutes} minutes"
# 
#                     df_agg_time_per_stage_avg['Average Time in Stage'] = df_agg_time_per_stage_avg['Time_in_Stage'].apply(format_time_in_stage)
# 
#                     st.write("Tempo MÃ©dio em Cada EstÃ¡gio:")
#                     st.dataframe(df_agg_time_per_stage_avg[['EstÃ¡gio', 'Average Time in Stage']])
# 
#                     st.subheader("Tempo MÃ©dio por EstÃ¡gio VisualizaÃ§Ã£o (Filtered)")
#                     fig5, ax5 = plt.subplots(figsize=(14, 7))
#                     se.set_palette("viridis")
#                     se.barplot(x='EstÃ¡gio', y='Time_in_Stage', data=df_agg_time_per_stage_avg, ax=ax5)
#                     ax5.set_title('Average Time per Stage (Filtered by Data de Abertura and Selection)')
#                     ax5.set_xlabel('EstÃ¡gio')
#                     ax5.set_ylabel('Average Time (hours)')
#                     plt.xticks(rotation=45, ha='right')
#                     plt.tight_layout()
#                     st.pyplot(fig5)
#                 else:
#                      st.info("Nenhum dado disponÃ­vel para AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio com os filtros selecionados.")
#             else:
#                 st.info("Dados de timeline nÃ£o disponÃ­veis.")
# 
#     elif page == "RelatÃ³rio de Oportunidade":
#         st.title("RelatÃ³rio de Oportunidade Individual")
#         st.markdown("""
#         Selecione um identificador de Oportunidade (OC + NÃºmero ou CTE + NÃºmero)
#         para visualizar sua linha do tempo e detalhes.
#         """)
# 
#         if not df.empty and not df_timeline.empty:
#             opportunity_identifiers = df['OC_Identifier'].dropna().unique()
#             selected_opportunity_identifier = st.selectbox("Selecionar Oportunidade (OC + NÃºmero ou CTE + NÃºmero):", opportunity_identifiers)
# 
#             st.subheader(f"Detalhes e Linha do Tempo para: {selected_opportunity_identifier}")
# 
#             opportunity_details_df = df[df['OC_Identifier'] == selected_opportunity_identifier]
# 
#             if not opportunity_details_df.empty:
#                 opportunity_details = opportunity_details_df.iloc[0]
# 
#                 col_info1, col_info2 = st.columns(2)
# 
#                 with col_info1:
#                     st.write("**ID:**", opportunity_details['ID'])
#                     st.write("**TÃ­tulo:**", opportunity_details['TÃ­tulo'])
#                     st.write("**ResponsÃ¡vel:**", opportunity_details['ResponsÃ¡vel'])
#                     st.write("**Estado:**", opportunity_details['Estado'])
#                     st.write("**EstÃ¡gio Atual:**", opportunity_details['EstÃ¡gio'])
# 
#                 with col_info2:
#                      st.write("**Valor:**", f"R$ {opportunity_details['Valor']:,.2f}" if pd.notna(opportunity_details['Valor']) else "N/A")
#                      st.write("**Origem:**", opportunity_details['Origem'])
#                      st.write("**Prob %:**", opportunity_details['Prob %'])
#                      # Removed the line causing the error: st.write("**OC:**", opportunity_details['OC'])
# 
#                 st.subheader("Datas Principais")
#                 col_dates1, col_dates2 = st.columns(2)
#                 with col_dates1:
#                      st.write("**Data de Abertura:**", opportunity_details['Data de abertura'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data de abertura']) else "N/A")
#                 with col_dates2:
#                      st.write("**Data de Fechamento:**", opportunity_details['Data fechamento'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data fechamento']) else "N/A")
# 
#                 if pd.notna(opportunity_details['Data fechamento']):
#                     with st.expander("Detalhes de Fechamento"):
#                         st.write("**Valor Fechamento:**", f"R$ {opportunity_details['Valor fechamento']:,.2f}" if pd.notna(opportunity_details['Valor fechamento']) else "N/A")
#                         st.write("**Valor Rec. Fechamento:**", f"R$ {opportunity_details['Valor rec. fechamento']:,.2f}" if pd.notna(opportunity_details['Valor rec. fechamento']) else "N/A")
#                         st.write("**RazÃ£o de Fechamento:**", opportunity_details['RazÃ£o de fechamento'])
#                         st.write("**ObservaÃ§Ã£o de Fechamento:**", opportunity_details['ObservaÃ§Ã£o de fechamento'])
# 
#                 opportunity_timeline = df_timeline[df_timeline['OC_Identifier'] == selected_opportunity_identifier].copy()
# 
#                 if not opportunity_timeline.empty:
#                     st.subheader("Linha do Tempo da Oportunidade")
#                     display_timeline_cols = ['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']
#                     st.dataframe(opportunity_timeline[display_timeline_cols])
#                 else:
#                     st.info(f"Nenhum dado de linha do tempo encontrado para: {selected_opportunity_identifier}")
# 
#                 # --- AI Agent Interaction Section - Removido esta seÃ§Ã£o
#                 # st.subheader("Assistente de IA para Oportunidade")
#                 # if client:
#                 #     # Predefined prompts
#                 #     predefined_prompts = [
#                 #         "Resuma esta oportunidade.",
#                 #         "Analise a linha do tempo desta oportunidade.",
#                 #         "Quais sÃ£o os detalhes principais?",
#                 #         "Qual o valor desta oportunidade?",
#                 #         "Quem Ã© o responsÃ¡vel por esta oportunidade?"
#                 #     ]
#                 #     selected_predefined_prompt = st.selectbox("Ou selecione uma pergunta predefinida:", [""] + predefined_prompts)
# 
#                 #     user_query = st.text_area(f"FaÃ§a uma pergunta sobre a oportunidade {selected_opportunity_identifier}:", height=100)
# 
#                 #     # Combine predefined prompt with user query if both are provided
#                 #     if selected_predefined_prompt and user_query:
#                 #         final_query = f"{selected_predefined_prompt} {user_query}"
#                 #     elif selected_predefined_prompt:
#                 #         final_query = selected_predefined_prompt
#                 #     else:
#                 #         final_query = user_query
# 
#                 #     # Button to trigger the AI
#                 #     if st.button("Obter Resposta da IA") and final_query:
#                 #         with st.spinner("Obtendo resposta da IA..."):
#                 #             try:
#                 #                 prompt = f"""
#                 #                 VocÃª Ã© um assistente de BI focado em analisar dados de oportunidades de negÃ³cios.
#                 #                 Sua tarefa Ã© responder a perguntas sobre uma oportunidade especÃ­fica com base nos dados fornecidos.
#                 #                 Seja conciso e Ãºtil, focando em insights de BI e na progressÃ£o da oportunidade.
#                 #                 **Use APENAS os dados fornecidos abaixo.**
#                 #                 Se a pergunta do usuÃ¡rio nÃ£o puder ser respondida com os dados disponÃ­veis, diga isso de forma educada.
# 
#                 #                 Dados da Oportunidade com identificador {selected_opportunity_identifier}:
# 
#                 #                 Detalhes Principais:
#                 #                 - ID: {opportunity_details.get('ID', 'N/A')}
#                 #                 - TÃ­tulo: {opportunity_details.get('TÃ­tulo', 'N/A')}
#                 #                 - ResponsÃ¡vel: {opportunity_details.get('ResponsÃ¡vel', 'N/A')}
#                 #                 - Estado: {opportunity_details.get('Estado', 'N/A')}
#                 #                 - EstÃ¡gio Atual: {opportunity_details.get('EstÃ¡gio', 'N/A')}
#                 #                 - Valor: R$ {opportunity_details.get('Valor', 'N/A')}
#                 #                 - Origem: {opportunity_details.get('Origem', 'N/A')}
#                 #                 - Prob %: {opportunity_details.get('Prob %', 'N/A')}
#                 #                 - OC: {opportunity_details.get('OC', 'N/A')}
#                 #                 - Data de Abertura: {opportunity_details.get('Data de abertura', 'N/A')}
#                 #                 - Data de Fechamento: {opportunity_details.get('Data fechamento', 'N/A')}
# 
#                 #                 Detalhes de Fechamento (se aplicÃ¡vel):
#                 #                 - Valor Fechamento: R$ {opportunity_details.get('Valor fechamento', 'N/A')}
#                 #                 - Valor Rec. Fechamento: R$ {opportunity_details.get('Valor rec. fechamento', 'N/A')}
#                 #                 - RazÃ£o de Fechamento: {opportunity_details.get('RazÃ£o de Fechamento', 'N/A')}
#                 #                 - ObservaÃ§Ã£o de Fechamento: {opportunity_details.get('ObservaÃ§Ã£o de Fechamento', 'N/A')}
# 
#                 #                 Linha do Tempo da Oportunidade (Progresso pelos EstÃ¡gios):
#                 #                 {opportunity_timeline[['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']].to_string(index=False)}
# 
#                 #                 Campos disponÃ­veis para anÃ¡lise nos Detalhes Principais: 'ID', 'TÃ­tulo', 'ResponsÃ¡vel', 'Estado', 'EstÃ¡gio', 'Valor', 'Origem', 'Prob %', 'OC', 'Data de Abertura', 'Data de Fechamento'.
#                 #                 Campos disponÃ­veis para anÃ¡lise nos Detalhes de Fechamento: 'Valor Fechamento', 'Valor Rec. Fechamento', 'RazÃ£o de Fechamento', 'ObservaÃ§Ã£o de Fechamento'.
#                 #                 Campos disponÃ­veis na Linha do Tempo: 'EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted'.
# 
#                 #                 Pergunta do UsuÃ¡rio: {final_query}
#                 #                 """
#                 #                 response = client.chat.completions.create(
#                 #                     model="gpt-4o-mini",
#                 #                     messages=[
#                 #                         {"role": "system", "content": "VocÃª Ã© um assistente de BI Ãºtil. Responda em PortuguÃªs."},
#                 #                         {"role": "user", "content": prompt}
#                 #                     ],
#                 #                     max_tokens=500
#                 #                 )
#                 #                 ai_response = response.choices[0].message.content
#                 #                 st.markdown("---")
#                 #                 st.markdown("**Resposta do Assistente de IA:**")
#                 #                 st.markdown(ai_response)
#                 #             except Exception as e:
#                 #                 st.error(f"Ocorreu um erro ao chamar a API da OpenAI: {e}")
#                 # elif st.button("Gerar Resumo da Oportunidade"):
#                 #      with st.spinner("Gerando resumo da oportunidade..."):
#                 #         try:
#                 #             prompt = f"""
#                 #             VocÃª Ã© um assistente de BI focado em analisar dados de oportunidades de negÃ³cios.
#                 #             Sua tarefa Ã© gerar um resumo conciso e Ãºtil da oportunidade com o identificador {selected_opportunity_identifier}
#                 #             com base nos dados fornecidos abaixo.
#                 #             **Use APENAS os dados fornecidos.**
#                 #             Inclua detalhes importantes como o estado atual, valor, responsÃ¡vel e uma breve anÃ¡lise da linha do tempo, se aplicÃ¡vel.
# 
#                 #             Dados da Oportunidade com identificador {selected_opportunity_identifier}:
# 
#                 #             Detalhes Principais:
#                 #             - ID: {opportunity_details.get('ID', 'N/A')}
#                 #             - TÃ­tulo: {opportunity_details.get('TÃ­tulo', 'N/A')}
#                 #             - ResponsÃ¡vel: {opportunity_details.get('ResponsÃ¡vel', 'N/A')}
#                 #             - Estado: {opportunity_details.get('Estado', 'N/A')}
#                 #             - EstÃ¡gio Atual: {opportunity_details.get('EstÃ¡gio', 'N/A')}
#                 #             - Valor: R$ {opportunity_details.get('Valor', 'N/A')}
#                 #             - Origem: {opportunity_details.get('Origem', 'N/A')}
#                 #             - Prob %: {opportunity_details.get('Prob %', 'N/A')}
#                 #             - OC: {opportunity_details.get('OC', 'N/A')}
#                 #             - Data de Abertura: {opportunity_details.get('Data de abertura', 'N/A')}
#                 #             - Data de Fechamento: {opportunity_details.get('Data fechamento', 'N/A')}
# 
#                 #             Detalhes de Fechamento (se aplicÃ¡vel):
#                 #             - Valor Fechamento: R$ {opportunity_details.get('Valor fechamento', 'N/A')}
#                 #             - Valor Rec. Fechamento: R$ {opportunity_details.get('Valor rec. fechamento', 'N/A')}
#                 #             - RazÃ£o de Fechamento: {opportunity_details.get('RazÃ£o de fechamento', 'N/A')}
#                 #             - ObservaÃ§Ã£o de Fechamento: {opportunity_details.get('ObservaÃ§Ã£o de fechamento', 'N/A')}
# 
#                 #             Linha do Tempo da Oportunidade (Progresso pelos EstÃ¡gios):
#                 #             {opportunity_timeline[['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']].to_string(index=False)}
# 
#                 #             Gere o resumo agora.
#                 #             """
#                 #             response = client.chat.completions.create(
#                 #                 model="gpt-4o-mini",
#                 #                 messages=[
#                 #                     {"role": "system", "content": "VocÃª Ã© um assistente de BI Ãºtil. Responda em PortuguÃªs."},
#                 #                     {"role": "user", "content": prompt}
#                 #                 ],
#                 #                 max_tokens=500
#                 #             )
#                 #             ai_response = response.choices[0].message.content
#                 #             st.markdown("---")
#                 #             st.markdown("**Resumo da Oportunidade:**")
#                 #             st.markdown(ai_response)
#                 #         except Exception as e:
#                 #             st.error(f"Ocorreu um erro ao chamar a API da OpenAI para gerar o resumo: {e}")
# 
# 
#                 # else:
#                 #     st.warning("O cliente OpenAI nÃ£o estÃ¡ inicializado. A funcionalidade do assistente de IA estÃ¡ desativada.")
# 
#             else:
#                  st.info(f"Nenhum detalhe encontrado para: {selected_opportunity_identifier} no DataFrame principal.")
# 
#         elif df.empty:
#              st.warning("NÃ£o foi possÃ­vel carregar os dados. Verifique a conexÃ£o com o Google Sheet.")
#         else:
#              st.info("Dados de linha do tempo nÃ£o disponÃ­veis.")
# 
# except Exception as e:
#     st.error(f"Erro ao carregar dados ou processar: {e}")

!pip install streamlit

!streamlit run app.py

# instalar pyngrok (se ainda nÃ£o estiver instalado)
!pip install pyngrok

# no Python (Colab) â€” use seu authtoken do ngrok das secrets do Colab
from pyngrok import ngrok
from google.colab import userdata

# Use o authtoken armazenado nas secrets do Colab
try:
    ngrok_authtoken = userdata.get('NGROK_AUTHTOKEN') # Assuming you stored it as NGROK_AUTHTOKEN
    ngrok.set_auth_token(ngrok_authtoken)
    print("ngrok authtoken configurado com sucesso.")
except userdata.SecretNotFoundError:
    print("NGROK_AUTHTOKEN nÃ£o encontrado nas secrets do Colab. Por favor, adicione-o.")
    print("VÃ¡ para a barra lateral esquerda (ğŸ”‘), expanda 'Secrets', e adicione um secret chamado NGROK_AUTHTOKEN.")
except Exception as e:
    print(f"Ocorreu um erro ao configurar o authtoken do ngrok: {e}")

# Conectar o ngrok Ã  porta 8501 (porta padrÃ£o do Streamlit)
try:
    public_url = ngrok.connect(8501, "http")
    print(f"URL pÃºblico do ngrok: {public_url}")
    print("Clique no URL acima para acessar o Streamlit app.")
except Exception as e:
    print(f"Ocorreu um erro ao conectar o ngrok: {e}")

# Kill any running streamlit processes
!pkill streamlit

# Run the streamlit app in the background
!streamlit run app.py &>/dev/null&

# instalar pyngrok (se ainda nÃ£o estiver instalado)
!pip install pyngrok

# no Python (Colab) â€” use seu authtoken do ngrok das secrets do Colab
from pyngrok import ngrok
from google.colab import userdata

# Use o authtoken armazenado nas secrets do Colab
try:
    ngrok_authtoken = userdata.get('NGROK_AUTHTOKEN') # Assuming you stored it as NGROK_AUTHTOKEN
    ngrok.set_auth_token(ngrok_authtoken)
    print("ngrok authtoken configurado com sucesso.")
except userdata.SecretNotFoundError:
    print("NGROK_AUTHTOKEN nÃ£o encontrado nas secrets do Colab. Por favor, adicione-o.")
    print("VÃ¡ para a barra lateral esquerda (ğŸ”‘), expanda 'Secrets', e adicione um secret chamado NGROK_AUTHTOKEN.")
except Exception as e:
    print(f"Ocorreu um erro ao configurar o authtoken do ngrok: {e}")

# Conectar o ngrok Ã  porta 8501 (porta padrÃ£o do Streamlit)
try:
    # Disconnect any existing tunnels
    ngrok.disconnect(public_url)
except:
    pass # Ignore if no tunnel exists

try:
    public_url = ngrok.connect(8501, "http")
    print(f"URL pÃºblico do ngrok: {public_url}")
    print("Clique no URL acima para acessar o Streamlit app.")
except Exception as e:
    print(f"Ocorreu um erro ao conectar o ngrok: {e}")

# Keep the cell alive for testing
# This will print a message every minute to indicate the cell is still running
import time
while True:
    time.sleep(60)
    print("Cell active for testing...")

# Kill any running streamlit processes
!pkill streamlit

# Run the streamlit app in the background
!streamlit run app.py &>/dev/null&

# instalar pyngrok (se ainda nÃ£o estiver instalado)
!pip install pyngrok

# no Python (Colab) â€” use seu authtoken do ngrok das secrets do Colab
from pyngrok import ngrok
from google.colab import userdata

# Use o authtoken armazenado nas secrets do Colab
try:
    ngrok_authtoken = userdata.get('NGROK_AUTHTOKEN') # Assuming you stored it as NGROK_AUTHTOKEN
    ngrok.set_auth_token(ngrok_authtoken)
    print("ngrok authtoken configurado com sucesso.")
except userdata.SecretNotFoundError:
    print("NGROK_AUTHTOKEN nÃ£o encontrado nas secrets do Colab. Por favor, adicione-o.")
    print("VÃ¡ para a barra lateral esquerda (ğŸ”‘), expanda 'Secrets', e adicione um secret chamado NGROK_AUTHTOKEN.")
except Exception as e:
    print(f"Ocorreu um erro ao configurar o authtoken do ngrok: {e}")

# Conectar o ngrok Ã  porta 8501 (porta padrÃ£o do Streamlit)
try:
    # Disconnect any existing tunnels
    ngrok.disconnect(public_url)
except:
    pass # Ignore if no tunnel exists

try:
    public_url = ngrok.connect(8501, "http")
    print(f"URL pÃºblico do ngrok: {public_url}")
    print("Clique no URL acima para acessar o Streamlit app.")
except Exception as e:
    print(f"Ocorreu um erro ao conectar o ngrok: {e}")

import os

secrets_dir = '/content/.streamlit'
secrets_file = os.path.join(secrets_dir, 'secrets.toml')

if os.path.exists(secrets_dir):
    print(f"Directory '{secrets_dir}' found.")
    if os.path.exists(secrets_file):
        print(f"File '{secrets_file}' found.")
        print("\nContents of secrets.toml:")
        try:
            with open(secrets_file, 'r') as f:
                print(f.read())
        except Exception as e:
            print(f"Error reading secrets.toml: {e}")
    else:
        print(f"File '{secrets_file}' not found.")
else:
    print(f"Directory '{secrets_dir}' not found.")
    print("Please create the directory and the secrets.toml file as instructed.")

"""## Summary:

### Data Analysis Key Findings

*   The "RelatÃ³rio de Oportunidade" page was successfully enhanced with improved layout using columns and expanders, displaying additional relevant details like 'Origem', 'Prob %', and 'OC'.
*   Currency and date values on the report page are now formatted for better readability.
*   The timeline data for individual opportunities is displayed in a table format with a human-readable "Time in Stage" column.
*   The Streamlit application is configured to securely load the OpenAI API key from secrets and initialize the OpenAI client, with a warning displayed if the key is not found.
*   An interactive UI for the OpenAI agent was added to the "RelatÃ³rio de Oportunidade" page, including a selectbox for predefined prompts, a text area for custom queries, and buttons to trigger the AI. This section is only visible if the OpenAI client is initialized.
*   The logic to construct a detailed prompt for the OpenAI agent using the specific opportunity's data (details and timeline) was implemented.
*   The application now makes calls to the OpenAI API (`gpt-4o-mini` model) with the constructed prompt and displays the agent's response using markdown formatting.
*   The agent is explicitly instructed to use only the provided data and handle questions that cannot be answered with the available context.
*   Separate buttons were added to either get a response to a specific question or generate a general summary of the opportunity.

### Insights or Next Steps

*   Implement more sophisticated data extraction from the Google Sheet to handle potential variations in column names or data structures more robustly.
*   Explore adding more complex analysis capabilities to the AI agent, such as identifying potential bottlenecks in the opportunity timeline or suggesting next steps based on the current stage and history.

# Task
Fix the bugs in the "RelatÃ³rio de Oportunidade" section of the Streamlit application in the file "/content/app.py".

## Review data filtering and display

### Subtask:
Review data filtering and display

**Reasoning**:
The subtask is to review the data filtering and display logic for the "RelatÃ³rio de Oportunidade" page in `app.py`. This involves examining the code responsible for filtering `df` and `df_timeline` based on the selected opportunity identifier and reviewing how the details and timeline are displayed. I will read the relevant section of the `app.py` file and describe the filtering and display logic. I will then use the `finish_task` command as the subtask is completed.
"""

with open('/content/app.py', 'r') as f:
    app_code = f.read()

# Extracting the relevant section for "RelatÃ³rio de Oportunidade"
report_section_start = app_code.find('elif page == "RelatÃ³rio de Oportunidade":')
report_section_end = app_code.find('except Exception as e:')

report_code = app_code[report_section_start:report_section_end]

print("--- Code for 'RelatÃ³rio de Oportunidade' section ---")
print(report_code)
print("-----------------------------------------------------")

print("\nReview of Filtering and Display Logic:")

# Filtering logic review
print("\nFiltering Logic:")
print("The code filters the main DataFrame `df` to get `opportunity_details_df` using `df[df['OC_Identifier'] == selected_opportunity_identifier]`. It then takes the first row using `.iloc[0]` to get `opportunity_details`.")
print("The timeline DataFrame `df_timeline` is filtered to get `opportunity_timeline` using `df_timeline[df_timeline['OC_Identifier'] == selected_opportunity_identifier]`. This seems correct for isolating the specific opportunity's timeline entries.")

# Display logic review
print("\nDisplay Logic:")
print("Opportunity details are displayed using `st.write` within two columns (`col_info1`, `col_info2`).")
print("Key fields displayed: 'ID', 'TÃ­tulo', 'ResponsÃ¡vel', 'Estado', 'EstÃ¡gio Atual', 'Valor', 'Origem', 'Prob %', 'OC'.")
print("Date fields ('Data de Abertura', 'Data de Fechamento') are displayed in separate columns (`col_dates1`, `col_dates2`) and formatted using `.strftime('%d/%m/%Y %H:%M:%S')` if not NaT.")
print("Closing details ('Valor Fechamento', 'Valor Rec. Fechamento', 'RazÃ£o de Fechamento', 'ObservaÃ§Ã£o de Fechamento') are displayed within an `st.expander` if 'Data fechamento' is not NaT.")
print("The timeline is displayed using `st.dataframe` with selected columns: 'EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted'.")
print("Missing values are handled during display for value and date fields by checking for `pd.notna` and displaying 'N/A' or using conditional formatting.")
print("The 'OC' column display had a potential error in a previous version, but it appears to be corrected to use `.get('OC', 'N/A')` or similar safe access in the latest `app.py` (based on the diff), preventing errors if the column is missing or empty. The provided code snippet still shows `opportunity_details['OC']` which might cause an error if 'OC' key is missing. This should be verified against the actual `df` structure.")

"""## Refine timeline calculation and display

### Subtask:
Re-check the logic for calculating `Time_in_Stage` in `df_timeline` to ensure accuracy, especially for open opportunities. Confirm that the formatted time is displayed correctly.

**Reasoning**:
Examine the code for calculating and formatting `Time_in_Stage` in `app.py` and confirm its correctness and usage.
"""

with open('/content/app.py', 'r') as f:
    app_code = f.read()

# Find the relevant sections in the code
load_data_start = app_code.find('def load_data():')
df_timeline_calculation_start = app_code.find('# Calculate Time in Stage for timeline analysis using OC_Identifier', load_data_start)
df_timeline_calculation_end = app_code.find('return df, df_timeline', df_timeline_calculation_start)

# Extract the code related to df_timeline calculation and formatting
df_timeline_code = app_code[df_timeline_calculation_start:df_timeline_calculation_end]

print("--- Code for df_timeline calculation and formatting ---")
print(df_timeline_code)
print("-------------------------------------------------------")

# Check the logic for calculating Time_in_Stage
print("\nReview of Time_in_Stage Calculation:")
print("The code calculates `Time_in_Stage` initially as the difference between 'Data fechamento' and 'Data de abertura' in hours.")
print("It then uses `.apply` with a lambda function to handle rows where 'Data fechamento' is missing (`pd.isna`). For these rows, it calculates the time difference between the `current_time` and 'Data de abertura'.")
print("This logic correctly handles open opportunities by calculating the time elapsed until the current moment.")

# Check the format_time_in_stage function
print("\nReview of format_time_in_stage function:")
print("The function converts total hours into days, hours, and minutes.")
print("It handles potential `NaN` values by returning 'N/A'.")
print("The calculation `total_minutes = int(hours * 60)` converts hours to minutes.")
print("`days = total_minutes // (24 * 60)` correctly calculates the number of full days.")
print("`remaining_minutes_after_days = total_minutes % (24 * 60)` gets the minutes remaining after accounting for full days.")
print("`hours = remaining_minutes_after_days // 60` calculates the remaining hours.")
print("`minutes = remaining_minutes_after_days % 60` calculates the remaining minutes.")
print("The formatted string `f'{days} days, {hours} hours, {minutes} minutes'` provides a human-readable output.")
print("The formatting logic appears correct.")

# Check where Time_in_Stage_Formatted is used in the report section
report_section_start = app_code.find('elif page == "RelatÃ³rio de Oportunidade":')
report_section_end = app_code.find('except Exception as e:', report_section_start)

report_code = app_code[report_section_start:report_section_end]

print("\nReview of Time_in_Stage_Formatted Usage in Report:")
time_formatted_usage = "df_timeline[display_timeline_cols])"
if time_formatted_usage in report_code:
    print("The `Time_in_Stage_Formatted` column is included in the `display_timeline_cols` list and is used when displaying the `opportunity_timeline` DataFrame with `st.dataframe`.")
    print("This confirms that the formatted time is displayed in the individual opportunity reports.")
else:
    print("The `Time_in_Stage_Formatted` column is NOT explicitly used in the `display_timeline_cols` list in the report section code snippet found.")
    print("This indicates a potential issue in displaying the formatted time.")

"""**Reasoning**:
Update the `display_timeline_cols` list in the "RelatÃ³rio de Oportunidade" section of `app.py` to include the `Time_in_Stage_Formatted` column so the human-readable time duration is displayed in the individual opportunity report.


"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from google.colab import auth
# import gspread
# from google.auth import default
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as se
# import numpy as np
# import plotly.express as px
# import plotly.graph_objects as go
# import re
# from datetime import datetime
# import openai # Keep openai import as it's used later
# 
# st.set_page_config(layout="wide")
# 
# # Configure OpenAI API key and initialize client
# # Make OpenAI client initialization conditional
# # Try to get the key from environment variables first
# openai_api_key = os.getenv("OPENAI_API_KEY")
# if openai_api_key is None:
#     try:
#         # If not in environment, try to get from Streamlit secrets
#         # This might still raise an error if secrets.toml is missing/empty,
#         # but the outer try-except will catch it.
#         openai_api_key = st.secrets["OPENAI_API_KEY"]
#     except Exception:
#         openai_api_key = None # Set to None if not found in secrets either
# 
# if openai_api_key:
#     openai.api_key = openai_api_key
#     client = openai.OpenAI(api_key=openai.api_key)
#     st.sidebar.success("OpenAI client initialized.")
# else:
#     st.sidebar.warning("OpenAI API key not found. OpenAI features will be disabled.")
#     client = None # Set client to None if key is not available
# 
# 
# @st.cache_data
# def load_data():
#     try:
#         auth.authenticate_user()
#         creds, _ = default()
#         gc = gspread.authorize(creds)
#         B2sheet = gc.open('BI_B2')
#         page = B2sheet.sheet1
#         all_data = page.get_all_values()
#         df = pd.DataFrame(all_data[1:], columns=all_data[0])
# 
#         value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
#         for col in value_cols:
#             df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
#             df[col] = pd.to_numeric(df[col], errors='coerce')
# 
#         date_cols = ['Data de abertura', 'Data fechamento']
#         for col in date_cols:
#             df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')
# 
#         def extract_oc_identifier(title):
#             if isinstance(title, str):
#                 match = re.search(r'(OC\s*\d+)', title, re.IGNORECASE)
#                 if match:
#                     return match.group(1).replace(" ", "")
#                 else:
#                     match_cte = re.search(r'(CTE\s*\d+)', title, re.IGNORECASE)
#                     if match_cte:
#                          return match_cte.group(1).replace(" ", "")
#             return None
# 
#         df['OC_Identifier'] = df['TÃ­tulo'].apply(extract_oc_identifier)
# 
#         # Ensure date columns are not NaT before accessing dt properties
#         df['Mes de Abertura'] = df['Data de abertura'].dt.month.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) else 0
#         df['Ano de Abertura'] = df['Data de abertura'].dt.year.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) else 0
#         df['Mes de Fechamento'] = df['Data fechamento'].dt.month.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data fechamento']) else 0
#         df['Ano de Fechamento'] = df['Data fechamento'].dt.year.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data fechamento']) else 0
# 
# 
#         df['MonthYear_Abertura'] = df['Data de abertura'].dt.to_period('M') if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) else None
#         df['MonthYear_Fechamento'] = df['Data fechamento'].dt.to_period('M') if pd.api.types.is_datetime64_any_dtype(df['Data fechamento']) else None
# 
#         df['Hour_of_Day_Abertura'] = df['Data de abertura'].apply(lambda x: x.hour if pd.notna(x) else -1).astype(int)
# 
# 
#         df_timeline = df[['OC_Identifier', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()
#         df_timeline.dropna(subset=['OC_Identifier', 'Data de abertura'], inplace=True)
#         df_timeline = df_timeline.sort_values(by=['OC_Identifier', 'Data de abertura'])
# 
#         current_time = pd.to_datetime('now')
#         df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600
# 
#         df_timeline['Time_in_Stage'] = df_timeline.apply(
#             lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
#             axis=1
#         )
# 
#         def format_time_in_stage(hours):
#             if pd.isna(hours):
#                 return "N/A"
#             total_minutes = int(hours * 60)
#             days = total_minutes // (24 * 60)
#             remaining_minutes_after_days = total_minutes % (24 * 60)
#             hours = remaining_minutes_after_days // 60
#             minutes = remaining_minutes_after_days % 60
#             return f"{days} days, {hours} hours, {minutes} minutes"
# 
#         df_timeline['Time_in_Stage_Formatted'] = df_timeline['Time_in_Stage'].apply(format_time_in_stage)
# 
#         return df, df_timeline
# 
#     except Exception as e:
#         st.error(f"Erro ao carregar dados do Google Sheet: {e}")
#         return pd.DataFrame(), pd.DataFrame()
# 
# df, df_timeline = load_data()
# 
# st.sidebar.title("NavegaÃ§Ã£o")
# page = st.sidebar.radio("Ir para:", ["Painel Geral", "RelatÃ³rio de Oportunidade"])
# 
# if st.sidebar.button("Atualizar Dados"):
#     st.cache_data.clear()
#     st.rerun()
# 
# # Corrected the try-except block structure for the main part of the script
# try:
#     if page == "Painel Geral":
#         st.title("Painel de BI Operacional - Geral")
#         st.markdown("""
#         Este painel apresenta insights gerais sobre os dados operacionais,
#         permitindo acompanhar o desempenho e a distribuiÃ§Ã£o dos negÃ³cios.
#         """)
# 
#         if not df.empty:
#             st.sidebar.subheader("Filtros do Painel Geral")
#             # Ensure Data de abertura is datetime before accessing dt properties or min/max
#             if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) and not df['Data de abertura'].empty:
#                 min_date_abertura = df['Data de abertura'].min().date()
#                 max_date_abertura = df['Data de abertura'].max().date()
#                 start_date = st.sidebar.date_input("Data de Abertura (InÃ­cio)", min_value=min_date_abertura, max_value=max_date_abertura, value=min_date_abertura)
#                 end_date = st.sidebar.date_input("Data de Abertura (Fim)", min_value=min_date_abertura, max_value=max_date_abertura, value=max_date_abertura)
#                 start_datetime = pd.to_datetime(start_date)
#                 end_datetime = pd.to_datetime(end_date)
#                 filtered_df = df[(df['Data de abertura'] >= start_datetime) & (df['Data de abertura'] <= end_datetime)].copy()
#             else:
#                 filtered_df = df.copy() # No date filter if data is not available or not datetime
# 
#             selected_estados = st.sidebar.multiselect("Selecionar Estado:", filtered_df['Estado'].unique(), filtered_df['Estado'].unique())
#             filtered_df = filtered_df[filtered_df['Estado'].isin(selected_estados)]
#             selected_responsaveis_sidebar = st.sidebar.multiselect("Selecionar ResponsÃ¡vel (Filtro Inicial):", filtered_df['ResponsÃ¡vel'].unique(), filtered_df['ResponsÃ¡vel'].unique())
#             filtered_df = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_sidebar)]
#             selected_estagios = st.sidebar.multiselect("Selecionar EstÃ¡gio:", filtered_df['EstÃ¡gio'].unique(), filtered_df['EstÃ¡gio'].unique())
#             filtered_df = filtered_df[filtered_df['EstÃ¡gio'].isin(selected_estagios)]
# 
#             df_agg_responsavel_count = filtered_df.groupby('ResponsÃ¡vel')['OC_Identifier'].nunique().reset_index()
#             df_agg_responsavel_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#             df_agg_estado_mes_count = filtered_df.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#             df_agg_estado_mes_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#             df_agg_estado_mes_count['MonthYear_Abertura'] = df_agg_estado_mes_count['MonthYear_Abertura'].astype(str)
# 
#             ganha_df_filtered = filtered_df[filtered_df['Estado'] == 'Ganha'].copy()
# 
#             st.subheader("Resumo Geral")
#             col_kpi1, col_kpi2, col_kpi3 = st.columns(3)
#             total_opportunities = filtered_df['OC_Identifier'].nunique() if not filtered_df.empty else 0
#             total_won_value = ganha_df_filtered['Valor'].sum() if not ganha_df_filtered.empty else 0
#             win_rate = (len(ganha_df_filtered) / total_opportunities * 100) if total_opportunities > 0 else 0
# 
#             col_kpi1.metric("Total Oportunidades Ãšnicas", total_opportunities)
#             col_kpi2.metric("Valor Total Ganho", f"R$ {total_won_value:,.2f}")
#             col_kpi3.metric("Taxa de Sucesso", f"{win_rate:.2f}%")
# 
#             st.subheader("AnÃ¡lise de Oportunidades e Valor")
#             col1, col2 = st.columns(2)
# 
#             with col1:
#                 st.subheader("Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel")
#                 if not df_agg_responsavel_count.empty:
#                     fig1 = px.bar(df_agg_responsavel_count, x='ResponsÃ¡vel', y='Unique Opportunity Count',
#                                   title='Unique Opportunity Count per ResponsÃ¡vel')
#                     selected_points = st.plotly_chart(fig1, use_container_width=True, on_select="rerun")
#                     if selected_points and selected_points.selection and selected_points.selection.points:
#                          selected_responsaveis_chart = [point['x'] for point in selected_points.selection.points]
#                          filtered_df_chart_selection = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_chart)].copy()
#                     else:
#                          filtered_df_chart_selection = filtered_df.copy()
#                 else:
#                     st.info("Nenhum dado disponÃ­vel para 'Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel' com os filtros selecionados.")
#                     filtered_df_chart_selection = filtered_df.copy()
# 
#             with col2:
#                  df_agg_estado_mes_count_filtered = filtered_df_chart_selection.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#                  df_agg_estado_mes_count_filtered.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#                  df_agg_estado_mes_count_filtered['MonthYear_Abertura'] = df_agg_estado_mes_count_filtered['MonthYear_Abertura'].astype(str)
#                  st.subheader("Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura")
#                  if not df_agg_estado_mes_count_filtered.empty:
#                      fig2 = px.bar(df_agg_estado_mes_count_filtered, x='MonthYear_Abertura', y='Unique Opportunity Count', color='Estado',
#                                    title='Unique Opportunity Count per Estado and Month of Opening (Filtered)',
#                                    barmode='group')
#                      fig2.update_layout(xaxis_title="MÃªs/Ano de Abertura", yaxis_title="Quantidade de Oportunidades Ãšnicas")
#                      st.plotly_chart(fig2, use_container_width=True)
#                  else:
#                       st.info("Nenhum dado disponÃ­vel para 'Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura' com os filtros selecionados.")
# 
#             st.subheader("Heatmap: Oportunidades por Etapa e Hora de Abertura")
#             if not df_timeline.empty:
#                  # Ensure 'Data de abertura' is datetime before filtering
#                  if pd.api.types.is_datetime64_any_dtype(df_timeline['Data de abertura']):
#                     df_timeline_filtered_for_heatmap = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#                     if selected_points and selected_points.selection and selected_points.selection.points:
#                          selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                          df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#                     df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
#                     # Ensure 'Data de abertura' is datetime before extracting hour
#                     df_timeline_filtered_for_heatmap['Hour_of_Day_Abertura'] = df_timeline_filtered_for_heatmap['Data de abertura'].dt.hour if pd.api.types.is_datetime64_any_dtype(df_timeline_filtered_for_heatmap['Data de abertura']) else -1
# 
# 
#                     if not df_timeline_filtered_for_heatmap.empty:
#                          heatmap_data = df_timeline_filtered_for_heatmap.groupby(['EstÃ¡gio', 'Hour_of_Day_Abertura'])['OC_Identifier'].nunique().unstack(fill_value=0)
#                          if not heatmap_data.empty:
#                             fig_heatmap = go.Figure(data=go.Heatmap(
#                                    z=heatmap_data.values,
#                                    x=heatmap_data.columns,
#                                    y=heatmap_data.index,
#                                    colorscale='Viridis'))
#                             fig_heatmap.update_layout(title='Opportunity Count by Stage and Hour of Opening (Filtered)',
#                                                       xaxis_title='Hour of Day',
#                                                       yaxis_title='Stage')
#                             st.plotly_chart(fig_heatmap, use_container_width=True)
#                          else:
#                               st.info("Nenhum dado agregado disponÃ­vel para o Heatmap com os filtros selecionados.")
#                     else:
#                          st.info("Nenhum dado de timeline disponÃ­vel para o Heatmap com os filtros selecionados.")
#                  else:
#                       st.info("A coluna 'Data de abertura' na linha do tempo nÃ£o estÃ¡ no formato de data/hora correto para o Heatmap.")
# 
#             else:
#                  st.info("Dados de timeline nÃ£o disponÃ­veis para o Heatmap.")
# 
# 
#             st.subheader("AnÃ¡lise de EstÃ¡gios")
#             col5, col6 = st.columns(2)
# 
#             with col5:
#                 if not ganha_df_filtered.empty:
#                     st.subheader("DistribuiÃ§Ã£o do Valor para NegÃ³cios Ganhos (Filtered)")
#                     fig3, ax3 = plt.subplots(figsize=(10, 6))
#                     se.set_palette("viridis")
#                     ax3.hist(ganha_df_filtered['Valor'].dropna(), bins=50)
#                     ax3.set_title('Distribution of Valor for Won Deals (Filtered)')
#                     ax3.set_xlabel('Valor')
#                     ax3.set_ylabel('Frequency')
#                     plt.tight_layout()
#                     st.pyplot(fig3)
#                 else:
#                     st.info("Nenhum dado disponÃ­vel para visualizaÃ§Ãµes de NegÃ³cios Ganhos com os filtros selecionados.")
# 
#             with col6:
#                 if not filtered_df_chart_selection.empty:
#                     st.subheader("DistribuiÃ§Ã£o de Todos os EstÃ¡gios (Filtered)")
#                     fig4, ax4 = plt.subplots(figsize=(12, 6))
#                     se.set_palette("viridis")
#                     filtered_df_chart_selection['EstÃ¡gio'].value_counts().plot(kind='bar', ax=ax4)
#                     ax4.set_title('Distribution of All Stages (Filtered)')
#                     ax4.set_xlabel('EstÃ¡gio')
#                     ax4.set_ylabel('Count')
#                     plt.xticks(rotation=45, ha='right')
#                     plt.tight_layout()
#                     st.pyplot(fig4)
#                 else:
#                     st.info("Nenhum dado disponÃ­vel para 'DistribuiÃ§Ã£o de Todos os EstÃ¡gios' com os filtros selecionados.")
# 
#             st.subheader("AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio (Filtered)")
#             if not df_timeline.empty:
#                  # Ensure 'Data de abertura' is datetime before filtering
#                  if pd.api.types.is_datetime64_any_dtype(df_timeline['Data de abertura']):
#                     df_timeline_filtered = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#                     if selected_points and selected_points.selection and selected_points.selection.points:
#                          selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                          df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#                     df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
#                     if not df_timeline_filtered.empty:
#                         df_agg_time_per_stage_avg = df_timeline_filtered.groupby('EstÃ¡gio')['Time_in_Stage'].mean().reset_index()
#                         df_agg_time_per_stage_avg = df_agg_time_per_stage_avg.sort_values(by='Time_in_Stage', ascending=False)
# 
#                         def format_time_in_stage(hours):
#                             if pd.isna(hours):
#                                 return "N/A"
#                             total_minutes = int(hours * 60)
#                             days = total_minutes // (24 * 60)
#                             remaining_minutes_after_days = total_minutes % (24 * 60)
#                             hours = remaining_minutes_after_days // 60
#                             minutes = remaining_minutes_after_days % 60
#                             return f"{days} days, {hours} hours, {minutes} minutes"
# 
#                         df_agg_time_per_stage_avg['Average Time in Stage'] = df_agg_time_per_stage_avg['Time_in_Stage'].apply(format_time_in_stage)
# 
#                         st.write("Tempo MÃ©dio em Cada EstÃ¡gio:")
#                         st.dataframe(df_agg_time_per_stage_avg[['EstÃ¡gio', 'Average Time in Stage']])
# 
#                         st.subheader("Tempo MÃ©dio por EstÃ¡gio VisualizaÃ§Ã£o (Filtered)")
#                         fig5, ax5 = plt.subplots(figsize=(14, 7))
#                         se.set_palette("viridis")
#                         se.barplot(x='EstÃ¡gio', y='Time_in_Stage', data=df_agg_time_per_stage_avg, ax=ax5)
#                         ax5.set_title('Average Time per Stage (Filtered by Data de Abertura and Selection)')
#                         ax5.set_xlabel('EstÃ¡gio')
#                         ax5.set_ylabel('Average Time (hours)')
#                         plt.xticks(rotation=45, ha='right')
#                         plt.tight_layout()
#                         st.pyplot(fig5)
#                     else:
#                          st.info("Nenhum dado disponÃ­vel para AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio com os filtros selecionados.")
#                  else:
#                       st.info("A coluna 'Data de abertura' na linha do tempo nÃ£o estÃ¡ no formato de data/hora correto para a AnÃ¡lise de Tempo MÃ©dio.")
# 
#             else:
#                 st.info("Dados de timeline nÃ£o disponÃ­veis.")
# 
# 
#     elif page == "RelatÃ³rio de Oportunidade":
#         st.title("RelatÃ³rio de Oportunidade Individual")
#         st.markdown("""
#         Selecione um identificador de Oportunidade (OC + NÃºmero ou CTE + NÃºmero)
#         para visualizar sua linha do tempo e detalhes.
#         """)
# 
#         if not df.empty and not df_timeline.empty:
#             opportunity_identifiers = df['OC_Identifier'].dropna().unique()
#             selected_opportunity_identifier = st.selectbox("Selecionar Oportunidade (OC + NÃºmero ou CTE + NÃºmero):", opportunity_identifiers)
# 
#             st.subheader(f"Detalhes e Linha do Tempo para: {selected_opportunity_identifier}")
# 
#             opportunity_details_df = df[df['OC_Identifier'] == selected_opportunity_identifier]
# 
#             if not opportunity_details_df.empty:
#                 opportunity_details = opportunity_details_df.iloc[0]
# 
#                 col_info1, col_info2 = st.columns(2)
# 
#                 with col_info1:
#                     st.write("**ID:**", opportunity_details.get('ID', 'N/A'))
#                     st.write("**TÃ­tulo:**", opportunity_details.get('TÃ­tulo', 'N/A'))
#                     st.write("**ResponsÃ¡vel:**", opportunity_details.get('ResponsÃ¡vel', 'N/A'))
#                     st.write("**Estado:**", opportunity_details.get('Estado', 'N/A'))
#                     st.write("**EstÃ¡gio Atual:**", opportunity_details.get('EstÃ¡gio', 'N/A'))
# 
#                 with col_info2:
#                      st.write("**Valor:**", f"R$ {opportunity_details['Valor']:,.2f}" if pd.notna(opportunity_details['Valor']) else "N/A")
#                      st.write("**Origem:**", opportunity_details.get('Origem', 'N/A'))
#                      st.write("**Prob %:**", opportunity_details.get('Prob %', 'N/A'))
#                      st.write("**OC:**", opportunity_details.get('OC', 'N/A')) # Use .get for safer access
# 
# 
#                 st.subheader("Datas Principais")
#                 col_dates1, col_dates2 = st.columns(2)
#                 with col_dates1:
#                      st.write("**Data de Abertura:**", opportunity_details['Data de abertura'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data de abertura']) else "N/A")
#                 with col_dates2:
#                      st.write("**Data de Fechamento:**", opportunity_details['Data fechamento'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data fechamento']) else "N/A")
# 
#                 if pd.notna(opportunity_details['Data fechamento']):
#                     with st.expander("Detalhes de Fechamento"):
#                         st.write("**Valor Fechamento:**", f"R$ {opportunity_details['Valor fechamento']:,.2f}" if pd.notna(opportunity_details['Valor fechamento']) else "N/A")
#                         st.write("**Valor Rec. Fechamento:**", f"R$ {opportunity_details['Valor rec. fechamento']:,.2f}" if pd.notna(opportunity_details['Valor rec. fechamento']) else "N/A")
#                         st.write("**RazÃ£o de Fechamento:**", opportunity_details.get('RazÃ£o de fechamento', 'N/A')) # Use .get
#                         st.write("**ObservaÃ§Ã£o de Fechamento:**", opportunity_details.get('ObservaÃ§Ã£o de fechamento', 'N/A')) # Use .get
# 
# 
#                 opportunity_timeline = df_timeline[df_timeline['OC_Identifier'] == selected_opportunity_identifier].copy()
# 
#                 if not opportunity_timeline.empty:
#                     st.subheader("Linha do Tempo da Oportunidade")
#                     # Use Time_in_Stage_Formatted here
#                     display_timeline_cols = ['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']
#                     st.dataframe(opportunity_timeline[display_timeline_cols])
#                 else:
#                     st.info(f"Nenhum dado de linha do tempo encontrado para: {selected_opportunity_identifier}")
# 
#                 # --- AI Agent Interaction Section ---
#                 st.subheader("Assistente de IA para Oportunidade")
#                 # Removed the OpenAI client check here to always show the input area
#                 # Add a placeholder message if client is not initialized
#                 if st.session_state.get('ai_response') is None: # Check if session state has a response
#                    st.session_state['ai_response'] = "" # Initialize if not present
# 
#                 if client:
#                     user_query = st.text_area(f"FaÃ§a uma pergunta sobre a oportunidade {selected_opportunity_identifier}:", height=100, key='user_query')
#                     col_ai_button1, col_ai_button2 = st.columns(2)
# 
#                     with col_ai_button1:
#                         if st.button("Obter Resposta da IA", use_container_width=True):
#                             if user_query:
#                                 with st.spinner("Obtendo resposta da IA..."):
#                                     try:
#                                         # Construct the prompt
#                                         prompt = f"""
#                                         VocÃª Ã© um assistente de BI focado em analisar dados de oportunidades de negÃ³cios.
#                                         Sua tarefa Ã© responder a perguntas sobre uma oportunidade especÃ­fica com base nos dados fornecidos.
#                                         Seja conciso e Ãºtil, focando em insights de BI e na progressÃ£o da oportunidade.
#                                         **Use APENAS os dados fornecidos abaixo.**
#                                         Se a pergunta do usuÃ¡rio nÃ£o puder ser respondida com os dados disponÃ­veis, diga isso de forma educada.
# 
#                                         Dados da Oportunidade com identificador {selected_opportunity_identifier}:
# 
#                                         Detalhes Principais:
#                                         - ID: {opportunity_details.get('ID', 'N/A')}
#                                         - TÃ­tulo: {opportunity_details.get('TÃ­tulo', 'N/A')}
#                                         - ResponsÃ¡vel: {opportunity_details.get('ResponsÃ¡vel', 'N/A')}
#                                         - Estado: {opportunity_details.get('Estado', 'N/A')}
#                                         - EstÃ¡gio Atual: {opportunity_details.get('EstÃ¡gio', 'N/A')}
#                                         - Valor: R$ {opportunity_details.get('Valor', 'N/A')}
#                                         - Origem: {opportunity_details.get('Origem', 'N/A')}
#                                         - Prob %: {opportunity_details.get('Prob %', 'N/A')}
#                                         - OC: {opportunity_details.get('OC', 'N/A')}
#                                         - Data de Abertura: {opportunity_details.get('Data de abertura', 'N/A')}
#                                         - Data de Fechamento: {opportunity_details.get('Data fechamento', 'N/A')}
# 
#                                         Detalhes de Fechamento (se aplicÃ¡vel):
#                                         - Valor Fechamento: R$ {opportunity_details.get('Valor fechamento', 'N/A')}
#                                         - Valor Rec. Fechamento: R$ {opportunity_details.get('Valor rec. fechamento', 'N/A')}
#                                         - RazÃ£o de Fechamento: {opportunity_details.get('RazÃ£o de fechamento', 'N/A')}
#                                         - ObservaÃ§Ã£o de Fechamento: {opportunity_details.get('ObservaÃ§Ã£o de fechamento', 'N/A')}
# 
#                                         Linha do Tempo da Oportunidade (Progresso pelos EstÃ¡gios):
#                                         {opportunity_timeline[['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']].to_string(index=False)}
# 
#                                         Campos disponÃ­veis para anÃ¡lise nos Detalhes Principais: 'ID', 'TÃ­tulo', 'ResponsÃ¡vel', 'Estado', 'EstÃ¡gio', 'Valor', 'Origem', 'Prob %', 'OC', 'Data de Abertura', 'Data de Fechamento'.
#                                         Campos disponÃ­veis para anÃ¡lise nos Detalhes de Fechamento: 'Valor Fechamento', 'Valor Rec. Fechamento', 'RazÃ£o de Fechamento', 'ObservaÃ§Ã£o de Fechamento'.
#                                         Campos disponÃ­veis na Linha do Tempo: 'EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted'.
# 
#                                         Pergunta do UsuÃ¡rio: {user_query}
#                                         """
#                                         response = client.chat.completions.create(
#                                             model="gpt-4o-mini", # Or another suitable model
#                                             messages=[
#                                                 {"role": "system", "content": "VocÃª Ã© um assistente de BI Ãºtil. Responda em PortuguÃªs."},
#                                                 {"role": "user", "content": prompt}
#                                             ],
#                                             max_tokens=500
#                                         )
#                                         st.session_state['ai_response'] = response.choices[0].message.content # Store response in session state
#                                         st.rerun() # Rerun to display response immediately
#                                     except Exception as e:
#                                         st.error(f"Ocorreu um erro ao chamar a API da OpenAI: {e}")
#                                         st.session_state['ai_response'] = f"Erro: {e}" # Store error message
#                                         st.rerun()
#                             else:
#                                 st.warning("Por favor, insira uma pergunta.")
# 
#                     with col_ai_button2:
#                         if st.button("Gerar Resumo da Oportunidade", use_container_width=True):
#                              with st.spinner("Gerando resumo da oportunidade..."):
#                                 try:
#                                     prompt = f"""
#                                     VocÃª Ã© um assistente de BI focado em analisar dados de oportunidades de negÃ³cios.
#                                     Sua tarefa Ã© gerar um resumo conciso e Ãºtil da oportunidade com o identificador {selected_opportunity_identifier}
#                                     com base nos dados fornecidos abaixo.
#                                     **Use APENAS os dados fornecidos.**
#                                     Inclua detalhes importantes como o estado atual, valor, responsÃ¡vel e uma breve anÃ¡lise da linha do tempo, se aplicÃ¡vel.
# 
#                                     Dados da Oportunidade com identificador {selected_opportunity_identifier}:
# 
#                                     Detalhes Principais:
#                                     - ID: {opportunity_details.get('ID', 'N/A')}
#                                     - TÃ­tulo: {opportunity_details.get('TÃ­tulo', 'N/A')}
#                                     - ResponsÃ¡vel: {opportunity_details.get('ResponsÃ¡vel', 'N/A')}
#                                     - Estado: {opportunity_details.get('Estado', 'N/A')}
#                                     - EstÃ¡gio Atual: {opportunity_details.get('EstÃ¡gio', 'N/A')}
#                                     - Valor: R$ {opportunity_details.get('Valor', 'N/A')}
#                                     - Origem: {opportunity_details.get('Origem', 'N/A')}
#                                     - Prob %: {opportunity_details.get('Prob %', 'N/A')}
#                                     - OC: {opportunity_details.get('OC', 'N/A')}
#                                     - Data de Abertura: {opportunity_details.get('Data de abertura', 'N/A')}
#                                     - Data de Fechamento: {opportunity_details.get('Data fechamento', 'N/A')}
# 
#                                     Detalhes de Fechamento (se aplicÃ¡vel):
#                                     - Valor Fechamento: R$ {opportunity_details.get('Valor fechamento', 'N/A')}
#                                     - Valor Rec. Fechamento: R$ {opportunity_details.get('Valor rec. fechamento', 'N/A')}
#                                     - RazÃ£o de Fechamento: {opportunity_details.get('RazÃ£o de fechamento', 'N/A')}
#                                     - ObservaÃ§Ã£o de Fechamento: {opportunity_details.get('ObservaÃ§Ã£o de fechamento', 'N/A')}
# 
#                                     Linha do Tempo da Oportunidade (Progresso pelos EstÃ¡gios):
#                                     {opportunity_timeline[['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']].to_string(index=False)}
# 
#                                     Gere o resumo agora.
#                                     """
#                                     response = client.chat.completions.create(
#                                         model="gpt-4o-mini",
#                                         messages=[
#                                             {"role": "system", "content": "VocÃª Ã© um assistente de BI Ãºtil. Responda em PortuguÃªs."},
#                                             {"role": "user", "content": prompt}
#                                         ],
#                                         max_tokens=500
#                                     )
#                                     st.session_state['ai_response'] = response.choices[0].message.content # Store response
#                                     st.rerun() # Rerun to display
#                                 except Exception as e:
#                                     st.error(f"Ocorreu um erro ao chamar a API da OpenAI para gerar o resumo: {e}")
#                                     st.session_state['ai_response'] = f"Erro: {e}" # Store error message
#                                     st.rerun()
#             else:
#                 st.warning("O cliente OpenAI nÃ£o estÃ¡ inicializado. A funcionalidade do assistente de IA estÃ¡ desativada.")
#                 st.write(st.session_state.get('ai_response', '')) # Display any previous error/message
# 
#             # Display AI response
#             if st.session_state.get('ai_response'): # Check if there's a response to display
#                  st.markdown("---") # Add a separator
#                  st.markdown("**Resposta/Resumo do Assistente de IA:**")
#                  st.markdown(st.session_state['ai_response'])
# 
# 
#         else:
#              st.info(f"Nenhum detalhe encontrado para: {selected_opportunity_identifier} no DataFrame principal.")
# 
#     elif df.empty:
#          st.warning("NÃ£o foi possÃ­vel carregar os dados. Verifique a conexÃ£o com o Google Sheet.")
#     else:
#          st.info("Dados de linha do tempo nÃ£o disponÃ­veis.")
# 
# except Exception as e:
#     st.error(f"Erro ao carregar dados ou processar: {e}")

"""## Enhance error handling

### Subtask:
Add more specific error handling within the "RelatÃ³rio de Oportunidade" section to provide more informative messages to the user if data is missing or there are issues with filtering or calculations.

**Reasoning**:
Examine the existing error handling in the "RelatÃ³rio de Oportunidade" section and add more specific try-except blocks around potential failure points to provide more informative messages to the user.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from google.colab import auth
# import gspread
# from google.auth import default
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as se
# import numpy as np
# import plotly.express as px
# import plotly.graph_objects as go
# import re
# from datetime import datetime
# import openai
# import os # Import os for environment variables
# 
# 
# st.set_page_config(layout="wide") # Set wide layout for better use of space
# 
# # Configure OpenAI API key and initialize client
# # Make OpenAI client initialization conditional
# openai_api_key = os.getenv("OPENAI_API_KEY") # Try to get the key from environment variables first
# if openai_api_key is None:
#     try:
#         # If not in environment, try to get from Streamlit secrets
#         # This might still raise an error if secrets.toml is missing/empty,
#         # but the outer try-except will catch it.
#         openai_api_key = st.secrets["OPENAI_API_KEY"]
#     except Exception:
#         openai_api_key = None # Set to None if not found in secrets either
# 
# if openai_api_key:
#     openai.api_key = openai_api_key
#     client = openai.OpenAI(api_key=openai.api_key)
#     st.sidebar.success("OpenAI client initialized.")
# else:
#     st.sidebar.warning("OpenAI API key not found. OpenAI features will be disabled.")
#     client = None # Set client to None if key is not available
# 
# 
# @st.cache_data # Cache the data loading for performance
# def load_data():
#     try:
#         # Authenticate to Google Cloud
#         auth.authenticate_user()
#         creds, _ = default()
#         gc = gspread.authorize(creds)
#         B2sheet = gc.open('BI_B2')
#         page = B2sheet.sheet1
#         all_data = page.get_all_values()
#         df = pd.DataFrame(all_data[1:], columns=all_data[0])
# 
#         # Limpeza e prÃ©-processing
#         value_cols = ['Valor', 'Valor Rec.', 'Valor fechamento', 'Valor rec. fechamento']
#         for col in value_cols:
#             df[col] = df[col].astype(str).str.replace('R$', '', regex=False).str.replace(',', '', regex=False).str.strip()
#             df[col] = pd.to_numeric(df[col], errors='coerce')
# 
#         date_cols = ['Data de abertura', 'Data fechamento']
#         for col in date_cols:
#             df[col] = pd.to_datetime(df[col], errors='coerce', format='%d/%m/%Y %H:%M:%S')
# 
#         # Extract unique opportunity identifier (OC + number) from 'TÃ­tulo'
#         def extract_oc_identifier(title):
#             if isinstance(title, str):
#                 match = re.search(r'(OC\s*\d+)', title, re.IGNORECASE)
#                 if match:
#                     return match.group(1).replace(" ", "") # Remove space in "OC "
#                 else:
#                     # Also check for "CTE" as it appears in titles and might be identifiers
#                     match_cte = re.search(r'(CTE\s*\d+)', title, re.IGNORECASE)
#                     if match_cte:
#                          return match_cte.group(1).replace(" ", "")
#             return None # Return None if no pattern is found
# 
#         df['OC_Identifier'] = df['TÃ­tulo'].apply(extract_oc_identifier)
# 
# 
#         # Transformation and Feature Engineering for BI
#         # Ensure date columns are not NaT before accessing dt properties
#         df['Mes de Abertura'] = df['Data de abertura'].dt.month.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) else 0
#         df['Ano de Abertura'] = df['Data de abertura'].dt.year.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) else 0
#         df['Mes de Fechamento'] = df['Data fechamento'].dt.month.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data fechamento']) else 0
#         df['Ano de Fechamento'] = df['Data fechamento'].dt.year.fillna(0).astype(int) if pd.api.types.is_datetime64_any_dtype(df['Data fechamento']) else 0
# 
#         # Create a 'MonthYear' column for time series analysis
#         df['MonthYear_Abertura'] = df['Data de abertura'].dt.to_period('M') if pd.api.types.is_datetime64_any_dtype(df['Data de abertura']) else None
#         df['MonthYear_Fechamento'] = df['Data fechamento'].dt.to_period('M') if pd.api.types.is_datetime64_any_dtype(df['Data fechamento']) else None
# 
#         # Extract Hour of Day from 'Data de abertura'
#         # Ensure 'Data de abertura' is not NaT before extracting hour
#         # Only extract hour if Data de abertura is a valid datetime
#         df['Hour_of_Day_Abertura'] = df['Data de abertura'].apply(lambda x: x.hour if pd.notna(x) else -1).astype(int)
# 
# 
#         # Calculate Time in Stage for timeline analysis using OC_Identifier
#         df_timeline = df[['OC_Identifier', 'EstÃ¡gio', 'Data de abertura', 'Data fechamento']].copy()
#         # Drop rows where OC_Identifier could not be extracted or Data de abertura is missing
#         df_timeline.dropna(subset=['OC_Identifier', 'Data de abertura'], inplace=True)
#         df_timeline = df_timeline.sort_values(by=['OC_Identifier', 'Data de abertura'])
# 
#         current_time = pd.to_datetime('now') # Timezone-naive for consistency
#         df_timeline['Time_in_Stage'] = (df_timeline['Data fechamento'] - df_timeline['Data de abertura']).dt.total_seconds() / 3600 # Time in hours
# 
#         df_timeline['Time_in_Stage'] = df_timeline.apply(
#             lambda row: (current_time - row['Data de abertura']).total_seconds() / 3600 if pd.isna(row['Data fechamento']) else row['Time_in_Stage'],
#             axis=1
#         )
# 
#         # Format 'Time_in_Stage' for display to include minutes
#         def format_time_in_stage(hours):
#             if pd.isna(hours):
#                 return "N/A"
#             total_minutes = int(hours * 60)
#             days = total_minutes // (24 * 60)
#             remaining_minutes_after_days = total_minutes % (24 * 60)
#             hours = remaining_minutes_after_days // 60
#             minutes = remaining_minutes_after_days % 60
#             return f"{days} days, {hours} hours, {minutes} minutes"
# 
#         df_timeline['Time_in_Stage_Formatted'] = df_timeline['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#         return df, df_timeline
# 
#     except Exception as e:
#         st.error(f"Erro ao carregar dados do Google Sheet: {e}")
#         return pd.DataFrame(), pd.DataFrame() # Return empty dataframes on error
# 
# 
# # Load data
# df, df_timeline = load_data()
# 
# # --- Multi-page Navigation ---
# st.sidebar.title("NavegaÃ§Ã£o")
# page = st.sidebar.radio("Ir para:", ["Painel Geral", "RelatÃ³rio de Oportunidade"])
# 
# # --- Refresh Button ---
# if st.sidebar.button("Atualizar Dados"):
#     st.cache_data.clear() # Clear the cache for load_data
#     st.rerun() # Rerun the app to load fresh data
# 
# 
# # Main app logic within a try-except block for general errors
# try:
#     if page == "Painel Geral":
#         # --- Painel Geral ---
#         st.title("Painel de BI Operacional - Geral")
# 
#         st.markdown("""
#         Este painel apresenta insights gerais sobre os dados operacionais,
#         permitindo acompanhar o desempenho e a distribuiÃ§Ã£o dos negÃ³cios.
#         """)
# 
#         if not df.empty:
#             # Add filters to the sidebar
#             st.sidebar.subheader("Filtros do Painel Geral")
# 
#             # Date range filter for 'Data de abertura'
#             if not df['Data de abertura'].empty:
#                 min_date_abertura = df['Data de abertura'].min().date()
#                 max_date_abertura = df['Data de abertura'].max().date()
# 
#                 start_date = st.sidebar.date_input("Data de Abertura (InÃ­cio)", min_value=min_date_abertura, max_value=max_date_abertura, value=min_date_abertura)
#                 end_date = st.sidebar.date_input("Data de Abertura (Fim)", min_value=min_date_abertura, max_value=max_date_abertura, value=max_date_abertura)
# 
#                 # Convert selected dates to datetime objects for filtering
#                 start_datetime = pd.to_datetime(start_date)
#                 end_datetime = pd.to_datetime(end_date)
# 
#                 # Apply date filter
#                 filtered_df = df[(df['Data de abertura'] >= start_datetime) & (df['Data de abertura'] <= end_datetime)].copy()
#             else:
#                 filtered_df = df.copy() # If date column is empty, use the original df
# 
# 
#             # Filter by Estado
#             selected_estados = st.sidebar.multiselect("Selecionar Estado:", filtered_df['Estado'].unique(), filtered_df['Estado'].unique())
#             filtered_df = filtered_df[filtered_df['Estado'].isin(selected_estados)]
# 
#             # Filter by ResponsÃ¡vel (Initial filter)
#             selected_responsaveis_sidebar = st.sidebar.multiselect("Selecionar ResponsÃ¡vel (Filtro Inicial):", filtered_df['ResponsÃ¡vel'].unique(), filtered_df['ResponsÃ¡vel'].unique())
#             filtered_df = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_sidebar)]
# 
#             # Filter by EstÃ¡gio
#             selected_estagios = st.sidebar.multiselect("Selecionar EstÃ¡gio:", filtered_df['EstÃ¡gio'].unique(), filtered_df['EstÃ¡gio'].unique())
#             filtered_df = filtered_df[filtered_df['EstÃ¡gio'].isin(selected_estagios)]
# 
#             # Add filter by OC_Identifier for the general dashboard
#             opportunity_identifiers = filtered_df['OC_Identifier'].dropna().unique()
#             selected_opportunity_identifier_general = st.sidebar.selectbox(
#                 "Filtrar por Oportunidade (OC + NÃºmero ou CTE + NÃºmero):",
#                 ['Todos'] + list(opportunity_identifiers)
#             )
# 
#             if selected_opportunity_identifier_general != 'Todos':
#                 filtered_df = filtered_df[filtered_df['OC_Identifier'] == selected_opportunity_identifier_general].copy()
# 
# 
#             # AgregaÃ§Ãµes para visualizaÃ§Ãµes (using filtered_df)
#             # Aggregate to count unique OC_Identifier per ResponsÃ¡vel
#             df_agg_responsavel_count = filtered_df.groupby('ResponsÃ¡vel')['OC_Identifier'].nunique().reset_index()
#             df_agg_responsavel_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
# 
# 
#             # Aggregate to count unique OC_Identifier per Estado and MonthYear_Abertura
#             df_agg_estado_mes_count = filtered_df.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#             df_agg_estado_mes_count.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#             df_agg_estado_mes_count['MonthYear_Abertura'] = df_agg_estado_mes_count['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
# 
#             # Filter for 'Ganha' deals for specific visualizations
#             ganha_df_filtered = filtered_df[filtered_df['Estado'] == 'Ganha'].copy()
# 
#             # --- Display Key Metrics (KPIs) ---
#             st.subheader("Resumo Geral")
#             col_kpi1, col_kpi2, col_kpi3 = st.columns(3)
# 
#             total_opportunities = filtered_df['OC_Identifier'].nunique() if not filtered_df.empty else 0
#             total_won_value = ganha_df_filtered['Valor'].sum() if not ganha_df_filtered.empty else 0
#             win_rate = (len(ganha_df_filtered) / total_opportunities * 100) if total_opportunities > 0 else 0
# 
# 
#             col_kpi1.metric("Total Oportunidades Ãšnicas", total_opportunities)
#             col_kpi2.metric("Valor Total Ganho", f"R$ {total_won_value:,.2f}")
#             col_kpi3.metric("Taxa de Sucesso", f"{win_rate:.2f}%")
# 
# 
#             # --- Visualizations ---
#             st.subheader("AnÃ¡lise de Oportunidades e Valor")
# 
#             # Row 1: Value/Count by Responsible and Deals by State/Month
#             col1, col2 = st.columns(2)
# 
#             with col1:
#                 st.subheader("Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel")
#                 if not df_agg_responsavel_count.empty:
#                     # Using Plotly Express for interactivity
#                     fig1 = px.bar(df_agg_responsavel_count, x='ResponsÃ¡vel', y='Unique Opportunity Count',
#                                   title='Unique Opportunity Count per ResponsÃ¡vel')
# 
#                     # Capture selection
#                     selected_points = st.plotly_chart(fig1, use_container_width=True, on_select="rerun")
# 
#                     # Filter based on selection (using the original filtered_df and the selection from the chart)
#                     if selected_points and selected_points.selection and selected_points.selection.points:
#                          selected_responsaveis_chart = [point['x'] for point in selected_points.selection.points]
#                          filtered_df_chart_selection = filtered_df[filtered_df['ResponsÃ¡vel'].isin(selected_responsaveis_chart)].copy()
#                     else:
#                          filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart selection
# 
#                 else:
#                     st.info("Nenhum dado disponÃ­vel para 'Quantidade de Oportunidades Ãšnicas por ResponsÃ¡vel' com os filtros selecionados.")
#                     filtered_df_chart_selection = filtered_df.copy() # Use sidebar filtered data if no chart data
# 
# 
#             with col2:
#                  # Recalculate aggregated data based on chart selection
#                  df_agg_estado_mes_count_filtered = filtered_df_chart_selection.groupby(['Estado', 'MonthYear_Abertura'])['OC_Identifier'].nunique().reset_index()
#                  df_agg_estado_mes_count_filtered.rename(columns={'OC_Identifier': 'Unique Opportunity Count'}, inplace=True)
#                  df_agg_estado_mes_count_filtered['MonthYear_Abertura'] = df_agg_estado_mes_count_filtered['MonthYear_Abertura'].astype(str) # Convert to string for plotting
# 
#                  st.subheader("Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura")
#                  if not df_agg_estado_mes_count_filtered.empty:
#                      # Using Plotly Express for the improved chart
#                      fig2 = px.bar(df_agg_estado_mes_count_filtered, x='MonthYear_Abertura', y='Unique Opportunity Count', color='Estado',
#                                    title='Unique Opportunity Count per Estado and Month of Opening (Filtered)',
#                                    barmode='group') # Use 'group' or 'stack'
# 
#                      fig2.update_layout(xaxis_title="MÃªs/Ano de Abertura", yaxis_title="Quantidade de Oportunidades Ãšnicas")
#                      st.plotly_chart(fig2, use_container_width=True)
# 
#                  else:
#                       st.info("Nenhum dado disponÃ­vel para 'Quantidade de NegÃ³cios Ãšnicos por Estado e MÃªs de Abertura' com os filtros selecionados.")
# 
# 
#         # Row 2: Heatmap of Opportunities by Stage and Hour
#         st.subheader("Heatmap: Oportunidades por Etapa e Hora de Abertura")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered_for_heatmap = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
#             # Filter heatmap data by selected OC_Identifier for the general dashboard
#             if selected_opportunity_identifier_general != 'Todos':
#                  df_timeline_filtered_for_heatmap = df_timeline_filtered_for_heatmap[df_timeline_filtered_for_heatmap['OC_Identifier'] == selected_opportunity_identifier_general].copy()
# 
# 
#             # Aggregate data for the heatmap (count unique OC_Identifier per EstÃ¡gio and Hour_of_Day_Abertura)
#             # Ensure 'Hour_of_Day_Abertura' is extracted for the filtered timeline data
#             df_timeline_filtered_for_heatmap['Hour_of_Day_Abertura'] = df_timeline_filtered_for_heatmap['Data de abertura'].dt.hour
# 
#             if not df_timeline_filtered_for_heatmap.empty:
#                  heatmap_data = df_timeline_filtered_for_heatmap.groupby(['EstÃ¡gio', 'Hour_of_Day_Abertura'])['OC_Identifier'].nunique().unstack(fill_value=0)
# 
#                  if not heatmap_data.empty:
#                     # Use go.Heatmap to create the heatmap
#                     fig_heatmap = go.Figure(data=go.Heatmap(
#                            z=heatmap_data.values,
#                            x=heatmap_data.columns,
#                            y=heatmap_data.index,
#                            colorscale='Viridis')) # Use a color scale
# 
#                     fig_heatmap.update_layout(title='Opportunity Count by Stage and Hour of Opening (Filtered)',
#                                               xaxis_title='Hour of Day',
#                                               yaxis_title='Stage')
# 
#                     st.plotly_chart(fig_heatmap, use_container_width=True)
#                  else:
#                       st.info("Nenhum dado agregado disponÃ­vel para o Heatmap com os filtros selecionados.")
#             else:
#                  st.info("Nenhum dado de timeline disponÃ­vel para o Heatmap com os filtros selecionados.")
#         else:
#              st.info("Dados de timeline nÃ£o disponÃ­veis para o Heatmap.")
# 
# 
#         # Row 3: Distribution of EstÃ¡gios
#         st.subheader("AnÃ¡lise de EstÃ¡gios")
#         col5, col6 = st.columns(2)
# 
#         with col5:
#              # New Visualizations for 'Ganha' deals
#             if not ganha_df_filtered.empty:
#                 st.subheader("DistribuiÃ§Ã£o do Valor para NegÃ³cios Ganhos (Filtered)")
#                 fig3, ax3 = plt.subplots(figsize=(10, 6))
#                 se.set_palette("viridis") # Set a color palette
#                 ax3.hist(ganha_df_filtered['Valor'].dropna(), bins=50)
#                 ax3.set_title('Distribution of Valor for Won Deals (Filtered)')
#                 ax3.set_xlabel('Valor')
#                 ax3.set_ylabel('Frequency')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig3)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para visualizaÃ§Ãµes de NegÃ³cios Ganhos com os filtros selecionados.")
# 
#         with col6:
#             if not filtered_df_chart_selection.empty:
#                 # Distribution of all stages (not just 'Ganha')
#                 st.subheader("DistribuiÃ§Ã£o de Todos os EstÃ¡gios (Filtered)")
#                 fig4, ax4 = plt.subplots(figsize=(12, 6))
#                 se.set_palette("viridis") # Set a color palette
#                 filtered_df_chart_selection['EstÃ¡gio'].value_counts().plot(kind='bar', ax=ax4)
#                 ax4.set_title('Distribution of All Stages (Filtered)')
#                 ax4.set_xlabel('EstÃ¡gio')
#                 ax4.set_ylabel('Count')
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig4)
#             else:
#                 st.info("Nenhum dado disponÃ­vel para 'DistribuiÃ§Ã£o de Todos os EstÃ¡gios' com os filtros selecionados.")
# 
# 
#         # Row 4: Timeline Analysis Metrics
#         st.subheader("AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio (Filtered)")
#         if not df_timeline.empty:
#             # Filter timeline data based on the date range selected for the main dashboard
#             df_timeline_filtered = df_timeline[(df_timeline['Data de abertura'] >= start_datetime) & (df_timeline['Data de abertura'] <= end_datetime)].copy()
# 
#             # Further filter timeline data based on chart selection and Estado/EstÃ¡gio filters
#             if selected_points and selected_points.selection and selected_points.selection.points:
#                  # Filter timeline data based on selected OC_Identifiers from the chart selection
#                  selected_oc_identifiers_chart = filtered_df_chart_selection['OC_Identifier'].unique()
#                  df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['OC_Identifier'].isin(selected_oc_identifiers_chart)].copy()
#             df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['EstÃ¡gio'].isin(selected_estagios)].copy()
# 
#             # Filter timeline data by selected OC_Identifier for the general dashboard
#             if selected_opportunity_identifier_general != 'Todos':
#                  df_timeline_filtered = df_timeline_filtered[df_timeline_filtered['OC_Identifier'] == selected_opportunity_identifier_general].copy()
# 
# 
#             if not df_timeline_filtered.empty:
#                 df_agg_time_per_stage_avg = df_timeline_filtered.groupby('EstÃ¡gio')['Time_in_Stage'].mean().reset_index()
#                 df_agg_time_per_stage_avg = df_agg_time_per_stage_avg.sort_values(by='Time_in_Stage', ascending=False)
# 
#                 # Format 'Time_in_Stage' for display to include minutes
#                 def format_time_in_stage(hours):
#                     if pd.isna(hours):
#                         return "N/A"
#                     total_minutes = int(hours * 60)
#                     days = total_minutes // (24 * 60)
#                     remaining_minutes_after_days = total_minutes % (24 * 60)
#                     hours = remaining_minutes_after_days // 60
#                     minutes = remaining_minutes_after_days % 60
#                     return f"{days} days, {hours} hours, {minutes} minutes"
# 
#                 df_agg_time_per_stage_avg['Average Time in Stage'] = df_agg_time_per_stage_avg['Time_in_Stage'].apply(format_time_in_stage)
# 
# 
#                 st.write("Tempo MÃ©dio em Cada EstÃ¡gio:")
#                 st.dataframe(df_agg_time_per_stage_avg[['EstÃ¡gio', 'Average Time in Stage']])
# 
#                 # Visualization of average time per stage (using the numerical value)
#                 st.subheader("Tempo MÃ©dio por EstÃ¡gio VisualizaÃ§Ã£o (Filtered)")
#                 fig5, ax5 = plt.subplots(figsize=(14, 7))
#                 se.set_palette("viridis") # Set a color palette
#                 se.barplot(x='EstÃ¡gio', y='Time_in_Stage', data=df_agg_time_per_stage_avg, ax=ax5)
#                 ax5.set_title('Average Time per Stage (Filtered by Data de Abertura and Selection)')
#                 ax5.set_xlabel('EstÃ¡gio')
#                 ax5.set_ylabel('Average Time (hours)') # Keep y-axis as hours for the plot to work directly with numerical data
#                 plt.xticks(rotation=45, ha='right')
#                 plt.tight_layout() # Ensure tight layout
#                 st.pyplot(fig5)
#             else:
#                  st.info("Nenhum dado disponÃ­vel para AnÃ¡lise de Tempo MÃ©dio por EstÃ¡gio com os filtros selecionados.")
#         else:
#             st.info("Dados de timeline nÃ£o disponÃ­veis.")
# 
# 
#     elif page == "RelatÃ³rio de Oportunidade":
#         # --- RelatÃ³rio de Oportunidade Individual ---
#         st.title("RelatÃ³rio de Oportunidade Individual")
# 
#         st.markdown("""
#         Selecione um identificador de Oportunidade (OC + NÃºmero ou CTE + NÃºmero)
#         para visualizar sua linha do tempo e detalhes.
#         """)
# 
#         # Error handling for empty df or df_timeline
#         if df.empty or df_timeline.empty:
#              st.warning("Dados de oportunidade ou linha do tempo nÃ£o disponÃ­veis. Por favor, verifique a conexÃ£o com o Google Sheet.")
#         else:
#             try:
#                 # Use OC_Identifier for selection
#                 opportunity_identifiers = df['OC_Identifier'].dropna().unique()
# 
#                 if len(opportunity_identifiers) == 0:
#                      st.info("Nenhum identificador de oportunidade Ãºnico encontrado nos dados.")
#                 else:
#                     selected_opportunity_identifier = st.selectbox("Selecionar Oportunidade (OC + NÃºmero ou CTE + NÃºmero):", opportunity_identifiers)
# 
#                     st.subheader(f"Detalhes e Linha do Tempo para: {selected_opportunity_identifier}")
# 
#                     try:
#                         # Filter main df for the selected opportunity identifier
#                         opportunity_details_df = df[df['OC_Identifier'] == selected_opportunity_identifier]
# 
#                         if opportunity_details_df.empty:
#                             st.warning(f"Nenhum detalhe encontrado para: {selected_opportunity_identifier} no DataFrame principal.")
#                         else:
#                             opportunity_details = opportunity_details_df.iloc[0]
# 
#                             # Use columns for a structured layout
#                             col_info1, col_info2 = st.columns(2)
# 
#                             with col_info1:
#                                 st.write("**ID:**", opportunity_details.get('ID', 'N/A'))
#                                 st.write("**TÃ­tulo:**", opportunity_details.get('TÃ­tulo', 'N/A'))
#                                 st.write("**ResponsÃ¡vel:**", opportunity_details.get('ResponsÃ¡vel', 'N/A'))
#                                 st.write("**Estado:**", opportunity_details.get('Estado', 'N/A'))
#                                 st.write("**EstÃ¡gio Atual:**", opportunity_details.get('EstÃ¡gio', 'N/A')) # Display current stage
# 
#                             with col_info2:
#                                  # Add error handling for potential non-numeric 'Valor' before formatting
#                                  valor_display = "N/A"
#                                  if pd.notna(opportunity_details.get('Valor')) and pd.api.types.is_numeric_dtype(opportunity_details.get('Valor')):
#                                      valor_display = f"R$ {opportunity_details['Valor']:,.2f}"
#                                  st.write("**Valor:**", valor_display)
# 
#                                  st.write("**Origem:**", opportunity_details.get('Origem', 'N/A'))
#                                  st.write("**Prob %:**", opportunity_details.get('Prob %', 'N/A'))
#                                  st.write("**OC:**", opportunity_details.get('OC', 'N/A')) # Use .get for safer access
# 
# 
#                             st.subheader("Datas Principais")
#                             col_dates1, col_dates2 = st.columns(2)
#                             with col_dates1:
#                                  st.write("**Data de Abertura:**", opportunity_details['Data de abertura'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data de abertura']) else "N/A")
#                             with col_dates2:
#                                  st.write("**Data de Fechamento:**", opportunity_details['Data fechamento'].strftime('%d/%m/%Y %H:%M:%S') if pd.notna(opportunity_details['Data fechamento']) else "N/A")
# 
#                             # Use an expander for Closing Details (if available)
#                             if pd.notna(opportunity_details.get('Data fechamento')):
#                                 with st.expander("Detalhes de Fechamento"):
#                                     # Add error handling for potential non-numeric closing values
#                                     valor_fechamento_display = "N/A"
#                                     if pd.notna(opportunity_details.get('Valor fechamento')) and pd.api.types.is_numeric_dtype(opportunity_details.get('Valor fechamento')):
#                                          valor_fechamento_display = f"R$ {opportunity_details['Valor fechamento']:,.2f}"
#                                     st.write("**Valor Fechamento:**", valor_fechamento_display)
# 
#                                     valor_rec_fechamento_display = "N/A"
#                                     if pd.notna(opportunity_details.get('Valor rec. fechamento')) and pd.api.types.is_numeric_dtype(opportunity_details.get('Valor rec. fechamento')):
#                                          valor_rec_fechamento_display = f"R$ {opportunity_details['Valor rec. fechamento']:,.2f}"
#                                     st.write("**Valor Rec. Fechamento:**", valor_rec_fechamento_display)
# 
#                                     st.write("**RazÃ£o de Fechamento:**", opportunity_details.get('RazÃ£o de fechamento', 'N/A')) # Use .get
#                                     st.write("**ObservaÃ§Ã£o de Fechamento:**", opportunity_details.get('ObservaÃ§Ã£o de fechamento', 'N/A')) # Use .get
# 
# 
#                             try:
#                                 # Filter timeline for the selected opportunity identifier
#                                 opportunity_timeline = df_timeline[df_timeline['OC_Identifier'] == selected_opportunity_identifier].copy()
# 
#                                 if not opportunity_timeline.empty:
#                                     st.subheader("Linha do Tempo da Oportunidade")
# 
#                                     # Display timeline using a more visually appealing table format
#                                     display_timeline_cols = ['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']
#                                     st.dataframe(opportunity_timeline[display_timeline_cols])
#                                 else:
#                                     st.info(f"Nenhum dado de linha do tempo encontrado para: {selected_opportunity_identifier}")
# 
#                             except Exception as e:
#                                 st.error(f"Erro ao processar ou exibir dados de linha do tempo para {selected_opportunity_identifier}: {e}")
# 
# 
#                             # --- AI Agent Interaction Section ---
#                             st.subheader("Assistente de IA para Oportunidade")
#                             # Removed the OpenAI client check here to always show the input area
#                             # Add a placeholder message if client is not initialized
#                             if st.session_state.get('ai_response') is None: # Check if session state has a response
#                                st.session_state['ai_response'] = "" # Initialize if not present
# 
#                             if client:
#                                 user_query = st.text_area(f"FaÃ§a uma pergunta sobre a oportunidade {selected_opportunity_identifier}:", height=100, key='user_query')
#                                 col_ai_button1, col_ai_button2 = st.columns(2)
# 
#                                 with col_ai_button1:
#                                     if st.button("Obter Resposta da IA", use_container_width=True):
#                                         if user_query:
#                                             with st.spinner("Obtendo resposta da IA..."):
#                                                 try:
#                                                     # Construct the prompt
#                                                     prompt = f"""
#                                                     VocÃª Ã© um assistente de BI focado em analisar dados de oportunidades de negÃ³cios.
#                                                     Sua tarefa Ã© responder a perguntas sobre uma oportunidade especÃ­fica com base nos dados fornecidos.
#                                                     Seja conciso e Ãºtil, focando em insights de BI e na progressÃ£o da oportunidade.
#                                                     **Use APENAS os dados fornecidos abaixo.**
#                                                     Se a pergunta do usuÃ¡rio nÃ£o puder ser respondida com os dados disponÃ­veis, diga isso de forma educada.
# 
#                                                     Dados da Oportunidade com identificador {selected_opportunity_identifier}:
# 
#                                                     Detalhes Principais:
#                                                     - ID: {opportunity_details.get('ID', 'N/A')}
#                                                     - TÃ­tulo: {opportunity_details.get('TÃ­tulo', 'N/A')}
#                                                     - ResponsÃ¡vel: {opportunity_details.get('ResponsÃ¡vel', 'N/A')}
#                                                     - Estado: {opportunity_details.get('Estado', 'N/A')}
#                                                     - EstÃ¡gio Atual: {opportunity_details.get('EstÃ¡gio', 'N/A')}
#                                                     - Valor: R$ {opportunity_details.get('Valor', 'N/A')}
#                                                     - Origem: {opportunity_details.get('Origem', 'N/A')}
#                                                     - Prob %: {opportunity_details.get('Prob %', 'N/A')}
#                                                     - OC: {opportunity_details.get('OC', 'N/A')}
#                                                     - Data de Abertura: {opportunity_details.get('Data de abertura', 'N/A')}
#                                                     - Data de Fechamento: {opportunity_details.get('Data fechamento', 'N/A')}
# 
#                                                     Detalhes de Fechamento (se aplicÃ¡vel):
#                                                     - Valor Fechamento: R$ {opportunity_details.get('Valor fechamento', 'N/A')}
#                                                     - Valor Rec. Fechamento: R$ {opportunity_details.get('Valor rec. fechamento', 'N/A')}
#                                                     - RazÃ£o de Fechamento: {opportunity_details.get('RazÃ£o de fechamento', 'N/A')}
#                                                     - ObservaÃ§Ã£o de Fechamento: {opportunity_details.get('ObservaÃ§Ã£o de fechamento', 'N/A')}
# 
#                                                     Linha do Tempo da Oportunidade (Progresso pelos EstÃ¡gios):
#                                                     {opportunity_timeline[['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']].to_string(index=False)}
# 
#                                                     Campos disponÃ­veis para anÃ¡lise nos Detalhes Principais: 'ID', 'TÃ­tulo', 'ResponsÃ¡vel', 'Estado', 'EstÃ¡gio', 'Valor', 'Origem', 'Prob %', 'OC', 'Data de Abertura', 'Data de Fechamento'.
#                                                     Campos disponÃ­veis para anÃ¡lise nos Detalhes de Fechamento: 'Valor Fechamento', 'Valor Rec. Fechamento', 'RazÃ£o de Fechamento', 'ObservaÃ§Ã£o de Fechamento'.
#                                                     Campos disponÃ­veis na Linha do Tempo: 'EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted'.
# 
#                                                     Pergunta do UsuÃ¡rio: {user_query}
#                                                     """
#                                                     response = client.chat.completions.create(
#                                                         model="gpt-4o-mini", # Or another suitable model
#                                                         messages=[
#                                                             {"role": "system", "content": "VocÃª Ã© um assistente de BI Ãºtil. Responda em PortuguÃªs."},
#                                                             {"role": "user", "content": prompt}
#                                                         ],
#                                                         max_tokens=500
#                                                     )
#                                                     st.session_state['ai_response'] = response.choices[0].message.content # Store response in session state
#                                                     st.rerun() # Rerun to display response immediately
#                                                 except Exception as e:
#                                                     st.error(f"Ocorreu um erro ao chamar a API da OpenAI: {e}")
#                                                     st.session_state['ai_response'] = f"Erro: {e}" # Store error message
#                                                     st.rerun()
#                                         else:
#                                             st.warning("Por favor, insira uma pergunta.")
# 
#                                 with col_ai_button2:
#                                     if st.button("Gerar Resumo da Oportunidade", use_container_width=True):
#                                          with st.spinner("Gerando resumo da oportunidade..."):
#                                             try:
#                                                 prompt = f"""
#                                                 VocÃª Ã© um assistente de BI focado em analisar dados de oportunidades de negÃ³cios.
#                                                 Sua tarefa Ã© gerar um resumo conciso e Ãºtil da oportunidade com o identificador {selected_opportunity_identifier}
#                                                 com base nos dados fornecidos abaixo.
#                                                 **Use APENAS os dados fornecidos.**
#                                                 Inclua detalhes importantes como o estado atual, valor, responsÃ¡vel e uma breve anÃ¡lise da linha do tempo, se aplicÃ¡vel.
# 
#                                                 Dados da Oportunidade com identificador {selected_opportunity_identifier}:
# 
#                                                 Detalhes Principais:
#                                                 - ID: {opportunity_details.get('ID', 'N/A')}
#                                                 - TÃ­tulo: {opportunity_details.get('TÃ­tulo', 'N/A')}
#                                                 - ResponsÃ¡vel: {opportunity_details.get('ResponsÃ¡vel', 'N/A')}
#                                                 - Estado: {opportunity_details.get('Estado', 'N/A')}
#                                                 - EstÃ¡gio Atual: {opportunity_details.get('EstÃ¡gio', 'N/A')}
#                                                 - Valor: R$ {opportunity_details.get('Valor', 'N/A')}
#                                                 - Origem: {opportunity_details.get('Origem', 'N/A')}
#                                                 - Prob %: {opportunity_details.get('Prob %', 'N/A')}
#                                                 - OC: {opportunity_details.get('OC', 'N/A')}
#                                                 - Data de Abertura: {opportunity_details.get('Data de abertura', 'N/A')}
#                                                 - Data de Fechamento: {opportunity_details.get('Data fechamento', 'N/A')}
# 
#                                                 Detalhes de Fechamento (se aplicÃ¡vel):
#                                                 - Valor Fechamento: R$ {opportunity_details.get('Valor fechamento', 'N/A')}
#                                                 - Valor Rec. Fechamento: R$ {opportunity_details.get('Valor rec. fechamento', 'N/A')}
#                                                 - RazÃ£o de Fechamento: {opportunity_details.get('RazÃ£o de fechamento', 'N/A')}
#                                                 - ObservaÃ§Ã£o de Fechamento: {opportunity_details.get('ObservaÃ§Ã£o de fechamento', 'N/A')}
# 
#                                                 Linha do Tempo da Oportunidade (Progresso pelos EstÃ¡gios):
#                                                 {opportunity_timeline[['EstÃ¡gio', 'Data de abertura', 'Data fechamento', 'Time_in_Stage_Formatted']].to_string(index=False)}
# 
#                                                 Gere o resumo agora.
#                                                 """
#                                                 response = client.chat.completions.create(
#                                                     model="gpt-4o-mini",
#                                                     messages=[
#                                                         {"role": "system", "content": "VocÃª Ã© um assistente de BI Ãºtil. Responda em PortuguÃªs."},
#                                                         {"role": "user", "content": prompt}
#                                                     ],
#                                                     max_tokens=500
#                                                 )
#                                                 st.session_state['ai_response'] = response.choices[0].message.content # Store response
#                                                 st.rerun() # Rerun to display
#                                             except Exception as e:
#                                                 st.error(f"Ocorreu um erro ao chamar a API da OpenAI para gerar o resumo: {e}")
#                                                 st.session_state['ai_response'] = f"Erro: {e}" # Store error message
#                                                 st.rerun()
#                             else:
#                                 st.warning("O cliente OpenAI nÃ£o estÃ¡ inicializado. A funcionalidade do assistente de IA estÃ¡ desativada.")
#                                 st.write(st.session_state.get('ai_response', '')) # Display any previous error/message
# 
#                             # Display AI response
#                             if st.session_state.get('ai_response'): # Check if there's a response to display
#                                  st.markdown("---") # Add a separator
#                                  st.markdown("**Resposta/Resumo do Assistente de IA:**")
#                                  st.markdown(st.session_state['ai_response'])
# 
#                     except Exception as e:
#                          st.error(f"Erro ao processar os detalhes da oportunidade {selected_opportunity_identifier}: {e}")
# 
#             except Exception as e:
#                  st.error(f"Erro ao carregar identificadores de oportunidade ou selecionar: {e}")
# 
# 
# except Exception as e:
#     st.error(f"Erro geral ao carregar ou processar dados: {e}")

"""## Review ai agent integration

### Subtask:
Examine the AI agent interaction code, including prompt construction and response handling, to ensure it's correctly using the opportunity data and providing relevant responses. Address any potential issues with API calls or response parsing.

**Reasoning**:
Inspect the AI agent interaction code in app.py to ensure correct prompt construction, data usage, API calls, and response handling.
"""

with open('/content/app.py', 'r') as f:
    app_code = f.read()

# Find the start of the AI Agent Interaction Section
ai_section_start = app_code.find('# --- AI Agent Interaction Section ---')
# Find the end of the AI Agent Interaction Section (or the end of the report page section)
ai_section_end = app_code.find('else:', ai_section_start) # Assuming 'else' is the start of the next main block

ai_code_snippet = app_code[ai_section_start:ai_section_end]

print("--- AI Agent Interaction Code Snippet ---")
print(ai_code_snippet)
print("-----------------------------------------")

print("\nReview of AI Agent Interaction Logic:")

print("\n1. OpenAI Client Initialization Check:")
print("   - The code checks `if client:` before displaying the AI interaction UI. This correctly ensures the UI is only shown if the OpenAI client was successfully initialized.")

print("\n2. User Input and Button Logic:")
print("   - A `st.text_area` is used for user input (`user_query`).")
print("   - Two buttons are provided: 'Obter Resposta da IA' and 'Gerar Resumo da Oportunidade'.")
print("   - The 'Obter Resposta da IA' button triggers the API call if `user_query` is not empty.")
print("   - The 'Gerar Resumo da Oportunidade' button triggers an API call with a predefined prompt for a summary.")
print("   - `st.spinner` is used to show a loading indicator during the API call.")

print("\n3. Prompt Construction:")
print("   - The prompt is constructed using an f-string, incorporating `selected_opportunity_identifier`, `opportunity_details`, and `opportunity_timeline`.")
print("   - `opportunity_details.get('ColumnName', 'N/A')` is correctly used to safely access details, handling potential missing columns gracefully.")
print("   - The `opportunity_timeline` DataFrame is converted to a string using `.to_string(index=False)` to include its content in the prompt.")
print("   - Explicit lists of 'Campos disponÃ­veis' are included in the prompt, which helps guide the model.")
print("   - Instructions are given to the model to 'Use APENAS os dados fornecidos' and to state if a question cannot be answered.")

print("\n4. OpenAI API Call:")
print("   - `client.chat.completions.create` is used for the API call.")
print("   - The model is set to 'gpt-4o-mini'.")
print("   - `messages` correctly includes a system message and the user prompt.")
print("   - `max_tokens` is set to 500, limiting the response length.")

print("\n5. Response Handling and Display:")
print("   - The response content is accessed via `response.choices[0].message.content`.")
print("   - The response is stored in `st.session_state['ai_response']`.")
print("   - `st.rerun()` is called after storing the response to update the UI and display the result.")
print("   - The stored response is displayed using `st.markdown` after a separator.")
print("   - Error handling for the API call is included in a `try...except` block, displaying an error message if the API call fails.")

print("\nPotential Issues or Areas for Refinement:")
print("- The prompt includes the full `opportunity_timeline.to_string()`. For opportunities with many stage changes, this string could become very long, potentially exceeding token limits or making the prompt less effective. Consider summarizing the timeline or providing key stages/transitions instead of the full table dump for very long timelines.")
print("- The `st.rerun()` after getting the AI response causes the entire app to reload, including re-running the data loading and all filters. This might be slow for large datasets or complex dashboards. Consider alternative ways to update only the AI response area if performance becomes an issue (though `st.session_state` and `st.rerun` is a standard Streamlit pattern for state management).")
print("- The `key='user_query'` in the text area is good for state management, but ensure unique keys are used for all interactive widgets if more are added.")
print("- The check `if st.session_state.get('ai_response')` and initialization `st.session_state['ai_response'] = ""` is a good pattern for persistent display across reruns.")

print("\nOverall, the AI agent interaction code appears well-structured and correctly integrated with the opportunity data and OpenAI API.")

"""## Test and verify

### Subtask:
After making corrections, thoroughly test the "RelatÃ³rio de Oportunidade" page with different opportunity selections to ensure all elements update correctly and the AI agent functions as expected.

**Reasoning**:
Run the Streamlit application and ngrok tunnel to test the "RelatÃ³rio de Oportunidade" page and the AI agent interaction as per the instructions.
"""

import os
from pyngrok import ngrok
from google.colab import userdata
import time

# Kill any running ngrok tunnels
ngrok.kill()

# Use the authtoken stored in Colab secrets
try:
    ngrok_authtoken = userdata.get('NGROK_AUTHTOKEN')
    ngrok.set_auth_token(ngrok_authtoken)
    print("ngrok authtoken configurado com sucesso.")
except userdata.SecretNotFoundError:
    print("NGROK_AUTHTOKEN nÃ£o encontrado nas secrets do Colab. Por favor, adicione-o.")
    print("VÃ¡ para a barra lateral esquerda (ğŸ”‘), expanda 'Secrets', e adicione um secret chamado NGROK_AUTHTOKEN.")
except Exception as e:
    print(f"Ocorreu um erro ao configurar o authtoken do ngrok: {e}")

# Run the Streamlit app in the background
# Use &>/dev/null& to suppress output if desired, but removing it can help debug
os.system("streamlit run app.py &>/dev/null&")

# Wait a few seconds for the app to start
time.sleep(5)

# Connect ngrok to the Streamlit port (8501)
try:
    public_url = ngrok.connect(8501, "http")
    print(f"URL pÃºblico do ngrok: {public_url}")
    print("Acesse este URL no seu navegador para interagir com o Streamlit app.")
except Exception as e:
    print(f"Ocorreu um erro ao conectar o ngrok: {e}")

# Keep the cell alive for testing
# This will print a message every minute to indicate the cell is still running
while True:
    time.sleep(60)
    print("Cell active for testing...")